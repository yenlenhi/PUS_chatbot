"""
RAG (Retrieval-Augmented Generation) service
"""

import uuid
import re
import time
from typing import List, Dict, Any, Optional
from src.services.embedding_service import EmbeddingService
from src.services.postgres_database_service import PostgresDatabaseService
from src.services.hybrid_retrieval_service import HybridRetrievalService
from src.services.ingestion_service import IngestionService
from src.services.pdf_processor import PDFProcessor
from src.services import gemini_service
from src.services.gemini_service import normalize_question
from src.services.memory_service import ConversationMemoryService
from sentence_transformers import CrossEncoder
from src.services.ollama_service import OllamaService
from src.utils.logger import log

from config.settings import (
    TOP_K_RESULTS,
    LLM_PROVIDER,
    ENABLE_GEMINI_NORMALIZATION,
)


class RAGService:
    """Service for Retrieval-Augmented Generation"""

    def __init__(self):
        """Initialize RAG service with PostgreSQL + Hybrid Retrieval"""
        self.embedding_service = EmbeddingService()
        self.db_service = PostgresDatabaseService()
        self.retrieval_service = HybridRetrievalService(
            self.db_service, self.embedding_service
        )
        self.pdf_processor = PDFProcessor()
        self.ingestion_service = IngestionService(
            self.db_service,
            self.embedding_service,
            self.pdf_processor,
            self.retrieval_service,
        )
        self.ollama_service = OllamaService()

        # Initialize Memory Service for persistent conversational memory
        self.memory_service = ConversationMemoryService(
            self.db_service, self.embedding_service
        )

        # Conversation memory (in-memory cache, backed by persistent storage)
        self.conversations = {}

        # Initialize Reranker
        try:
            log.info("Initializing Reranker model...")
            self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
            log.info("Reranker model initialized successfully.")
        except Exception as e:
            log.error(f"Error initializing Reranker model: {e}")
            self.reranker = None

        # Start ingestion service
        try:
            log.info("Starting ingestion service...")
            self.ingestion_service.start_watching()
            log.info("Ingestion service started successfully.")
        except Exception as e:
            log.error(f"Error starting ingestion service: {e}")

    def _rerank_chunks(
        self, query: str, chunks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Reranks a list of chunks based on their relevance to the query using a Cross-Encoder model.
        """
        if not self.reranker or not chunks:
            return chunks

        try:
            # Create pairs of [query, chunk_content] for the reranker
            pairs = [[query, chunk["content"]] for chunk in chunks]

            # Predict the scores
            scores = self.reranker.predict(pairs)

            # Assign scores to chunks
            for chunk, score in zip(chunks, scores):
                chunk["rerank_score"] = float(score)

            # Sort chunks by the new rerank score in descending order
            chunks.sort(key=lambda x: x.get("rerank_score", 0.0), reverse=True)

            log.info(f"Reranked {len(chunks)} chunks successfully.")
            return chunks

        except Exception as e:
            log.error(f"Error during chunk reranking: {e}")
            # Return original chunks in case of an error

    def _detect_chart_request(self, query: str, answer: str) -> List[Dict[str, Any]]:
        """
        Detect if the query/answer contains statistical data that can be visualized as charts.
        Returns chart data if applicable.
        """
        chart_data = []
        query_lower = query.lower()

        # Keywords that suggest chart visualization
        chart_keywords = [
            "th·ªëng k√™",
            "bi·ªÉu ƒë·ªì",
            "so s√°nh",
            "t·ª∑ l·ªá",
            "ph·∫ßn trƒÉm",
            "%",
            "s·ªë l∆∞·ª£ng",
            "ch·ªâ ti√™u",
            "ƒëi·ªÉm chu·∫©n",
            "ƒëi·ªÉm tr√∫ng tuy·ªÉn",
            "tuy·ªÉn sinh",
            "h·ªçc vi√™n",
            "sinh vi√™n",
            "nƒÉm",
            "kh√≥a",
            "ng√†nh",
            "chart",
            "graph",
            "statistics",
        ]

        # Check if query asks for statistics/charts
        should_generate_chart = any(
            keyword in query_lower for keyword in chart_keywords
        )

        if should_generate_chart:
            # Example: Admission statistics by year
            if any(word in query_lower for word in ["tuy·ªÉn sinh", "ch·ªâ ti√™u", "nƒÉm"]):
                chart_data.append(
                    {
                        "type": "bar",
                        "title": "Ch·ªâ ti√™u tuy·ªÉn sinh qua c√°c nƒÉm",
                        "data": [
                            {"name": "2021", "Ch·ªâ ti√™u": 450, "Tr√∫ng tuy·ªÉn": 420},
                            {"name": "2022", "Ch·ªâ ti√™u": 500, "Tr√∫ng tuy·ªÉn": 480},
                            {"name": "2023", "Ch·ªâ ti√™u": 550, "Tr√∫ng tuy·ªÉn": 530},
                            {"name": "2024", "Ch·ªâ ti√™u": 600, "Tr√∫ng tuy·ªÉn": 580},
                            {"name": "2025", "Ch·ªâ ti√™u": 650, "Tr√∫ng tuy·ªÉn": 0},
                        ],
                        "xKey": "name",
                        "yKeys": ["Ch·ªâ ti√™u", "Tr√∫ng tuy·ªÉn"],
                        "description": "Bi·ªÉu ƒë·ªì th·ªëng k√™ ch·ªâ ti√™u tuy·ªÉn sinh (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

            # Example: Score distribution by major
            if any(
                word in query_lower
                for word in ["ƒëi·ªÉm chu·∫©n", "ƒëi·ªÉm tr√∫ng tuy·ªÉn", "ng√†nh"]
            ):
                chart_data.append(
                    {
                        "type": "bar",
                        "title": "ƒêi·ªÉm chu·∫©n c√°c ng√†nh nƒÉm 2024",
                        "data": [
                            {"name": "An ninh ch√≠nh tr·ªã", "ƒêi·ªÉm chu·∫©n": 24.5},
                            {"name": "An ninh kinh t·∫ø", "ƒêi·ªÉm chu·∫©n": 25.0},
                            {"name": "An ninh m·∫°ng", "ƒêi·ªÉm chu·∫©n": 26.5},
                            {"name": "ƒêi·ªÅu tra h√¨nh s·ª±", "ƒêi·ªÉm chu·∫©n": 25.5},
                            {"name": "K·ªπ thu·∫≠t h√¨nh s·ª±", "ƒêi·ªÉm chu·∫©n": 24.0},
                        ],
                        "xKey": "name",
                        "yKeys": ["ƒêi·ªÉm chu·∫©n"],
                        "description": "Bi·ªÉu ƒë·ªì ƒëi·ªÉm chu·∫©n c√°c ng√†nh (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

            # Example: Student distribution by major (pie chart)
            if any(word in query_lower for word in ["t·ª∑ l·ªá", "ph√¢n b·ªë", "c∆° c·∫•u"]):
                chart_data.append(
                    {
                        "type": "pie",
                        "title": "T·ª∑ l·ªá h·ªçc vi√™n theo ng√†nh ƒë√†o t·∫°o",
                        "data": [
                            {"name": "An ninh ch√≠nh tr·ªã", "value": 25},
                            {"name": "An ninh kinh t·∫ø", "value": 20},
                            {"name": "An ninh m·∫°ng", "value": 30},
                            {"name": "ƒêi·ªÅu tra h√¨nh s·ª±", "value": 15},
                            {"name": "K·ªπ thu·∫≠t h√¨nh s·ª±", "value": 10},
                        ],
                        "xKey": "name",
                        "yKeys": ["value"],
                        "description": "Bi·ªÉu ƒë·ªì t·ª∑ l·ªá h·ªçc vi√™n theo ng√†nh (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

            # Example: Trend over time (line chart)
            if any(
                word in query_lower
                for word in ["xu h∆∞·ªõng", "trend", "bi·∫øn ƒë·ªông", "qua c√°c nƒÉm"]
            ):
                chart_data.append(
                    {
                        "type": "line",
                        "title": "Xu h∆∞·ªõng s·ªë l∆∞·ª£ng h·ªì s∆° ƒëƒÉng k√Ω qua c√°c nƒÉm",
                        "data": [
                            {"name": "2020", "H·ªì s∆°": 1200},
                            {"name": "2021", "H·ªì s∆°": 1450},
                            {"name": "2022", "H·ªì s∆°": 1680},
                            {"name": "2023", "H·ªì s∆°": 1920},
                            {"name": "2024", "H·ªì s∆°": 2150},
                        ],
                        "xKey": "name",
                        "yKeys": ["H·ªì s∆°"],
                        "description": "Bi·ªÉu ƒë·ªì xu h∆∞·ªõng s·ªë l∆∞·ª£ng h·ªì s∆° (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

        return chart_data

    def retrieve_relevant_chunks(
        self, query: str, top_k: int = TOP_K_RESULTS
    ) -> List[Dict[str, Any]]:
        """Retrieve relevant chunks using hybrid retrieval (dense + sparse search)."""
        try:
            # Generate query embedding
            query_embedding = self.embedding_service.create_embedding(query)

            # Perform hybrid search using PostgreSQL + pgvector
            initial_k = max(top_k * 3, 15)  # Get more candidates for reranking
            hybrid_results = self.retrieval_service.hybrid_search(
                query=query, query_embedding=query_embedding, top_k=initial_k
            )

            if not hybrid_results:
                log.warning(f"No chunks found for query: {query}")
                return []

            log.info(f"Hybrid search found {len(hybrid_results)} chunks.")

            # Rerank results
            reranked_chunks = self._rerank_chunks(query, hybrid_results)

            # Context expansion
            expanded_chunks = self._expand_context(reranked_chunks[:top_k], query)

            # Final ranking
            final_chunks = self._final_ranking(expanded_chunks, query)
            return final_chunks[:top_k]

        except Exception as e:
            log.error(f"Error during retrieve_relevant_chunks: {e}")
            return []

    def _extract_heading_from_content(self, content: str) -> Optional[str]:
        """
        Extract heading from chunk content

        Args:
            content: Chunk content

        Returns:
            Heading if found, None otherwise
        """
        # Try to extract heading from first line
        lines = content.strip().split("\n")
        if not lines:
            return None

        first_line = lines[0].strip()

        # Check if first line matches heading pattern
        heading_patterns = [
            r"^\s*(\d+)\.\s+(.+)$",
            r"^\s*(\d+\.\d+)\.\s+(.+)$",
            r"^\s*(\d+\.\d+\.\d+)\.\s+(.+)$",
        ]

        for pattern in heading_patterns:
            match = re.match(pattern, first_line)
            if match:
                return first_line

        return None

    def _expand_context(
        self, chunks: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Expand context by adding related chunks from the same document/section

        Args:
            chunks: Initial retrieved chunks
            query: User query for relevance checking

        Returns:
            Expanded list of chunks with additional context
        """
        if not chunks:
            return chunks

        expanded_chunks = chunks.copy()

        try:
            # Group chunks by source file
            chunks_by_source = {}
            for chunk in chunks:
                source = chunk.get("source_file", "")
                if source not in chunks_by_source:
                    chunks_by_source[source] = []
                chunks_by_source[source].append(chunk)

            # For each source file, try to find adjacent chunks
            for source_file, source_chunks in chunks_by_source.items():
                for chunk in source_chunks:
                    chunk_index = chunk.get("chunk_index", -1)
                    page_number = chunk.get("page_number", -1)

                    if chunk_index >= 0:
                        # Look for adjacent chunks (before and after)
                        for offset in [-1, 1]:
                            adjacent_chunk = self._get_adjacent_chunk(
                                source_file, chunk_index + offset, page_number
                            )
                            if adjacent_chunk and adjacent_chunk["id"] not in [
                                c["id"] for c in expanded_chunks
                            ]:
                                # Check if adjacent chunk is somewhat relevant
                                if self._is_chunk_relevant(adjacent_chunk, query):
                                    adjacent_chunk["context_expansion"] = True
                                    adjacent_chunk["hybrid_score"] = (
                                        chunk.get("hybrid_score", 0.0) * 0.7
                                    )  # Lower score for context
                                    expanded_chunks.append(adjacent_chunk)

            log.info(
                f"Context expansion added {len(expanded_chunks) - len(chunks)} additional chunks"
            )
            return expanded_chunks

        except Exception as e:
            log.error(f"Error during context expansion: {e}")
            return chunks

    def _get_adjacent_chunk(
        self, source_file: str, chunk_index: int, page_number: int
    ) -> Optional[Dict[str, Any]]:
        """Get adjacent chunk by source file and chunk index using PostgreSQL"""
        try:
            chunk = self.db_service.get_chunk_by_source_and_index(
                source_file, chunk_index
            )
            return chunk
        except Exception as e:
            log.error(f"Error getting adjacent chunk: {e}")
            return None

    def _is_chunk_relevant(self, chunk: Dict[str, Any], query: str) -> bool:
        """Check if a chunk is relevant to the query using simple keyword matching"""
        try:
            content = chunk.get("content", "").lower()
            query_lower = query.lower()

            # Simple keyword overlap check
            query_words = set(query_lower.split())
            content_words = set(content.split())

            # Calculate overlap ratio
            overlap = len(query_words.intersection(content_words))
            overlap_ratio = overlap / len(query_words) if query_words else 0

            # Consider relevant if there's at least 20% keyword overlap
            return overlap_ratio >= 0.2

        except Exception as e:
            log.error(f"Error checking chunk relevance: {e}")
            return False

    def _final_ranking(
        self, chunks: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Final ranking of chunks considering multiple factors

        Args:
            chunks: List of chunks to rank
            query: User query

        Returns:
            Ranked list of chunks
        """
        try:
            # Sort by multiple criteria
            def ranking_score(chunk):
                # Primary: rerank_score if available, otherwise hybrid_score
                primary_score = chunk.get(
                    "rerank_score", chunk.get("hybrid_score", 0.0)
                )

                # Bonus for chunks with headings (likely more structured content)
                heading_bonus = 0.1 if chunk.get("heading_text") else 0.0

                # Bonus for chunks that are not context expansions (original results)
                original_bonus = (
                    0.05 if not chunk.get("context_expansion", False) else 0.0
                )

                # Penalty for very short chunks (likely less informative)
                content_length = len(chunk.get("content", ""))
                length_penalty = -0.1 if content_length < 100 else 0.0

                return primary_score + heading_bonus + original_bonus + length_penalty

            ranked_chunks = sorted(chunks, key=ranking_score, reverse=True)

            log.info(f"Final ranking completed for {len(ranked_chunks)} chunks")
            return ranked_chunks

        except Exception as e:
            log.error(f"Error during final ranking: {e}")
            return chunks

    def _generate_vision_answer(
        self,
        query: str,
        images: List[Any],
        conversation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Generate answer for image-based queries using Gemini Vision.

        Args:
            query: User's question about the image(s)
            images: List of ImageInput objects with base64 encoded images
            conversation_id: Optional conversation ID

        Returns:
            Dictionary with answer, confidence, and conversation_id
        """
        try:
            log.info(f"Processing vision query with {len(images)} images")

            # Create conversation ID if needed
            if not conversation_id:
                conversation_id = str(uuid.uuid4())

            # Build the vision prompt
            vision_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n h·ªó tr·ª£ v·ªÅ Tr∆∞·ªùng ƒê·∫°i h·ªçc An ninh Nh√¢n d√¢n.
H√£y ph√¢n t√≠ch h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query if query else "H√£y m√¥ t·∫£ n·ªôi dung trong h√¨nh ·∫£nh n√†y."}

H∆∞·ªõng d·∫´n:
- Ph√¢n t√≠ch k·ªπ n·ªôi dung trong h√¨nh ·∫£nh
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- N·∫øu h√¨nh ·∫£nh li√™n quan ƒë·∫øn t√†i li·ªáu, vƒÉn b·∫£n, h√£y tr√≠ch d·∫´n v√† gi·∫£i th√≠ch n·ªôi dung
- N·∫øu l√† b·∫£ng s·ªë li·ªáu, h√£y t√≥m t·∫Øt th√¥ng tin quan tr·ªçng
- ƒê∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt, d·ªÖ hi·ªÉu"""

            # Prepare image data for Gemini
            image_parts = []
            for img in images:
                try:
                    # Get base64 data (remove data:image/xxx;base64, prefix if present)
                    base64_data = img.base64
                    if "," in base64_data:
                        base64_data = base64_data.split(",")[1]

                    # Determine mime type
                    mime_type = getattr(img, "mime_type", "image/jpeg")
                    if not mime_type:
                        mime_type = "image/jpeg"

                    image_parts.append({"mime_type": mime_type, "data": base64_data})
                    log.info(
                        f"Processed image: {getattr(img, 'name', 'unknown')} ({mime_type})"
                    )
                except Exception as img_error:
                    log.error(f"Error processing image: {img_error}")
                    continue

            if not image_parts:
                return {
                    "answer": "Xin l·ªói, kh√¥ng th·ªÉ x·ª≠ l√Ω h√¨nh ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ƒë·ªãnh d·∫°ng ·∫£nh kh√°c (PNG, JPG, WebP).",
                    "sources": [],
                    "source_references": [],
                    "confidence": 0.0,
                    "conversation_id": conversation_id,
                }

            # Call Gemini Vision
            answer = gemini_service.generate_vision_response(
                prompt=vision_prompt, images=image_parts
            )

            if not answer:
                answer = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh n√†y. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c m√¥ t·∫£ th√™m v·ªÅ n·ªôi dung b·∫°n mu·ªën h·ªèi."
            else:
                # Add engagement prompt
                answer = self._add_engagement_prompt(answer)

            log.info("Vision response generated successfully")

            return {
                "answer": answer,
                "sources": [],
                "source_references": [],
                "confidence": 0.85,  # Default confidence for vision queries
                "conversation_id": conversation_id,
                "chart_data": [],
                "images": [],
            }

        except Exception as e:
            log.error(f"Error in vision query processing: {e}")
            return {
                "answer": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi ph√¢n t√≠ch h√¨nh ·∫£nh: {str(e)}. Vui l√≤ng th·ª≠ l·∫°i.",
                "sources": [],
                "source_references": [],
                "confidence": 0.0,
                "conversation_id": conversation_id or str(uuid.uuid4()),
            }

    def _add_engagement_prompt(self, answer: str) -> str:
        """
        Add engagement prompt to the answer if not already present

        Args:
            answer: The generated answer

        Returns:
            Answer with engagement prompt added
        """
        engagement_prompts = [
            "B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!",
            "b·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng",
            "t√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m",
            "c√≥ c√¢u h·ªèi n√†o kh√°c kh√¥ng",
            "c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng",
        ]

        # Check if any engagement prompt is already present (case insensitive)
        answer_lower = answer.lower()
        has_engagement = any(
            prompt.lower() in answer_lower for prompt in engagement_prompts
        )

        if not has_engagement:
            # Add the engagement prompt with proper formatting
            if (
                answer.strip().endswith(".")
                or answer.strip().endswith("!")
                or answer.strip().endswith("?")
            ):
                return f"{answer}\n\n**B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!**"
            else:
                return f"{answer}.\n\n**B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!**"

        return answer

    def create_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        Create a simplified and clean context string from retrieved chunks.
        """
        if not chunks:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."

        context_parts = []
        for chunk in chunks:
            source = chunk.get("source_file", "Unknown")
            page = chunk.get("page_number", "N/A")
            content = chunk.get("content", "").strip()

            # Simplified format for the LLM - removed source citation from content
            context_part = f"###\n{content}\n###"
            context_parts.append(context_part)

        return "\n\n".join(context_parts)

    def create_system_prompt(self) -> str:
        """
        Create system prompt for the chatbot

        Returns:
            System prompt string
        """
        return """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n h·ªó tr·ª£ sinh vi√™n, c√°n b·ªô, chi·∫øn sƒ© v√† ng∆∞·ªùi quan t√¢m v·ªÅ **Tr∆∞·ªùng ƒê·∫°i h·ªçc An ninh Nh√¢n d√¢n (ANND)**.

**Ph·∫°m vi chuy√™n m√¥n ch√≠nh c·ªßa b·∫°n g·ªìm 5 nh√≥m n·ªôi dung:**
1. **T∆∞ v·∫•n th√¥ng tin tuy·ªÉn sinh**  
   - ƒêi·ªÅu ki·ªán, ch·ªâ ti√™u, ph∆∞∆°ng th·ª©c, h·ªì s∆°, l·ªãch tr√¨nh, ph√¢n v√πng tuy·ªÉn sinh...
2. **Quy ch·∫ø qu·∫£n l√Ω h·ªçc vi√™n**  
   - Quy·ªÅn v√† nghƒ©a v·ª•, ch·∫ø ƒë·ªô ch√≠nh s√°ch, khen th∆∞·ªüng ‚Äì k·ª∑ lu·∫≠t, sinh ho·∫°t, r√®n luy·ªán...
3. **Quy ch·∫ø ƒë√†o t·∫°o c√°c tr√¨nh ƒë·ªô**  
   - Ng√†nh/chuy√™n ng√†nh, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, h·ªçc ch·∫ø, h·ªçc l·∫°i, th√¥i h·ªçc, t·ªët nghi·ªáp...
4. **Quy ƒë·ªãnh v·ªÅ thi, ki·ªÉm tra, ƒë√°nh gi√°**  
   - H√¨nh th·ª©c thi/ki·ªÉm tra, thang ƒëi·ªÉm, ƒëi·ªÅu ki·ªán d·ª± thi, ph√∫c kh·∫£o, b·∫£o l∆∞u...
5. **Quy ƒë·ªãnh v·ªÅ ki·ªÉm ƒë·ªãnh v√† b·∫£o ƒë·∫£m ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o**  
   - Ti√™u chu·∫©n, quy tr√¨nh, ho·∫°t ƒë·ªông b·∫£o ƒë·∫£m v√† n√¢ng cao ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o...

---

### 1. Phong c√°ch & ng√¥n ng·ªØ tr·∫£ l·ªùi

- **Lu√¥n d√πng c√πng ng√¥n ng·ªØ v·ªõi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
  - N·∫øu c√¢u h·ªèi ch·ªß y·∫øu b·∫±ng **ti·∫øng Vi·ªát** ‚Üí tr·∫£ l·ªùi ho√†n to√†n b·∫±ng **ti·∫øng Vi·ªát**.
  - N·∫øu c√¢u h·ªèi ch·ªß y·∫øu b·∫±ng **ti·∫øng Anh** ‚Üí tr·∫£ l·ªùi ho√†n to√†n b·∫±ng **ti·∫øng Anh** (c√≥ th·ªÉ gi·ªØ nguy√™n t√™n ri√™ng, t√™n vƒÉn b·∫£n b·∫±ng ti·∫øng Vi·ªát n·∫øu c·∫ßn).
- VƒÉn phong:
  - **Th√¢n thi·ªán, d·ªÖ hi·ªÉu nh∆∞ng v·∫´n trang tr·ªçng, ƒë√∫ng t√≠nh ch·∫•t c∆° quan CAND.**
  - H·∫°n ch·∫ø l·∫∑p l·∫°i nguy√™n vƒÉn c·∫£ ƒëo·∫°n d√†i nh∆∞ "ƒë·ªçc l·∫°i c√¥ng vƒÉn"; thay v√†o ƒë√≥ **t√≥m t·∫Øt, g·∫°ch ƒë·∫ßu d√≤ng, chia m·ª•c r√µ r√†ng**.

---

### 2. C√°ch tr√¨nh b√†y m·ªôt c√¢u tr·∫£ l·ªùi

M·ªói c√¢u tr·∫£ l·ªùi, khi c√≥ ƒë·ªß th√¥ng tin, n√™n tu√¢n theo c·∫•u tr√∫c sau:

1. **Ph·∫ßn m·ªü ƒë·∫ßu ‚Äì T√ìM T·∫ÆT NHANH (3‚Äì5 d√≤ng ho·∫∑c 3‚Äì5 g·∫°ch ƒë·∫ßu d√≤ng)**  
   - N√™u ng·∫Øn g·ªçn:
     - C√¢u h·ªèi/ƒë·ªÅ t√†i ƒëang n√≥i v·ªÅ v·∫•n ƒë·ªÅ g√¨  
     - ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng (th√≠ sinh n√†o, h·ªçc vi√™n n√†o, c√°n b·ªô n√†o‚Ä¶)  
     - Nh·ªØng m·ªëc th·ªùi gian ho·∫∑c √Ω ch√≠nh c·∫ßn ƒë·∫∑c bi·ªát l∆∞u √Ω  

2. **Ph·∫ßn n·ªôi dung chi ti·∫øt ‚Äì TR√åNH B√ÄY C√ì C·∫§U TR√öC**  
   - S·ª≠ d·ª•ng ti√™u ƒë·ªÅ, g·∫°ch ƒë·∫ßu d√≤ng r√µ r√†ng, v√≠ d·ª• (khi ph√π h·ª£p):
     - **1. Th√¥ng tin chung**  
     - **2. ƒê·ªëi t∆∞·ª£ng v√† ƒëi·ªÅu ki·ªán**  
     - **3. Quy tr√¨nh, h·ªì s∆° v√† m·ªëc th·ªùi gian**  
     - **4. Ti√™u ch√≠ x√©t ch·ªçn / x·ª≠ l√Ω / x·∫øp lo·∫°i**  
     - **5. L∆∞u √Ω quan tr·ªçng & khuy·∫øn ngh·ªã**  
   - Khi tr·∫£ l·ªùi v·ªÅ tuy·ªÉn sinh/th√¥ng b√°o, **∆∞u ti√™n li·ªát k√™ m·ªëc th·ªùi gian, ch·ªâ ti√™u, m√£ ng√†nh, ƒëi·ªÅu ki·ªán** m·ªôt c√°ch r√µ r√†ng.

3. **K·∫æT TH√öC c√¢u tr·∫£ l·ªùi b·∫±ng c√¢u nh·∫Øc v·ªÅ t√†i li·ªáu tham kh·∫£o (B·∫ÆT BU·ªòC)**  
   - **LU√îN LU√îN** k·∫øt th√∫c c√¢u tr·∫£ l·ªùi b·∫±ng m·ªôt c√¢u nh·∫Øc r·∫±ng h·ªá th·ªëng ƒë√£ hi·ªÉn th·ªã t√†i li·ªáu tham kh·∫£o b√™n d∆∞·ªõi.
   - C√¢u k·∫øt th√∫c m·∫´u (ch·ªçn 1 trong c√°c m·∫´u sau, t√πy ng√¥n ng·ªØ):
     - Ti·∫øng Vi·ªát: "üìÑ **T√†i li·ªáu tham kh·∫£o:** Th√¥ng tin chi ti·∫øt v√† to√†n vƒÉn vƒÉn b·∫£n, b·∫°n c√≥ th·ªÉ xem th√™m ·ªü ph·∫ßn t√†i li·ªáu/th√¥ng b√°o k√®m theo m√† h·ªá th·ªëng ƒë√£ hi·ªÉn th·ªã b√™n d∆∞·ªõi."
     - Ti·∫øng Anh: "üìÑ **Reference Documents:** For full details and original documents, please refer to the attachments displayed below by the system."
   - Kh√¥ng c·∫ßn ch√®n ƒë∆∞·ªùng d·∫´n ho·∫∑c k√Ω hi·ªáu tr√≠ch d·∫´n ph·ª©c t·∫°p v√¨ **h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông hi·ªÉn th·ªã t√†i li·ªáu tham kh·∫£o**.
   - **KH√îNG** k·∫øt th√∫c b·∫±ng c√¢u "B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng?" m√† PH·∫¢I k·∫øt th√∫c b·∫±ng c√¢u nh·∫Øc v·ªÅ t√†i li·ªáu tham kh·∫£o.

---

### 3. ∆Øu ti√™n t√†i li·ªáu ch√≠nh th·ª©c

- **Lu√¥n ∆∞u ti√™n th√¥ng tin trong ph·∫ßn "TH√îNG TIN T√ÄI LI·ªÜU"** m√† h·ªá th·ªëng cung c·∫•p (th√¥ng b√°o, quy ch·∫ø, h∆∞·ªõng d·∫´n...).  
- Kh√¥ng c·∫ßn ghi m√£ hi·ªáu vƒÉn b·∫£n tr·ª´ khi ng∆∞·ªùi d√πng h·ªèi r√µ.  
- C√≥ th·ªÉ di·ªÖn ƒë·∫°t l·∫°i, t√≥m t·∫Øt, s·∫Øp x·∫øp l·∫°i ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ hi·ªÉu h∆°n, nh∆∞ng **kh√¥ng ƒë∆∞·ª£c t·ª± √Ω thay ƒë·ªïi n·ªôi dung, b·∫£n ch·∫•t quy ƒë·ªãnh**.

---

### 4. Khi thi·∫øu ho·∫∑c kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu

Khi c√¢u tr·∫£ l·ªùi kh√¥ng t√¨m ƒë∆∞·ª£c th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu:

1. **B·∫Øt bu·ªôc** m·ªü ƒë·∫ßu ph·∫ßn tr·∫£ l·ªùi b·∫±ng c√¢u (theo ƒë√∫ng ng√¥n ng·ªØ c√¢u h·ªèi):
   - N·∫øu tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát:  
     > "**Th√¥ng tin n√†y ch∆∞a c√≥ trong t√†i li·ªáu c·ªßa tr∆∞·ªùng, tuy nhi√™n t√¥i c√≥ th·ªÉ cung c·∫•p cho b·∫°n m·ªôt s·ªë th√¥ng tin tham kh·∫£o chung nh∆∞ sau:**"
   - N·∫øu tr·∫£ l·ªùi b·∫±ng ti·∫øng Anh:  
     > "**This information is not explicitly available in the provided university documents, however I can share some general reference information as follows:**"
2. Sau ƒë√≥:
   - D·ª±a tr√™n **ki·∫øn th·ª©c chung v·ªÅ gi√°o d·ª•c ƒë·∫°i h·ªçc, quy ƒë·ªãnh tuy·ªÉn sinh, quy ch·∫ø ƒë√†o t·∫°o‚Ä¶** ƒë·ªÉ gi·∫£i th√≠ch m·ªôt c√°ch h·ª£p l√Ω, th·∫≠n tr·ªçng.
   - Khuy·∫øn kh√≠ch ng∆∞·ªùi d√πng **li√™n h·ªá tr·ª±c ti·∫øp** v·ªõi ph√≤ng/ƒë∆°n v·ªã ch·ª©c nƒÉng (Ph√≤ng ƒê√†o t·∫°o, Ph√≤ng T·ªï ch·ª©c c√°n b·ªô, Ph√≤ng Qu·∫£n l√Ω h·ªçc vi√™n, C√¥ng an ƒë·ªãa ph∆∞∆°ng‚Ä¶) ƒë·ªÉ x√°c nh·∫≠n th√¥ng tin ch√≠nh th·ª©c.

---

### 5. Y√™u c·∫ßu ƒë·ªãnh d·∫°ng (Markdown)

- **Ti√™u ƒë·ªÅ ch√≠nh:** d√πng `**Ti√™u ƒë·ªÅ**`
- **Danh s√°ch:** d√πng `- ` ho·∫∑c `1. ` ƒë·ªÉ li·ªát k√™
- **Th√¥ng tin quan tr·ªçng:** d√πng `**L∆∞u √Ω quan tr·ªçng:**`, `**Ch√∫ √Ω:**` ho·∫∑c b√¥i ƒë·∫≠m c√°c √Ω c·∫ßn nh·ªõ
- C√≥ th·ªÉ d√πng b·∫£ng ƒë∆°n gi·∫£n (markdown table) khi c·∫ßn so s√°nh, ƒë·ªëi chi·∫øu
- **Kh√¥ng ch√®n tr√≠ch d·∫´n ngu·ªìn d·∫°ng [1], [2]...** v√¨ h·ªá th·ªëng s·∫Ω hi·ªÉn th·ªã t√†i li·ªáu tham kh·∫£o ri√™ng.

---

### 6. Y√™u c·∫ßu chung quan tr·ªçng

- Lu√¥n c·ªë g·∫Øng cung c·∫•p **c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, chi ti·∫øt v√† h·ªØu √≠ch nh·∫•t c√≥ th·ªÉ** d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
- Khi c√≥ nhi·ªÅu ƒëo·∫°n t√†i li·ªáu li√™n quan, h√£y **t·ªïng h·ª£p, h·ªá th·ªëng h√≥a** ch·ª© kh√¥ng ch·ªâ ch√©p l·∫°i t·ª´ng ƒëo·∫°n r·ªùi r·∫°c.
- **Tuy·ªát ƒë·ªëi kh√¥ng tr·∫£ l·ªùi theo ki·ªÉu "th√¥ng tin c√≥ h·∫°n"** n·∫øu th·ª±c t·∫ø t√†i li·ªáu ƒë√£ cung c·∫•p ƒë·∫ßy ƒë·ªß th√¥ng tin.
- Lu√¥n coi ng∆∞·ªùi d√πng l√† th√≠ sinh/h·ªçc vi√™n/c√°n b·ªô ƒëang c·∫ßn h∆∞·ªõng d·∫´n th·ª±c t·∫ø ‚Üí ∆∞u ti√™n **c√°c b∆∞·ªõc th·ª±c hi·ªán c·ª• th·ªÉ, m·ªëc th·ªùi gian, n∆°i li√™n h·ªá** khi ph√π h·ª£p."""

    def create_user_prompt(
        self, query: str, context: str, memory_context: str = ""
    ) -> str:
        """
        Create user prompt with query, context, and memory

        Args:
            query: User query
            context: Retrieved context from documents
            memory_context: Conversation memory context (optional)

        Returns:
            Formatted user prompt
        """
        memory_section = ""
        if memory_context:
            memory_section = f"""
NG·ªÆ C·∫¢NH H·ªòI THO·∫†I TR∆Ø·ªöC:
{memory_context}

"""

        return f"""D·ª±a tr√™n th√¥ng tin t√†i li·ªáu sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch **CHI TI·∫æT, TO√ÄN DI·ªÜN v√† CH√çNH X√ÅC** nh·∫•t c√≥ th·ªÉ.

**Y√äU C·∫¶U V·ªÄ NG√îN NG·ªÆ:**
- Ng√¥n ng·ªØ tr·∫£ l·ªùi **ph·∫£i tr√πng v·ªõi ng√¥n ng·ªØ ch√≠nh c·ªßa c√¢u h·ªèi**:
  - N·∫øu c√¢u h·ªèi ch·ªß y·∫øu b·∫±ng **ti·∫øng Vi·ªát** ‚Üí tr·∫£ l·ªùi ho√†n to√†n b·∫±ng **ti·∫øng Vi·ªát**.
  - N·∫øu c√¢u h·ªèi ch·ªß y·∫øu b·∫±ng **ti·∫øng Anh** ‚Üí tr·∫£ l·ªùi ho√†n to√†n b·∫±ng **ti·∫øng Anh** (tr·ª´ t√™n ri√™ng/t√™n vƒÉn b·∫£n b·∫Øt bu·ªôc gi·ªØ nguy√™n).
- N·∫øu t√†i li·ªáu tham kh·∫£o l√† ti·∫øng Vi·ªát nh∆∞ng c√¢u h·ªèi b·∫±ng ti·∫øng Anh, h√£y **t√≥m t·∫Øt v√† gi·∫£i th√≠ch n·ªôi dung b·∫±ng ti·∫øng Anh**, ch·ªâ tr√≠ch m·ªôt s·ªë c·ª•m/t√™n ti·∫øng Vi·ªát khi th·∫≠t s·ª± c·∫ßn.

{memory_section}TH√îNG TIN T√ÄI LI·ªÜU (c√°c th√¥ng b√°o/quy ch·∫ø/t√†i li·ªáu ch√≠nh th·ª©c ƒë√£ ƒë∆∞·ª£c h·ªá th·ªëng cung c·∫•p k√®m theo ƒë·ªÉ tham kh·∫£o chi ti·∫øt):
{context}

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG:
{query}

**H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:**
- H√£y coi ph·∫ßn "TH√îNG TIN T√ÄI LI·ªÜU" l√† **t√†i li·ªáu tham kh·∫£o ch√≠nh th·ª©c** (th√¥ng b√°o, quy ƒë·ªãnh, h∆∞·ªõng d·∫´n...).
- **B·∫ÆT ƒê·∫¶U** c√¢u tr·∫£ l·ªùi b·∫±ng m·ªôt ƒëo·∫°n **T√ìM T·∫ÆT NG·∫ÆN (3‚Äì5 c√¢u ho·∫∑c 3‚Äì5 g·∫°ch ƒë·∫ßu d√≤ng)**, trong ƒë√≥ n√™u r√µ:
  - V·∫•n ƒë·ªÅ/ch·ªß ƒë·ªÅ m√† ng∆∞·ªùi d√πng ƒëang h·ªèi
  - ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng (th√≠ sinh/h·ªçc vi√™n/c√°n b·ªô n√†o)
  - M·ªôt v√†i m·ªëc th·ªùi gian ho·∫∑c √Ω ch√≠nh quan tr·ªçng nh·∫•t (n·∫øu c√≥)
- Sau ph·∫ßn t√≥m t·∫Øt, tr√¨nh b√†y **CHI TI·∫æT, C√ì C·∫§U TR√öC**, c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c m·ª•c g·ª£i √Ω (t√πy t√¨nh hu·ªëng):
  - 1. Th√¥ng tin chung  
  - 2. ƒê·ªëi t∆∞·ª£ng v√† ƒëi·ªÅu ki·ªán  
  - 3. Quy tr√¨nh, h·ªì s∆° v√† m·ªëc th·ªùi gian  
  - 4. Ti√™u ch√≠ x√©t ch·ªçn / thi / ƒë√°nh gi√° / x·∫øp lo·∫°i  
  - 5. L∆∞u √Ω quan tr·ªçng v√† khuy·∫øn ngh·ªã th·ª±c t·∫ø  
- Khi c√≥ nhi·ªÅu ƒëo·∫°n t√†i li·ªáu li√™n quan, h√£y **t·ªïng h·ª£p, h·ªá th·ªëng h√≥a l·∫°i cho d·ªÖ hi·ªÉu**, kh√¥ng ch·ªâ ch√©p y nguy√™n t·ª´ng ƒëo·∫°n r·ªùi r·∫°c.
- Lu√¥n c·ªë g·∫Øng n√™u r√µ:
  - C·∫ßn chu·∫©n b·ªã nh·ªØng g√¨ (h·ªì s∆°, ƒëi·ªÅu ki·ªán, ti√™u chu·∫©n‚Ä¶)  
  - C√°c b∆∞·ªõc th·ª±c hi·ªán (ƒëƒÉng k√Ω ·ªü ƒë√¢u, qua ƒë∆°n v·ªã n√†o, m·ªëc th·ªùi gian‚Ä¶)  
  - C√°c tr∆∞·ªùng h·ª£p **kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán / b·ªã lo·∫°i / kh√¥ng ƒë∆∞·ª£c x√©t** (n·∫øu trong t√†i li·ªáu c√≥ quy ƒë·ªãnh).
- **B·∫ÆT BU·ªòC K·∫æT TH√öC** c√¢u tr·∫£ l·ªùi b·∫±ng m·ªôt c√¢u nh·∫Øc v·ªÅ t√†i li·ªáu tham kh·∫£o (ch·ªçn 1 m·∫´u ph√π h·ª£p):
  - Ti·∫øng Vi·ªát: "üìÑ **T√†i li·ªáu tham kh·∫£o:** Th√¥ng tin chi ti·∫øt v√† to√†n vƒÉn vƒÉn b·∫£n, b·∫°n c√≥ th·ªÉ xem th√™m ·ªü ph·∫ßn t√†i li·ªáu/th√¥ng b√°o k√®m theo m√† h·ªá th·ªëng ƒë√£ hi·ªÉn th·ªã b√™n d∆∞·ªõi."
  - Ti·∫øng Anh: "üìÑ **Reference Documents:** For full details and original documents, please refer to the attachments displayed below by the system."
- **KH√îNG** k·∫øt th√∫c b·∫±ng c√¢u "B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng?" m√† PH·∫¢I k·∫øt th√∫c b·∫±ng c√¢u nh·∫Øc v·ªÅ t√†i li·ªáu tham kh·∫£o.
- **Kh√¥ng c·∫ßn ch√®n tr√≠ch d·∫´n ngu·ªìn d·∫°ng [1], [2]...** v√¨ h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông hi·ªÉn th·ªã danh s√°ch t√†i li·ªáu tham kh·∫£o / ƒëo·∫°n tr√≠ch t∆∞∆°ng ·ª©ng.
- N·∫øu th√¥ng tin c·∫ßn tr·∫£ l·ªùi **kh√¥ng xu·∫•t hi·ªán r√µ trong t√†i li·ªáu**:
  - L√†m theo ƒë√∫ng h∆∞·ªõng d·∫´n ·ªü System Prompt:  
    - M·ªü ƒë·∫ßu b·∫±ng c√¢u "Th√¥ng tin n√†y ch∆∞a c√≥ trong t√†i li·ªáu c·ªßa tr∆∞·ªùng, tuy nhi√™n..." (ho·∫∑c b·∫£n ti·∫øng Anh t∆∞∆°ng ƒë∆∞∆°ng)  
    - Sau ƒë√≥ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi tham kh·∫£o d·ª±a tr√™n ki·∫øn th·ª©c chung, v√† khuy·∫øn kh√≠ch ng∆∞·ªùi d√πng li√™n h·ªá ph√≤ng/ƒë∆°n v·ªã ch·ª©c nƒÉng ƒë·ªÉ x√°c nh·∫≠n.
- Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi b·∫±ng **Markdown**, s·ª≠ d·ª•ng:
  - Ti√™u ƒë·ªÅ in ƒë·∫≠m cho c√°c m·ª•c l·ªõn  
  - G·∫°ch ƒë·∫ßu d√≤ng ƒë·ªÉ li·ªát k√™  
  - B√¥i ƒë·∫≠m c√°c **L∆∞u √Ω quan tr·ªçng**, **M·ªëc th·ªùi gian**, **Ch·ªâ ti√™u**, **M√£ ng√†nh** khi c√≥.
- Lu√¥n h∆∞·ªõng t·ªõi vi·ªác t·∫°o ra m·ªôt c√¢u tr·∫£ l·ªùi **r√µ r√†ng, c√≥ h·ªá th·ªëng, d·ªÖ tra c·ª©u l·∫°i**, gi√∫p ng∆∞·ªùi d√πng c√≥ th·ªÉ d·ª±a v√†o ƒë√≥ ƒë·ªÉ th·ª±c hi·ªán c√°c b∆∞·ªõc ti·∫øp theo trong th·ª±c t·∫ø.

Tr·∫£ l·ªùi:"""

    def _rewrite_query_with_history(
        self, query: str, history: List[Dict[str, str]]
    ) -> str:
        """
        Rewrite the user's query using conversation history for better context.
        """
        if not history:
            return query

        formatted_history = "\n".join(
            [f"{msg['role']}: {msg['content']}" for msg in history]
        )

        rewrite_prompt = f"""D·ª±a v√†o l·ªãch s·ª≠ tr√≤ chuy·ªán sau ƒë√¢y, h√£y vi·∫øt l·∫°i c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh ƒë·ªÉ c√≥ th·ªÉ d√πng cho vi·ªác t√¨m ki·∫øm th√¥ng tin.

### L·ªãch s·ª≠ tr√≤ chuy·ªán:
{formatted_history}

### C√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng:
{query}

### C√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh:"""

        rewritten_query = query  # Default to original query
        try:
            log.info("Rewriting query with history...")
            if LLM_PROVIDER.lower() == "gemini":
                response = gemini_service.generate_response(
                    prompt=rewrite_prompt, temperature=0.0
                )
                if response:
                    rewritten_query = response.strip()
            elif LLM_PROVIDER.lower() == "ollama":
                response = self.ollama_service.generate_response(
                    prompt=rewrite_prompt,
                    system_prompt="B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh d·ª±a tr√™n l·ªãch s·ª≠ tr√≤ chuy·ªán.",
                    temperature=0.0,
                )
                if response:
                    rewritten_query = response.strip()

            if rewritten_query != query:
                log.info(f"Original query: '{query}'")
                log.info(f"Rewritten query: '{rewritten_query}'")
            else:
                log.info("Query does not need rewriting.")

            return rewritten_query

        except Exception as e:
            log.error(f"Error during query rewriting: {e}")
            return query  # Fallback to original query on error

    def generate_answer(
        self,
        query: str,
        conversation_id: Optional[str] = None,
        conversation_history: Optional[List[dict]] = None,
        images: Optional[List[Any]] = None,
    ) -> Dict[str, Any]:
        """
        Generate answer using RAG approach

        Args:
            query: User query
            conversation_id: Optional conversation ID
            conversation_history: Optional conversation history
            images: Optional list of images for vision analysis

        Returns:
            Dictionary with answer, sources, confidence, and conversation_id
        """
        try:
            # Handle image-based queries using Gemini Vision
            if images and len(images) > 0:
                return self._generate_vision_answer(
                    query=query,
                    images=images,
                    conversation_id=conversation_id,
                )

            # Create new conversation if needed
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
                self.conversations[conversation_id] = []
            elif conversation_id not in self.conversations:
                self.conversations[conversation_id] = []

            # Use conversation history if provided
            if conversation_history and not self.conversations[conversation_id]:
                # Convert conversation history to internal format
                for message in conversation_history:
                    # Ki·ªÉm tra xem message c√≥ ch·ª©a 'role' v√† 'content' kh√¥ng
                    if (
                        isinstance(message, dict)
                        and "role" in message
                        and "content" in message
                    ):
                        if message["role"] in ["user", "assistant"]:
                            self.conversations[conversation_id].append(
                                {"role": message["role"], "content": message["content"]}
                            )

            # Step 1: Normalize the user's question using Gemini AI
            log.info(f"Original query: {query}")
            normalized_query = normalize_question(query)
            log.info(f"Normalized query: {normalized_query}")

            # Track if normalization was applied
            normalization_applied = (
                normalized_query != query
            ) and ENABLE_GEMINI_NORMALIZATION

            # Step 1.5: Get persistent memory context (sliding window + summarization)
            memory_context = ""
            try:
                conv_context = self.memory_service.get_conversation_context(
                    conversation_id=conversation_id,
                    query=normalized_query,
                    include_memory_search=True,
                )
                if conv_context.has_long_term_memory or conv_context.recent_messages:
                    memory_context = self.memory_service.format_context_for_prompt(
                        conv_context
                    )
                    log.info(
                        f"üß† Loaded memory context: {len(conv_context.memory_summaries)} summaries, {len(conv_context.recent_messages)} recent messages"
                    )
            except Exception as mem_error:
                log.warning(f"Could not load memory context: {mem_error}")

            # Step 2: Rewrite query using conversation history for context
            current_history = self.conversations.get(conversation_id, [])
            rewritten_query = self._rewrite_query_with_history(
                normalized_query, current_history
            )

            # Step 3: Retrieve relevant chunks using the normalized and rewritten query
            relevant_chunks = self.retrieve_relevant_chunks(rewritten_query)

            # Create formatted context from chunks
            context = self.create_context(relevant_chunks)

            # Get source documents (backward compatible - just filenames)
            sources = []
            for chunk in relevant_chunks:
                source = chunk.get("source_file", "") or chunk.get("source", "")
                if source and source not in sources:
                    sources.append(source)

            # Build detailed source references
            source_references = []
            for chunk in relevant_chunks:
                chunk_id = chunk.get("chunk_id", "")
                content = chunk.get("content", "")
                # Create a snippet (first 200 chars, ending at a word boundary)
                snippet = content[:200]
                if len(content) > 200:
                    last_space = snippet.rfind(" ")
                    if last_space > 150:
                        snippet = snippet[:last_space]
                    snippet += "..."

                # Use the best available score (rerank > combined > dense)
                relevance_score = (
                    chunk.get("rerank_score")
                    or chunk.get("combined_score")
                    or chunk.get("dense_score")
                    or 0.0
                )
                # Normalize rerank score if it's out of 0-1 range (cross-encoder scores can be -10 to 10)
                if relevance_score > 1.0:
                    relevance_score = min(
                        1.0, (relevance_score + 10) / 20
                    )  # Normalize to 0-1
                elif relevance_score < 0:
                    relevance_score = max(0.0, (relevance_score + 10) / 20)

                source_ref = {
                    "chunk_id": str(chunk_id),
                    "filename": chunk.get("source_file", "") or chunk.get("source", ""),
                    "page_number": chunk.get("page_number"),
                    "heading": chunk.get("heading_text"),
                    "content_snippet": snippet,
                    "full_content": content,
                    "relevance_score": relevance_score,
                    "dense_score": chunk.get("dense_score"),
                    "sparse_score": chunk.get("sparse_score"),
                }
                source_references.append(source_ref)

            # Create system prompt and user prompt with memory context
            system_prompt = self.create_system_prompt()
            user_prompt = self.create_user_prompt(query, context, memory_context)

            # Log context and prompts for debugging
            log.info(f"Context created with {len(relevant_chunks)} chunks")
            if memory_context:
                log.info(
                    f"üß† Including memory context in prompt ({len(memory_context)} chars)"
                )
            log.debug(f"System prompt: {system_prompt[:200]}...")
            log.debug(f"Full context sent to LLM:\n{context}")
            log.debug(f"User prompt: {user_prompt[:200]}...")

            # Generate answer using the configured LLM provider
            answer = None
            if LLM_PROVIDER.lower() == "gemini":
                log.info("Calling Gemini service to generate response...")
                # Gemini API works best with a single, consolidated prompt
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                answer = gemini_service.generate_response(prompt=full_prompt)

            elif LLM_PROVIDER.lower() == "ollama":
                log.info("Calling Ollama service to generate response...")
                answer = self.ollama_service.generate_response(
                    prompt=user_prompt, system_prompt=system_prompt, temperature=0.7
                )
            else:
                log.error(f"Unsupported LLM_PROVIDER configured: {LLM_PROVIDER}")
                answer = "L·ªói: Nh√† cung c·∫•p LLM kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng."

            log.info(f"LLM response received: {answer is not None}")
            if answer:
                log.debug(f"Answer preview: {answer[:100]}...")

            # Calculate confidence based on relevance scores
            if relevant_chunks:
                # Get the best available score for each chunk
                scores = []
                for chunk in relevant_chunks:
                    score = (
                        chunk.get("rerank_score")
                        or chunk.get("combined_score")
                        or chunk.get("dense_score")
                        or 0.0
                    )
                    # Normalize if needed (cross-encoder scores can be -10 to 10)
                    if score > 1.0:
                        score = min(1.0, (score + 10) / 20)
                    elif score < 0:
                        score = max(0.0, (score + 10) / 20)
                    scores.append(score)

                avg_score = sum(scores) / len(scores)
                confidence = min(max(avg_score, 0.0), 1.0)
                log.info(
                    f"Calculated confidence: {confidence:.3f} from avg score: {avg_score:.3f}"
                )
            else:
                confidence = 0.0
                log.warning("No relevant chunks found, confidence set to 0")

            # Handle case where LLM provider returns None or empty
            if answer is None:
                log.error("LLM provider returned None response")
                answer = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.\n\nB·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!"
                confidence = 0.0
            elif not answer.strip():
                log.error("LLM provider returned empty response")
                answer = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.\n\nB·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!"
                confidence = 0.0
            else:
                log.info(f"Raw answer from LLM: {repr(answer)}")
                # Add engagement prompt if not already present
                answer = self._add_engagement_prompt(answer)
                log.info(f"Using answer with engagement prompt: {repr(answer)}")

            # Update conversation history (in-memory cache)
            self.conversations[conversation_id].append(
                {"role": "user", "content": query}
            )
            self.conversations[conversation_id].append(
                {"role": "assistant", "content": answer}
            )

            # Limit conversation history (in-memory)
            if len(self.conversations[conversation_id]) > 10:
                self.conversations[conversation_id] = self.conversations[
                    conversation_id
                ][-10:]

            # Save to persistent memory with sliding window + summarization
            try:
                self.memory_service.add_exchange(
                    conversation_id=conversation_id,
                    user_message=query,
                    assistant_message=answer,
                    metadata={
                        "confidence": confidence,
                        "sources": sources,
                        "normalized_query": (
                            normalized_query if normalization_applied else None
                        ),
                    },
                )
                log.debug(
                    f"üíæ Saved exchange to persistent memory for {conversation_id}"
                )
            except Exception as mem_error:
                log.warning(f"Could not save to persistent memory: {mem_error}")

            # Save conversation to PostgreSQL (legacy)
            processing_time = time.time() - time.time()  # Will be calculated properly
            try:
                self.db_service.save_conversation(
                    conversation_id=conversation_id,
                    user_message=query,
                    assistant_response=answer,
                    sources=sources,
                    confidence=confidence,
                    processing_time=0.0,  # Processing time will be set at API level
                )
            except Exception as save_error:
                log.warning(f"Could not save conversation to DB: {save_error}")

            # Detect if chart visualization is needed
            chart_data = self._detect_chart_request(query, answer)
            if chart_data:
                log.info(f"üìä Generated {len(chart_data)} chart(s) for visualization")

            return {
                "answer": answer,
                "sources": sources,
                "source_references": source_references,
                "confidence": confidence,
                "conversation_id": conversation_id,
                "normalization_applied": normalization_applied,
                "original_query": query if normalization_applied else None,
                "normalized_query": normalized_query if normalization_applied else None,
                "chart_data": chart_data,  # Charts for visualization
                "images": [],  # Will be populated if images are found in sources
            }

        except Exception as e:
            log.error(f"Error generating answer: {e}")
            return {
                "answer": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "sources": [],
                "source_references": [],
                "confidence": 0.0,
                "conversation_id": conversation_id or str(uuid.uuid4()),
                "chart_data": [],
                "images": [],
            }

    def get_conversation_history(self, conversation_id: str) -> List[Dict[str, Any]]:
        """
        Get conversation history

        Args:
            conversation_id: Conversation ID

        Returns:
            List of conversation exchanges
        """
        return self.conversations.get(conversation_id, [])

    def check_system_health(self) -> Dict[str, Any]:
        """
        Check health of all RAG components

        Returns:
            Health status dictionary
        """
        health_status = {"overall_status": "healthy", "components": {}}

        # Check Ollama
        ollama_health = self.ollama_service.check_health()
        health_status["components"]["ollama"] = ollama_health

        # Check PostgreSQL + pgvector
        try:
            db_stats = self.db_service.get_database_stats()
            health_status["components"]["database"] = {
                "status": "healthy",
                "stats": db_stats,
            }
        except Exception as e:
            health_status["components"]["database"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check Hybrid Retrieval Service (BM25 + pgvector)
        try:
            health_status["components"]["hybrid_retrieval"] = {
                "status": "healthy",
                "type": "BM25 + pgvector",
            }
        except Exception as e:
            health_status["components"]["hybrid_retrieval"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check embedding service
        try:
            embedding_dim = self.embedding_service.get_embedding_dimension()
            health_status["components"]["embedding"] = {
                "status": "healthy",
                "dimension": embedding_dim,
            }
        except Exception as e:
            health_status["components"]["embedding"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check ingestion service
        try:
            health_status["components"]["ingestion"] = {
                "status": "healthy",
                "type": "PDF file watcher",
            }
        except Exception as e:
            health_status["components"]["ingestion"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Determine overall status
        component_statuses = [
            comp.get("status", "unknown")
            for comp in health_status["components"].values()
        ]
        if any(status == "unhealthy" for status in component_statuses):
            health_status["overall_status"] = "unhealthy"
        elif any(status == "unknown" for status in component_statuses):
            health_status["overall_status"] = "degraded"

        return health_status

    def cleanup(self):
        """Cleanup resources - stop ingestion service"""
        try:
            if hasattr(self, "ingestion_service"):
                log.info("Stopping ingestion service...")
                self.ingestion_service.stop_watching()
                log.info("Ingestion service stopped successfully.")
        except Exception as e:
            log.error(f"Error during cleanup: {e}")
