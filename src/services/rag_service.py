"""
RAG (Retrieval-Augmented Generation) service
"""

import uuid
import re
import time
from typing import List, Dict, Any, Optional
from src.services.embedding_service import EmbeddingService
from src.services.postgres_database_service import PostgresDatabaseService
from src.services.hybrid_retrieval_service import HybridRetrievalService
from src.services.ingestion_service import IngestionService
from src.services.pdf_processor import PDFProcessor
from src.services import gemini_service
from src.services.gemini_service import normalize_question
from src.services.memory_service import ConversationMemoryService
from src.services.attachment_service import AttachmentService
from sentence_transformers import CrossEncoder
from src.services.ollama_service import OllamaService
from src.utils.logger import log

from config.settings import (
    TOP_K_RESULTS,
    LLM_PROVIDER,
    ENABLE_GEMINI_NORMALIZATION,
)


class RAGService:
    """Service for Retrieval-Augmented Generation"""

    def __init__(self, analytics_service=None):
        """Initialize RAG service with PostgreSQL + Hybrid Retrieval"""
        self.embedding_service = EmbeddingService()
        self.db_service = PostgresDatabaseService()
        self.retrieval_service = HybridRetrievalService(
            self.db_service, self.embedding_service
        )
        self.pdf_processor = PDFProcessor()

        # Import analytics service lazily to avoid circular imports
        if analytics_service is None:
            try:
                from src.services.analytics_service import AnalyticsService

                analytics_service = AnalyticsService(self.db_service)
            except Exception as e:
                log.warning(f"Could not initialize analytics service: {e}")
                analytics_service = None

        self.analytics_service = analytics_service

        self.ingestion_service = IngestionService(
            self.db_service,
            self.embedding_service,
            self.pdf_processor,
            self.retrieval_service,
            analytics_service,  # Pass analytics service for document tracking
        )
        self.ollama_service = OllamaService()

        # Initialize Memory Service for persistent conversational memory
        self.memory_service = ConversationMemoryService(
            self.db_service, self.embedding_service
        )

        # Initialize Attachment Service
        self.attachment_service = AttachmentService(self.db_service)

        # Conversation memory (in-memory cache, backed by persistent storage)
        self.conversations = {}

        # Initialize Reranker
        try:
            log.info("Initializing Reranker model...")
            self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
            log.info("Reranker model initialized successfully.")
        except Exception as e:
            log.error(f"Error initializing Reranker model: {e}")
            self.reranker = None

        # Start ingestion service
        try:
            log.info("Starting ingestion service...")
            self.ingestion_service.start_watching()
            log.info("Ingestion service started successfully.")
        except Exception as e:
            log.error(f"Error starting ingestion service: {e}")

    def _rerank_chunks(
        self, query: str, chunks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Reranks a list of chunks based on their relevance to the query using a Cross-Encoder model.
        """
        if not self.reranker or not chunks:
            return chunks

        try:
            # Create pairs of [query, chunk_content] for the reranker
            pairs = [[query, chunk["content"]] for chunk in chunks]

            # Predict the scores
            scores = self.reranker.predict(pairs)

            # Assign scores to chunks
            for chunk, score in zip(chunks, scores):
                chunk["rerank_score"] = float(score)

            # Sort chunks by the new rerank score in descending order
            chunks.sort(key=lambda x: x.get("rerank_score", 0.0), reverse=True)

            log.info(f"Reranked {len(chunks)} chunks successfully.")
            return chunks

        except Exception as e:
            log.error(f"Error during chunk reranking: {e}")
            # Return original chunks in case of an error

    def _detect_chart_request(self, query: str, answer: str) -> List[Dict[str, Any]]:
        """
        Detect if the query/answer contains statistical data that can be visualized as charts.
        Returns chart data if applicable.
        """
        chart_data = []
        query_lower = query.lower()

        # Keywords that suggest chart visualization
        chart_keywords = [
            "th·ªëng k√™",
            "bi·ªÉu ƒë·ªì",
            "so s√°nh",
            "t·ª∑ l·ªá",
            "ph·∫ßn trƒÉm",
            "%",
            "s·ªë l∆∞·ª£ng",
            "ch·ªâ ti√™u",
            "ƒëi·ªÉm chu·∫©n",
            "ƒëi·ªÉm tr√∫ng tuy·ªÉn",
            "tuy·ªÉn sinh",
            "h·ªçc vi√™n",
            "sinh vi√™n",
            "nƒÉm",
            "kh√≥a",
            "ng√†nh",
            "chart",
            "graph",
            "statistics",
        ]

        # Check if query asks for statistics/charts
        should_generate_chart = any(
            keyword in query_lower for keyword in chart_keywords
        )

        if should_generate_chart:
            # Example: Admission statistics by year
            if any(word in query_lower for word in ["tuy·ªÉn sinh", "ch·ªâ ti√™u", "nƒÉm"]):
                chart_data.append(
                    {
                        "type": "bar",
                        "title": "Ch·ªâ ti√™u tuy·ªÉn sinh qua c√°c nƒÉm",
                        "data": [
                            {"name": "2021", "Ch·ªâ ti√™u": 450, "Tr√∫ng tuy·ªÉn": 420},
                            {"name": "2022", "Ch·ªâ ti√™u": 500, "Tr√∫ng tuy·ªÉn": 480},
                            {"name": "2023", "Ch·ªâ ti√™u": 550, "Tr√∫ng tuy·ªÉn": 530},
                            {"name": "2024", "Ch·ªâ ti√™u": 600, "Tr√∫ng tuy·ªÉn": 580},
                            {"name": "2025", "Ch·ªâ ti√™u": 650, "Tr√∫ng tuy·ªÉn": 0},
                        ],
                        "xKey": "name",
                        "yKeys": ["Ch·ªâ ti√™u", "Tr√∫ng tuy·ªÉn"],
                        "description": "Bi·ªÉu ƒë·ªì th·ªëng k√™ ch·ªâ ti√™u tuy·ªÉn sinh (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

            # Example: Score distribution by major
            if any(
                word in query_lower
                for word in ["ƒëi·ªÉm chu·∫©n", "ƒëi·ªÉm tr√∫ng tuy·ªÉn", "ng√†nh"]
            ):
                chart_data.append(
                    {
                        "type": "bar",
                        "title": "ƒêi·ªÉm chu·∫©n c√°c ng√†nh nƒÉm 2024",
                        "data": [
                            {"name": "An ninh ch√≠nh tr·ªã", "ƒêi·ªÉm chu·∫©n": 24.5},
                            {"name": "An ninh kinh t·∫ø", "ƒêi·ªÉm chu·∫©n": 25.0},
                            {"name": "An ninh m·∫°ng", "ƒêi·ªÉm chu·∫©n": 26.5},
                            {"name": "ƒêi·ªÅu tra h√¨nh s·ª±", "ƒêi·ªÉm chu·∫©n": 25.5},
                            {"name": "K·ªπ thu·∫≠t h√¨nh s·ª±", "ƒêi·ªÉm chu·∫©n": 24.0},
                        ],
                        "xKey": "name",
                        "yKeys": ["ƒêi·ªÉm chu·∫©n"],
                        "description": "Bi·ªÉu ƒë·ªì ƒëi·ªÉm chu·∫©n c√°c ng√†nh (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

            # Example: Student distribution by major (pie chart)
            if any(word in query_lower for word in ["t·ª∑ l·ªá", "ph√¢n b·ªë", "c∆° c·∫•u"]):
                chart_data.append(
                    {
                        "type": "pie",
                        "title": "T·ª∑ l·ªá h·ªçc vi√™n theo ng√†nh ƒë√†o t·∫°o",
                        "data": [
                            {"name": "An ninh ch√≠nh tr·ªã", "value": 25},
                            {"name": "An ninh kinh t·∫ø", "value": 20},
                            {"name": "An ninh m·∫°ng", "value": 30},
                            {"name": "ƒêi·ªÅu tra h√¨nh s·ª±", "value": 15},
                            {"name": "K·ªπ thu·∫≠t h√¨nh s·ª±", "value": 10},
                        ],
                        "xKey": "name",
                        "yKeys": ["value"],
                        "description": "Bi·ªÉu ƒë·ªì t·ª∑ l·ªá h·ªçc vi√™n theo ng√†nh (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

            # Example: Trend over time (line chart)
            if any(
                word in query_lower
                for word in ["xu h∆∞·ªõng", "trend", "bi·∫øn ƒë·ªông", "qua c√°c nƒÉm"]
            ):
                chart_data.append(
                    {
                        "type": "line",
                        "title": "Xu h∆∞·ªõng s·ªë l∆∞·ª£ng h·ªì s∆° ƒëƒÉng k√Ω qua c√°c nƒÉm",
                        "data": [
                            {"name": "2020", "H·ªì s∆°": 1200},
                            {"name": "2021", "H·ªì s∆°": 1450},
                            {"name": "2022", "H·ªì s∆°": 1680},
                            {"name": "2023", "H·ªì s∆°": 1920},
                            {"name": "2024", "H·ªì s∆°": 2150},
                        ],
                        "xKey": "name",
                        "yKeys": ["H·ªì s∆°"],
                        "description": "Bi·ªÉu ƒë·ªì xu h∆∞·ªõng s·ªë l∆∞·ª£ng h·ªì s∆° (D·ªØ li·ªáu minh h·ªça)",
                    }
                )

        return chart_data

    def retrieve_relevant_chunks(
        self, query: str, top_k: int = TOP_K_RESULTS
    ) -> List[Dict[str, Any]]:
        """Retrieve relevant chunks using hybrid retrieval (dense + sparse search)."""
        try:
            # Generate query embedding
            query_embedding = self.embedding_service.create_embedding(query)

            # Perform hybrid search using PostgreSQL + pgvector
            initial_k = max(top_k * 3, 15)  # Get more candidates for reranking
            hybrid_results = self.retrieval_service.hybrid_search(
                query=query, query_embedding=query_embedding, top_k=initial_k
            )

            if not hybrid_results:
                log.warning(f"No chunks found for query: {query}")
                return []

            log.info(f"Hybrid search found {len(hybrid_results)} chunks.")

            # Rerank results
            reranked_chunks = self._rerank_chunks(query, hybrid_results)

            # Context expansion
            expanded_chunks = self._expand_context(reranked_chunks[:top_k], query)

            # Final ranking
            final_chunks = self._final_ranking(expanded_chunks, query)
            return final_chunks[:top_k]

        except Exception as e:
            log.error(f"Error during retrieve_relevant_chunks: {e}")
            return []

    def _extract_heading_from_content(self, content: str) -> Optional[str]:
        """
        Extract heading from chunk content

        Args:
            content: Chunk content

        Returns:
            Heading if found, None otherwise
        """
        # Try to extract heading from first line
        lines = content.strip().split("\n")
        if not lines:
            return None

        first_line = lines[0].strip()

        # Check if first line matches heading pattern
        heading_patterns = [
            r"^\s*(\d+)\.\s+(.+)$",
            r"^\s*(\d+\.\d+)\.\s+(.+)$",
            r"^\s*(\d+\.\d+\.\d+)\.\s+(.+)$",
        ]

        for pattern in heading_patterns:
            match = re.match(pattern, first_line)
            if match:
                return first_line

        return None

    def _expand_context(
        self, chunks: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Expand context by adding related chunks from the same document/section

        Args:
            chunks: Initial retrieved chunks
            query: User query for relevance checking

        Returns:
            Expanded list of chunks with additional context
        """
        if not chunks:
            return chunks

        expanded_chunks = chunks.copy()

        try:
            # Group chunks by source file
            chunks_by_source = {}
            for chunk in chunks:
                source = chunk.get("source_file", "")
                if source not in chunks_by_source:
                    chunks_by_source[source] = []
                chunks_by_source[source].append(chunk)

            # For each source file, try to find adjacent chunks
            for source_file, source_chunks in chunks_by_source.items():
                for chunk in source_chunks:
                    chunk_index = chunk.get("chunk_index", -1)
                    page_number = chunk.get("page_number", -1)

                    if chunk_index >= 0:
                        # Look for adjacent chunks (before and after)
                        for offset in [-1, 1]:
                            adjacent_chunk = self._get_adjacent_chunk(
                                source_file, chunk_index + offset, page_number
                            )
                            if adjacent_chunk and adjacent_chunk["id"] not in [
                                c["id"] for c in expanded_chunks
                            ]:
                                # Check if adjacent chunk is somewhat relevant
                                if self._is_chunk_relevant(adjacent_chunk, query):
                                    adjacent_chunk["context_expansion"] = True
                                    adjacent_chunk["hybrid_score"] = (
                                        chunk.get("hybrid_score", 0.0) * 0.7
                                    )  # Lower score for context
                                    expanded_chunks.append(adjacent_chunk)

            log.info(
                f"Context expansion added {len(expanded_chunks) - len(chunks)} additional chunks"
            )
            return expanded_chunks

        except Exception as e:
            log.error(f"Error during context expansion: {e}")
            return chunks

    def _get_adjacent_chunk(
        self, source_file: str, chunk_index: int, page_number: int
    ) -> Optional[Dict[str, Any]]:
        """Get adjacent chunk by source file and chunk index using PostgreSQL"""
        try:
            chunk = self.db_service.get_chunk_by_source_and_index(
                source_file, chunk_index
            )
            return chunk
        except Exception as e:
            log.error(f"Error getting adjacent chunk: {e}")
            return None

    def _is_chunk_relevant(self, chunk: Dict[str, Any], query: str) -> bool:
        """Check if a chunk is relevant to the query using simple keyword matching"""
        try:
            content = chunk.get("content", "").lower()
            query_lower = query.lower()

            # Simple keyword overlap check
            query_words = set(query_lower.split())
            content_words = set(content.split())

            # Calculate overlap ratio
            overlap = len(query_words.intersection(content_words))
            overlap_ratio = overlap / len(query_words) if query_words else 0

            # Consider relevant if there's at least 20% keyword overlap
            return overlap_ratio >= 0.2

        except Exception as e:
            log.error(f"Error checking chunk relevance: {e}")
            return False

    def _final_ranking(
        self, chunks: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Final ranking of chunks considering multiple factors

        Args:
            chunks: List of chunks to rank
            query: User query

        Returns:
            Ranked list of chunks
        """
        try:
            # Sort by multiple criteria
            def ranking_score(chunk):
                # Primary: rerank_score if available, otherwise hybrid_score
                primary_score = chunk.get(
                    "rerank_score", chunk.get("hybrid_score", 0.0)
                )

                # Bonus for chunks with headings (likely more structured content)
                heading_bonus = 0.1 if chunk.get("heading_text") else 0.0

                # Bonus for chunks that are not context expansions (original results)
                original_bonus = (
                    0.05 if not chunk.get("context_expansion", False) else 0.0
                )

                # Penalty for very short chunks (likely less informative)
                content_length = len(chunk.get("content", ""))
                length_penalty = -0.1 if content_length < 100 else 0.0

                return primary_score + heading_bonus + original_bonus + length_penalty

            ranked_chunks = sorted(chunks, key=ranking_score, reverse=True)

            log.info(f"Final ranking completed for {len(ranked_chunks)} chunks")
            return ranked_chunks

        except Exception as e:
            log.error(f"Error during final ranking: {e}")
            return chunks

    def _generate_vision_answer(
        self,
        query: str,
        images: List[Any],
        conversation_id: Optional[str] = None,
        language: str = "vi",  # Add language parameter
    ) -> Dict[str, Any]:
        """
        Generate answer for image-based queries using Gemini Vision.

        Args:
            query: User's question about the image(s)
            images: List of ImageInput objects with base64 encoded images
            conversation_id: Optional conversation ID
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            Dictionary with answer, confidence, and conversation_id
        """
        try:
            log.info(
                f"Processing vision query with {len(images)} images, language={language}"
            )

            # Create conversation ID if needed
            if not conversation_id:
                conversation_id = str(uuid.uuid4())

            # Build the vision prompt based on language
            if language == "en":
                vision_prompt = f"""You are an AI assistant specializing in supporting information about People's Security University (PSU).
Please analyze the provided image(s) and answer the user's question in ENGLISH.

User's question: {query if query else "Please describe the content of this image."}

Instructions:
- Analyze the image content carefully
- Respond ENTIRELY in ENGLISH
- If the image contains documents or text (which may be in Vietnamese), translate and explain the content in English
- If it's a data table, summarize the important information in English
- Provide a detailed, easy-to-understand answer in English"""
            else:
                vision_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n h·ªó tr·ª£ v·ªÅ Tr∆∞·ªùng ƒê·∫°i h·ªçc An ninh Nh√¢n d√¢n.
H√£y ph√¢n t√≠ch h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query if query else "H√£y m√¥ t·∫£ n·ªôi dung trong h√¨nh ·∫£nh n√†y."}

H∆∞·ªõng d·∫´n:
- Ph√¢n t√≠ch k·ªπ n·ªôi dung trong h√¨nh ·∫£nh
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- N·∫øu h√¨nh ·∫£nh li√™n quan ƒë·∫øn t√†i li·ªáu, vƒÉn b·∫£n, h√£y tr√≠ch d·∫´n v√† gi·∫£i th√≠ch n·ªôi dung
- N·∫øu l√† b·∫£ng s·ªë li·ªáu, h√£y t√≥m t·∫Øt th√¥ng tin quan tr·ªçng
- ƒê∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt, d·ªÖ hi·ªÉu"""

            # Prepare image data for Gemini
            image_parts = []
            for img in images:
                try:
                    # Get base64 data (remove data:image/xxx;base64, prefix if present)
                    base64_data = img.base64
                    if "," in base64_data:
                        base64_data = base64_data.split(",")[1]

                    # Determine mime type
                    mime_type = getattr(img, "mime_type", "image/jpeg")
                    if not mime_type:
                        mime_type = "image/jpeg"

                    image_parts.append({"mime_type": mime_type, "data": base64_data})
                    log.info(
                        f"Processed image: {getattr(img, 'name', 'unknown')} ({mime_type})"
                    )
                except Exception as img_error:
                    log.error(f"Error processing image: {img_error}")
                    continue

            if not image_parts:
                error_msg = (
                    "Sorry, unable to process the image. Please try again with a different format (PNG, JPG, WebP)."
                    if language == "en"
                    else "Xin l·ªói, kh√¥ng th·ªÉ x·ª≠ l√Ω h√¨nh ·∫£nh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi ƒë·ªãnh d·∫°ng ·∫£nh kh√°c (PNG, JPG, WebP)."
                )
                return {
                    "answer": error_msg,
                    "sources": [],
                    "source_references": [],
                    "confidence": 0.0,
                    "conversation_id": conversation_id,
                }

            # Call Gemini Vision
            answer = gemini_service.generate_vision_response(
                prompt=vision_prompt, images=image_parts
            )

            if not answer:
                answer = (
                    "Sorry, I couldn't analyze this image. Please try again or describe more about what you want to ask."
                    if language == "en"
                    else "Xin l·ªói, t√¥i kh√¥ng th·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh n√†y. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c m√¥ t·∫£ th√™m v·ªÅ n·ªôi dung b·∫°n mu·ªën h·ªèi."
                )
            else:
                # Add engagement prompt
                answer = self._add_engagement_prompt(answer)

            log.info("Vision response generated successfully")

            return {
                "answer": answer,
                "sources": [],
                "source_references": [],
                "confidence": 0.85,  # Default confidence for vision queries
                "conversation_id": conversation_id,
                "chart_data": [],
                "images": [],
            }

        except Exception as e:
            log.error(f"Error in vision query processing: {e}")
            return {
                "answer": f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi ph√¢n t√≠ch h√¨nh ·∫£nh: {str(e)}. Vui l√≤ng th·ª≠ l·∫°i.",
                "sources": [],
                "source_references": [],
                "confidence": 0.0,
                "conversation_id": conversation_id or str(uuid.uuid4()),
            }

    def _add_engagement_prompt(self, answer: str) -> str:
        """
        Add engagement prompt to the answer if not already present

        Args:
            answer: The generated answer

        Returns:
            Answer with engagement prompt added
        """
        engagement_prompts = [
            "B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!",
            "b·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng",
            "t√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m",
            "c√≥ c√¢u h·ªèi n√†o kh√°c kh√¥ng",
            "c·∫ßn h·ªó tr·ª£ th√™m g√¨ kh√¥ng",
        ]

        # Check if any engagement prompt is already present (case insensitive)
        answer_lower = answer.lower()
        has_engagement = any(
            prompt.lower() in answer_lower for prompt in engagement_prompts
        )

        if not has_engagement:
            # Add the engagement prompt with proper formatting
            if (
                answer.strip().endswith(".")
                or answer.strip().endswith("!")
                or answer.strip().endswith("?")
            ):
                return f"{answer}\n\n**B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!**"
            else:
                return f"{answer}.\n\n**B·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!**"

        return answer

    def create_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        Create a simplified and clean context string from retrieved chunks.
        """
        if not chunks:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."

        context_parts = []
        for chunk in chunks:
            source = chunk.get("source_file", "Unknown")
            page = chunk.get("page_number", "N/A")
            content = chunk.get("content", "").strip()

            # Simplified format for the LLM - removed source citation from content
            context_part = f"###\n{content}\n###"
            context_parts.append(context_part)

        return "\n\n".join(context_parts)

    def create_system_prompt(self, language: str = "vi") -> str:
        """
        Create system prompt for the chatbot

        Args:
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            System prompt string
        """
        # Language-specific instructions
        if language == "en":
            language_instruction = """
**IMPORTANT - RESPONSE LANGUAGE: ENGLISH**
You MUST respond ENTIRELY in ENGLISH. This is a strict requirement from the user who has selected English as their preferred language.
- Translate ALL content to English, including explanations, instructions, and summaries.
- You may keep Vietnamese proper nouns (names of schools, documents, regulations) in their original form when necessary.
- All headings, bullet points, and explanations must be in English.
"""
        else:
            language_instruction = """
**NG√îN NG·ªÆ TR·∫¢ L·ªúI: TI·∫æNG VI·ªÜT**
B·∫°n PH·∫¢I tr·∫£ l·ªùi ho√†n to√†n b·∫±ng TI·∫æNG VI·ªÜT.
"""

        return f"""{language_instruction}

B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n h·ªó tr·ª£ sinh vi√™n, c√°n b·ªô, chi·∫øn sƒ© v√† ng∆∞·ªùi quan t√¢m v·ªÅ **Tr∆∞·ªùng ƒê·∫°i h·ªçc An ninh Nh√¢n d√¢n (ANND)** / **People's Security University (PSU)**.

**Ph·∫°m vi chuy√™n m√¥n ch√≠nh c·ªßa b·∫°n g·ªìm 5 nh√≥m n·ªôi dung:**
1. **T∆∞ v·∫•n th√¥ng tin tuy·ªÉn sinh / Admission Information**  
   - ƒêi·ªÅu ki·ªán, ch·ªâ ti√™u, ph∆∞∆°ng th·ª©c, h·ªì s∆°, l·ªãch tr√¨nh, ph√¢n v√πng tuy·ªÉn sinh...
2. **Quy ch·∫ø qu·∫£n l√Ω h·ªçc vi√™n / Student Management Regulations**  
   - Quy·ªÅn v√† nghƒ©a v·ª•, ch·∫ø ƒë·ªô ch√≠nh s√°ch, khen th∆∞·ªüng ‚Äì k·ª∑ lu·∫≠t, sinh ho·∫°t, r√®n luy·ªán...
3. **Quy ch·∫ø ƒë√†o t·∫°o c√°c tr√¨nh ƒë·ªô / Training Regulations**  
   - Ng√†nh/chuy√™n ng√†nh, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, h·ªçc ch·∫ø, h·ªçc l·∫°i, th√¥i h·ªçc, t·ªët nghi·ªáp...
4. **Quy ƒë·ªãnh v·ªÅ thi, ki·ªÉm tra, ƒë√°nh gi√° / Examination and Assessment Rules**  
   - H√¨nh th·ª©c thi/ki·ªÉm tra, thang ƒëi·ªÉm, ƒëi·ªÅu ki·ªán d·ª± thi, ph√∫c kh·∫£o, b·∫£o l∆∞u...
5. **Quy ƒë·ªãnh v·ªÅ ki·ªÉm ƒë·ªãnh v√† b·∫£o ƒë·∫£m ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o / Quality Assurance**  
   - Ti√™u chu·∫©n, quy tr√¨nh, ho·∫°t ƒë·ªông b·∫£o ƒë·∫£m v√† n√¢ng cao ch·∫•t l∆∞·ª£ng ƒë√†o t·∫°o...

---

### 1. Phong c√°ch & ng√¥n ng·ªØ tr·∫£ l·ªùi / Response Style & Language

{"- **ALWAYS respond in ENGLISH** as the user has selected English language preference." if language == "en" else "- **LU√îN tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT** v√¨ ng∆∞·ªùi d√πng ƒë√£ ch·ªçn ng√¥n ng·ªØ Ti·∫øng Vi·ªát."}
- VƒÉn phong / Style:
  - **Th√¢n thi·ªán, d·ªÖ hi·ªÉu nh∆∞ng v·∫´n trang tr·ªçng / Friendly but formal**
  - H·∫°n ch·∫ø l·∫∑p l·∫°i nguy√™n vƒÉn; **t√≥m t·∫Øt, g·∫°ch ƒë·∫ßu d√≤ng, chia m·ª•c r√µ r√†ng / Use summaries and bullet points**

---

### 2. C√°ch tr√¨nh b√†y m·ªôt c√¢u tr·∫£ l·ªùi / Answer Structure

1. **Ph·∫ßn m·ªü ƒë·∫ßu ‚Äì T√ìM T·∫ÆT NHANH / Opening - Quick Summary (3‚Äì5 lines)**  
   - V·∫•n ƒë·ªÅ ƒëang ƒë∆∞·ª£c h·ªèi / The topic being asked
   - ƒê·ªëi t∆∞·ª£ng √°p d·ª•ng / Who this applies to
   - M·ªëc th·ªùi gian ho·∫∑c √Ω ch√≠nh / Key dates or main points

2. **Ph·∫ßn n·ªôi dung chi ti·∫øt ‚Äì TR√åNH B√ÄY C√ì C·∫§U TR√öC / Detailed Content**  
   - S·ª≠ d·ª•ng ti√™u ƒë·ªÅ, g·∫°ch ƒë·∫ßu d√≤ng r√µ r√†ng / Use clear headings and bullets

3. **K·∫æT TH√öC b·∫±ng c√¢u nh·∫Øc v·ªÅ t√†i li·ªáu tham kh·∫£o / End with reference reminder (REQUIRED)**  
   {"- English: 'üìÑ **Reference Documents:** For full details and original documents, please refer to the attachments displayed below by the system.'" if language == "en" else "- Ti·∫øng Vi·ªát: 'üìÑ **T√†i li·ªáu tham kh·∫£o:** Th√¥ng tin chi ti·∫øt v√† to√†n vƒÉn vƒÉn b·∫£n, b·∫°n c√≥ th·ªÉ xem th√™m ·ªü ph·∫ßn t√†i li·ªáu/th√¥ng b√°o k√®m theo m√† h·ªá th·ªëng ƒë√£ hi·ªÉn th·ªã b√™n d∆∞·ªõi.'"}

---

### 3. ∆Øu ti√™n t√†i li·ªáu ch√≠nh th·ª©c / Prioritize Official Documents

- **Lu√¥n ∆∞u ti√™n th√¥ng tin trong ph·∫ßn "TH√îNG TIN T√ÄI LI·ªÜU"** / Always prioritize information from the provided documents.
- {"Translate and explain Vietnamese documents in English for the user." if language == "en" else "C√≥ th·ªÉ di·ªÖn ƒë·∫°t l·∫°i, t√≥m t·∫Øt ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ hi·ªÉu h∆°n."}

---

### 4. Khi thi·∫øu th√¥ng tin / When Information is Missing

{"1. Start with: '**This information is not explicitly available in the provided university documents, however I can share some general reference information as follows:**'" if language == "en" else "1. M·ªü ƒë·∫ßu b·∫±ng: '**Th√¥ng tin n√†y ch∆∞a c√≥ trong t√†i li·ªáu c·ªßa tr∆∞·ªùng, tuy nhi√™n t√¥i c√≥ th·ªÉ cung c·∫•p cho b·∫°n m·ªôt s·ªë th√¥ng tin tham kh·∫£o chung nh∆∞ sau:**'"}
2. {"Provide general knowledge and recommend contacting the relevant department." if language == "en" else "D·ª±a tr√™n ki·∫øn th·ª©c chung v√† khuy·∫øn kh√≠ch li√™n h·ªá ƒë∆°n v·ªã ch·ª©c nƒÉng."}

---

### 5. Y√™u c·∫ßu ƒë·ªãnh d·∫°ng / Formatting (Markdown)

- **Ti√™u ƒë·ªÅ ch√≠nh / Main headings:** d√πng `**Ti√™u ƒë·ªÅ**`
- **Danh s√°ch / Lists:** d√πng `- ` ho·∫∑c `1. `
- **Th√¥ng tin quan tr·ªçng / Important info:** d√πng `**L∆∞u √Ω quan tr·ªçng:**` ho·∫∑c `**Important:**`
- **Kh√¥ng ch√®n tr√≠ch d·∫´n ngu·ªìn d·∫°ng [1], [2]...** / No citation numbers needed

---

### 6. Y√™u c·∫ßu chung quan tr·ªçng / Important General Requirements

- Lu√¥n cung c·∫•p **c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, chi ti·∫øt v√† h·ªØu √≠ch nh·∫•t** / Always provide complete, detailed, and helpful answers.
- **T·ªïng h·ª£p, h·ªá th·ªëng h√≥a** th√¥ng tin / Synthesize and organize information.
- **{"REMEMBER: ALL responses must be in ENGLISH" if language == "en" else "NH·ªö: T·∫•t c·∫£ c√¢u tr·∫£ l·ªùi ph·∫£i b·∫±ng TI·∫æNG VI·ªÜT"}**"""

    def create_user_prompt(
        self, query: str, context: str, memory_context: str = "", language: str = "vi"
    ) -> str:
        """
        Create user prompt with query, context, and memory

        Args:
            query: User query
            context: Retrieved context from documents
            memory_context: Conversation memory context (optional)
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            Formatted user prompt
        """
        memory_section = ""
        if memory_context:
            memory_section = f"""
NG·ªÆ C·∫¢NH H·ªòI THO·∫†I TR∆Ø·ªöC / PREVIOUS CONVERSATION CONTEXT:
{memory_context}

"""

        # Language-specific instructions
        if language == "en":
            lang_instruction = """**LANGUAGE REQUIREMENT: ENGLISH**
You MUST respond ENTIRELY in ENGLISH. The user has selected English as their preferred language.
- Translate ALL content to English, including explanations, instructions, and summaries.
- You may keep Vietnamese proper nouns (names of schools, documents, regulations) in their original form when necessary.
- All headings, bullet points, and explanations MUST be in English."""
            ending_note = "üìÑ **Reference Documents:** For full details and original documents, please refer to the attachments displayed below by the system."
        else:
            lang_instruction = """**Y√äU C·∫¶U V·ªÄ NG√îN NG·ªÆ: TI·∫æNG VI·ªÜT**
B·∫°n PH·∫¢I tr·∫£ l·ªùi ho√†n to√†n b·∫±ng TI·∫æNG VI·ªÜT. Ng∆∞·ªùi d√πng ƒë√£ ch·ªçn Ti·∫øng Vi·ªát l√†m ng√¥n ng·ªØ ∆∞a th√≠ch."""
            ending_note = "üìÑ **T√†i li·ªáu tham kh·∫£o:** Th√¥ng tin chi ti·∫øt v√† to√†n vƒÉn vƒÉn b·∫£n, b·∫°n c√≥ th·ªÉ xem th√™m ·ªü ph·∫ßn t√†i li·ªáu/th√¥ng b√°o k√®m theo m√† h·ªá th·ªëng ƒë√£ hi·ªÉn th·ªã b√™n d∆∞·ªõi."

        return f"""D·ª±a tr√™n th√¥ng tin t√†i li·ªáu sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch **CHI TI·∫æT, TO√ÄN DI·ªÜN v√† CH√çNH X√ÅC** nh·∫•t c√≥ th·ªÉ.

{lang_instruction}

{memory_section}TH√îNG TIN T√ÄI LI·ªÜU / DOCUMENT INFORMATION (c√°c th√¥ng b√°o/quy ch·∫ø/t√†i li·ªáu ch√≠nh th·ª©c):
{context}

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG / USER QUESTION:
{query}

**H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI / RESPONSE GUIDELINES:**
- {"Respond ENTIRELY in ENGLISH." if language == "en" else "Tr·∫£ l·ªùi ho√†n to√†n b·∫±ng TI·∫æNG VI·ªÜT."}
- **B·∫ÆT ƒê·∫¶U / START** v·ªõi **T√ìM T·∫ÆT NG·∫ÆN / BRIEF SUMMARY (3‚Äì5 points)**
- Sau ƒë√≥ tr√¨nh b√†y **CHI TI·∫æT, C√ì C·∫§U TR√öC / DETAILED & STRUCTURED**
- **B·∫ÆT BU·ªòC K·∫æT TH√öC / MUST END** v·ªõi: "{ending_note}"
- **KH√îNG** k·∫øt th√∫c b·∫±ng c√¢u h·ªèi kh√°c / Do NOT end with another question
- Tr√¨nh b√†y b·∫±ng **Markdown** v·ªõi ti√™u ƒë·ªÅ, g·∫°ch ƒë·∫ßu d√≤ng / Use Markdown formatting

{"**IMPORTANT: ALL text must be in ENGLISH (except proper nouns).**" if language == "en" else ""}

Tr·∫£ l·ªùi / Response:"""

    def _rewrite_query_with_history(
        self, query: str, history: List[Dict[str, str]]
    ) -> str:
        """
        Rewrite the user's query using conversation history for better context.
        """
        if not history:
            return query

        formatted_history = "\n".join(
            [f"{msg['role']}: {msg['content']}" for msg in history]
        )

        rewrite_prompt = f"""D·ª±a v√†o l·ªãch s·ª≠ tr√≤ chuy·ªán sau ƒë√¢y, h√£y vi·∫øt l·∫°i c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh ƒë·ªÉ c√≥ th·ªÉ d√πng cho vi·ªác t√¨m ki·∫øm th√¥ng tin.

### L·ªãch s·ª≠ tr√≤ chuy·ªán:
{formatted_history}

### C√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng:
{query}

### C√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh:"""

        rewritten_query = query  # Default to original query
        try:
            log.info("Rewriting query with history...")
            if LLM_PROVIDER.lower() == "gemini":
                response = gemini_service.generate_response(
                    prompt=rewrite_prompt, temperature=0.0
                )
                if response:
                    rewritten_query = response.strip()
            elif LLM_PROVIDER.lower() == "ollama":
                response = self.ollama_service.generate_response(
                    prompt=rewrite_prompt,
                    system_prompt="B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n vi·∫øt l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng th√†nh m·ªôt c√¢u h·ªèi ƒë·∫ßy ƒë·ªß ng·ªØ c·∫£nh d·ª±a tr√™n l·ªãch s·ª≠ tr√≤ chuy·ªán.",
                    temperature=0.0,
                )
                if response:
                    rewritten_query = response.strip()

            if rewritten_query != query:
                log.info(f"Original query: '{query}'")
                log.info(f"Rewritten query: '{rewritten_query}'")
            else:
                log.info("Query does not need rewriting.")

            return rewritten_query

        except Exception as e:
            log.error(f"Error during query rewriting: {e}")
            return query  # Fallback to original query on error

    def generate_answer(
        self,
        query: str,
        conversation_id: Optional[str] = None,
        conversation_history: Optional[List[dict]] = None,
        images: Optional[List[Any]] = None,
        language: str = "vi",  # Add language parameter
    ) -> Dict[str, Any]:
        """
        Generate answer using RAG approach

        Args:
            query: User query
            conversation_id: Optional conversation ID
            conversation_history: Optional conversation history
            images: Optional list of images for vision analysis
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            Dictionary with answer, sources, confidence, and conversation_id
        """
        try:
            # Handle image-based queries using Gemini Vision
            if images and len(images) > 0:
                return self._generate_vision_answer(
                    query=query,
                    images=images,
                    conversation_id=conversation_id,
                    language=language,  # Pass language to vision handler
                )

            # Create new conversation if needed
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
                self.conversations[conversation_id] = []
            elif conversation_id not in self.conversations:
                self.conversations[conversation_id] = []

            # Use conversation history if provided
            if conversation_history and not self.conversations[conversation_id]:
                # Convert conversation history to internal format
                for message in conversation_history:
                    # Ki·ªÉm tra xem message c√≥ ch·ª©a 'role' v√† 'content' kh√¥ng
                    if (
                        isinstance(message, dict)
                        and "role" in message
                        and "content" in message
                    ):
                        if message["role"] in ["user", "assistant"]:
                            self.conversations[conversation_id].append(
                                {"role": message["role"], "content": message["content"]}
                            )

            # Step 1: Normalize the user's question using Gemini AI
            log.info(f"Original query: {query}")
            normalized_query = normalize_question(query)
            log.info(f"Normalized query: {normalized_query}")

            # Track if normalization was applied
            normalization_applied = (
                normalized_query != query
            ) and ENABLE_GEMINI_NORMALIZATION

            # Step 1.5: Get persistent memory context (sliding window + summarization)
            memory_context = ""
            try:
                conv_context = self.memory_service.get_conversation_context(
                    conversation_id=conversation_id,
                    query=normalized_query,
                    include_memory_search=True,
                )
                if conv_context.has_long_term_memory or conv_context.recent_messages:
                    memory_context = self.memory_service.format_context_for_prompt(
                        conv_context
                    )
                    log.info(
                        f"üß† Loaded memory context: {len(conv_context.memory_summaries)} summaries, {len(conv_context.recent_messages)} recent messages"
                    )
            except Exception as mem_error:
                log.warning(f"Could not load memory context: {mem_error}")

            # Step 2: Rewrite query using conversation history for context
            current_history = self.conversations.get(conversation_id, [])
            rewritten_query = self._rewrite_query_with_history(
                normalized_query, current_history
            )

            # Step 3: Retrieve relevant chunks using the normalized and rewritten query
            relevant_chunks = self.retrieve_relevant_chunks(rewritten_query)

            # Create formatted context from chunks
            context = self.create_context(relevant_chunks)

            # Get source documents (backward compatible - just filenames)
            sources = []
            for chunk in relevant_chunks:
                source = chunk.get("source_file", "") or chunk.get("source", "")
                if source and source not in sources:
                    sources.append(source)

            # Build detailed source references
            source_references = []
            for chunk in relevant_chunks:
                chunk_id = chunk.get("chunk_id", "")
                content = chunk.get("content", "")
                # Create a snippet (first 200 chars, ending at a word boundary)
                snippet = content[:200]
                if len(content) > 200:
                    last_space = snippet.rfind(" ")
                    if last_space > 150:
                        snippet = snippet[:last_space]
                    snippet += "..."

                # Use the best available score (rerank > combined > dense)
                relevance_score = (
                    chunk.get("rerank_score")
                    or chunk.get("combined_score")
                    or chunk.get("dense_score")
                    or 0.0
                )
                # Normalize rerank score if it's out of 0-1 range (cross-encoder scores can be -10 to 10)
                if relevance_score > 1.0:
                    relevance_score = min(
                        1.0, (relevance_score + 10) / 20
                    )  # Normalize to 0-1
                elif relevance_score < 0:
                    relevance_score = max(0.0, (relevance_score + 10) / 20)

                source_ref = {
                    "chunk_id": str(chunk_id),
                    "filename": chunk.get("source_file", "") or chunk.get("source", ""),
                    "page_number": chunk.get("page_number"),
                    "heading": chunk.get("heading_text"),
                    "content_snippet": snippet,
                    "full_content": content,
                    "relevance_score": relevance_score,
                    "dense_score": chunk.get("dense_score"),
                    "sparse_score": chunk.get("sparse_score"),
                }
                source_references.append(source_ref)

            # Create system prompt and user prompt with memory context
            system_prompt = self.create_system_prompt(language=language)
            user_prompt = self.create_user_prompt(
                query, context, memory_context, language=language
            )

            # Log context and prompts for debugging
            log.info(f"Context created with {len(relevant_chunks)} chunks")
            if memory_context:
                log.info(
                    f"üß† Including memory context in prompt ({len(memory_context)} chars)"
                )
            log.debug(f"System prompt: {system_prompt[:200]}...")
            log.debug(f"Full context sent to LLM:\n{context}")
            log.debug(f"User prompt: {user_prompt[:200]}...")

            # Generate answer using the configured LLM provider
            answer = None
            if LLM_PROVIDER.lower() == "gemini":
                log.info("Calling Gemini service to generate response...")
                # Gemini API works best with a single, consolidated prompt
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                answer = gemini_service.generate_response(prompt=full_prompt)

            elif LLM_PROVIDER.lower() == "ollama":
                log.info("Calling Ollama service to generate response...")
                answer = self.ollama_service.generate_response(
                    prompt=user_prompt, system_prompt=system_prompt, temperature=0.7
                )
            else:
                log.error(f"Unsupported LLM_PROVIDER configured: {LLM_PROVIDER}")
                answer = "L·ªói: Nh√† cung c·∫•p LLM kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng."

            log.info(f"LLM response received: {answer is not None}")
            if answer:
                log.debug(f"Answer preview: {answer[:100]}...")

            # Calculate confidence based on relevance scores
            if relevant_chunks:
                # Get the best available score for each chunk
                scores = []
                for chunk in relevant_chunks:
                    score = (
                        chunk.get("rerank_score")
                        or chunk.get("combined_score")
                        or chunk.get("dense_score")
                        or 0.0
                    )
                    # Normalize if needed (cross-encoder scores can be -10 to 10)
                    if score > 1.0:
                        score = min(1.0, (score + 10) / 20)
                    elif score < 0:
                        score = max(0.0, (score + 10) / 20)
                    scores.append(score)

                avg_score = sum(scores) / len(scores)
                confidence = min(max(avg_score, 0.0), 1.0)
                log.info(
                    f"Calculated confidence: {confidence:.3f} from avg score: {avg_score:.3f}"
                )
            else:
                confidence = 0.0
                log.warning("No relevant chunks found, confidence set to 0")

            # Handle case where LLM provider returns None or empty
            if answer is None:
                log.error("LLM provider returned None response")
                answer = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.\n\nB·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!"
                confidence = 0.0
            elif not answer.strip():
                log.error("LLM provider returned empty response")
                answer = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.\n\nB·∫°n c√≤n c√≥ th·∫Øc m·∫Øc g√¨ kh√°c kh√¥ng? T√¥i s·∫µn s√†ng h·ªó tr·ª£ th√™m!"
                confidence = 0.0
            else:
                log.info(f"Raw answer from LLM: {repr(answer)}")
                # Add engagement prompt if not already present
                answer = self._add_engagement_prompt(answer)
                log.info(f"Using answer with engagement prompt: {repr(answer)}")

            # Update conversation history (in-memory cache)
            self.conversations[conversation_id].append(
                {"role": "user", "content": query}
            )
            self.conversations[conversation_id].append(
                {"role": "assistant", "content": answer}
            )

            # Limit conversation history (in-memory)
            if len(self.conversations[conversation_id]) > 10:
                self.conversations[conversation_id] = self.conversations[
                    conversation_id
                ][-10:]

            # Save to persistent memory with sliding window + summarization
            try:
                self.memory_service.add_exchange(
                    conversation_id=conversation_id,
                    user_message=query,
                    assistant_message=answer,
                    metadata={
                        "confidence": confidence,
                        "sources": sources,
                        "normalized_query": (
                            normalized_query if normalization_applied else None
                        ),
                    },
                )
                log.debug(
                    f"üíæ Saved exchange to persistent memory for {conversation_id}"
                )
            except Exception as mem_error:
                log.warning(f"Could not save to persistent memory: {mem_error}")

            # Save conversation to PostgreSQL (legacy)
            processing_time = time.time() - time.time()  # Will be calculated properly
            try:
                self.db_service.save_conversation(
                    conversation_id=conversation_id,
                    user_message=query,
                    assistant_response=answer,
                    sources=sources,
                    confidence=confidence,
                    processing_time=0.0,  # Processing time will be set at API level
                )
            except Exception as save_error:
                log.warning(f"Could not save conversation to DB: {save_error}")

            # Detect if chart visualization is needed
            chart_data = self._detect_chart_request(query, answer)
            if chart_data:
                log.info(f"üìä Generated {len(chart_data)} chart(s) for visualization")

            # Get attachments using hybrid approach: chunk linking + keyword matching
            attachments = []
            try:
                log.info("üîç Starting attachment retrieval...")
                attachment_ids_found = set()

                # Strategy 1: Get attachments linked to retrieved chunks
                if relevant_chunks:
                    log.info(
                        f"Strategy 1: Checking {len(relevant_chunks)} chunks for linked attachments"
                    )
                    chunk_ids = [
                        chunk.get("id") for chunk in relevant_chunks if chunk.get("id")
                    ]
                    if chunk_ids:
                        chunk_attachments = (
                            self.attachment_service.get_attachments_by_chunk_ids(
                                chunk_ids
                            )
                        )
                        for att in chunk_attachments:
                            attachment_ids_found.add(att.id)
                            attachments.append(
                                {
                                    "file_name": att.file_name,
                                    "file_type": att.file_type,
                                    "download_url": att.download_url,
                                    "description": att.description,
                                    "file_size": att.file_size,
                                }
                            )

                # Strategy 2: Search attachments by keywords from query
                from src.services.smart_attachment_matcher import SmartAttachmentMatcher

                query_keywords = SmartAttachmentMatcher.extract_keywords_from_query(
                    query
                )
                log.info(f"Strategy 2: Extracted keywords from query: {query_keywords}")
                if query_keywords:
                    keyword_attachments = self.attachment_service.search_attachments(
                        keywords=query_keywords
                    )
                    log.info(
                        f"Strategy 2: Found {len(keyword_attachments)} attachments by keyword search"
                    )

                    # Score and filter keyword-matched attachments
                    for att in keyword_attachments:
                        if att.id not in attachment_ids_found:
                            # Calculate relevance score
                            score = SmartAttachmentMatcher.score_attachment_relevance(
                                att.keywords or [], query_keywords
                            )

                            # Only include if relevance score is high enough
                            if score > 0.3:  # Threshold
                                attachment_ids_found.add(att.id)
                                attachments.append(
                                    {
                                        "file_name": att.file_name,
                                        "file_type": att.file_type,
                                        "download_url": att.download_url,
                                        "description": att.description,
                                        "file_size": att.file_size,
                                    }
                                )
                                log.info(
                                    f"üìé Found keyword-matched attachment: {att.file_name} (score: {score:.2f})"
                                )

                if attachments:
                    log.info(
                        f"üìé Found {len(attachments)} attachment(s) for this response"
                    )
            except Exception as att_error:
                log.warning(f"Could not retrieve attachments: {att_error}")

            return {
                "answer": answer,
                "sources": sources,
                "source_references": source_references,
                "attachments": attachments,
                "confidence": confidence,
                "conversation_id": conversation_id,
                "normalization_applied": normalization_applied,
                "original_query": query if normalization_applied else None,
                "normalized_query": normalized_query if normalization_applied else None,
                "chart_data": chart_data,  # Charts for visualization
                "images": [],  # Will be populated if images are found in sources
            }

        except Exception as e:
            log.error(f"Error generating answer: {e}")
            return {
                "answer": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.",
                "sources": [],
                "source_references": [],
                "attachments": [],
                "confidence": 0.0,
                "conversation_id": conversation_id or str(uuid.uuid4()),
                "chart_data": [],
                "images": [],
            }

    def get_conversation_history(self, conversation_id: str) -> List[Dict[str, Any]]:
        """
        Get conversation history

        Args:
            conversation_id: Conversation ID

        Returns:
            List of conversation exchanges
        """
        return self.conversations.get(conversation_id, [])

    def check_system_health(self) -> Dict[str, Any]:
        """
        Check health of all RAG components

        Returns:
            Health status dictionary
        """
        health_status = {"overall_status": "healthy", "components": {}}

        # Check Ollama
        ollama_health = self.ollama_service.check_health()
        health_status["components"]["ollama"] = ollama_health

        # Check PostgreSQL + pgvector
        try:
            db_stats = self.db_service.get_database_stats()
            health_status["components"]["database"] = {
                "status": "healthy",
                "stats": db_stats,
            }
        except Exception as e:
            health_status["components"]["database"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check Hybrid Retrieval Service (BM25 + pgvector)
        try:
            health_status["components"]["hybrid_retrieval"] = {
                "status": "healthy",
                "type": "BM25 + pgvector",
            }
        except Exception as e:
            health_status["components"]["hybrid_retrieval"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check embedding service
        try:
            embedding_dim = self.embedding_service.get_embedding_dimension()
            health_status["components"]["embedding"] = {
                "status": "healthy",
                "dimension": embedding_dim,
            }
        except Exception as e:
            health_status["components"]["embedding"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check ingestion service
        try:
            health_status["components"]["ingestion"] = {
                "status": "healthy",
                "type": "PDF file watcher",
            }
        except Exception as e:
            health_status["components"]["ingestion"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Determine overall status
        component_statuses = [
            comp.get("status", "unknown")
            for comp in health_status["components"].values()
        ]
        if any(status == "unhealthy" for status in component_statuses):
            health_status["overall_status"] = "unhealthy"
        elif any(status == "unknown" for status in component_statuses):
            health_status["overall_status"] = "degraded"

        return health_status

    def cleanup(self):
        """Cleanup resources - stop ingestion service"""
        try:
            if hasattr(self, "ingestion_service"):
                log.info("Stopping ingestion service...")
                self.ingestion_service.stop_watching()
                log.info("Ingestion service stopped successfully.")
        except Exception as e:
            log.error(f"Error during cleanup: {e}")
