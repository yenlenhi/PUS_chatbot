"""
RAG (Retrieval-Augmented Generation) service
"""

import uuid
import re
import time
from typing import List, Dict, Any, Optional
from src.services.embedding_service import EmbeddingService
from src.services.postgres_database_service import PostgresDatabaseService
from src.services.hybrid_retrieval_service import HybridRetrievalService
from src.services.ingestion_service import IngestionService
from src.services.pdf_processor import PDFProcessor
from src.services import gemini_service
from src.services.gemini_service import normalize_question
from src.services.memory_service import ConversationMemoryService
from src.services.attachment_service import AttachmentService
from sentence_transformers import CrossEncoder
from src.services.ollama_service import OllamaService
from src.utils.logger import log

from config.settings import (
    TOP_K_RESULTS,
    LLM_PROVIDER,
    ENABLE_GEMINI_NORMALIZATION,
)


class RAGService:
    """Service for Retrieval-Augmented Generation"""

    def __init__(self, analytics_service=None):
        """Initialize RAG service with PostgreSQL + Hybrid Retrieval"""
        self.embedding_service = EmbeddingService()
        self.db_service = PostgresDatabaseService()
        self.retrieval_service = HybridRetrievalService(
            self.db_service, self.embedding_service
        )
        self.pdf_processor = PDFProcessor()

        # Import analytics service lazily to avoid circular imports
        if analytics_service is None:
            try:
                from src.services.analytics_service import AnalyticsService

                analytics_service = AnalyticsService(self.db_service)
            except Exception as e:
                log.warning(f"Could not initialize analytics service: {e}")
                analytics_service = None

        self.analytics_service = analytics_service

        self.ingestion_service = IngestionService(
            self.db_service,
            self.embedding_service,
            self.pdf_processor,
            self.retrieval_service,
            analytics_service,  # Pass analytics service for document tracking
        )
        self.ollama_service = OllamaService()

        # Initialize Memory Service for persistent conversational memory
        self.memory_service = ConversationMemoryService(
            self.db_service, self.embedding_service
        )

        # Initialize Attachment Service
        self.attachment_service = AttachmentService(self.db_service)

        # Conversation memory (in-memory cache, backed by persistent storage)
        self.conversations = {}

        # Initialize Reranker
        try:
            log.info("Initializing Reranker model...")
            self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
            log.info("Reranker model initialized successfully.")
        except Exception as e:
            log.error(f"Error initializing Reranker model: {e}")
            self.reranker = None

        # Start ingestion service
        try:
            log.info("Starting ingestion service...")
            self.ingestion_service.start_watching()
            log.info("Ingestion service started successfully.")
        except Exception as e:
            log.error(f"Error starting ingestion service: {e}")

    def _rerank_chunks(
        self, query: str, chunks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Reranks a list of chunks based on their relevance to the query using a Cross-Encoder model.
        """
        if not self.reranker or not chunks:
            return chunks

        try:
            # Create pairs of [query, chunk_content] for the reranker
            pairs = [[query, chunk["content"]] for chunk in chunks]

            # Predict the scores
            scores = self.reranker.predict(pairs)

            # Assign scores to chunks
            for chunk, score in zip(chunks, scores):
                chunk["rerank_score"] = float(score)

            # Sort chunks by the new rerank score in descending order
            chunks.sort(key=lambda x: x.get("rerank_score", 0.0), reverse=True)

            log.info(f"Reranked {len(chunks)} chunks successfully.")
            return chunks

        except Exception as e:
            log.error(f"Error during chunk reranking: {e}")
            # Return original chunks in case of an error

    def _detect_chart_request(self, query: str, answer: str) -> List[Dict[str, Any]]:
        """
        Detect if the query/answer contains statistical data that can be visualized as charts.
        Returns chart data if applicable.
        """
        chart_data = []
        query_lower = query.lower()

        # Keywords that suggest chart visualization
        chart_keywords = [
            "thá»‘ng kÃª",
            "biá»ƒu Ä‘á»“",
            "so sÃ¡nh",
            "tá»· lá»‡",
            "pháº§n trÄƒm",
            "%",
            "sá»‘ lÆ°á»£ng",
            "chá»‰ tiÃªu",
            "Ä‘iá»ƒm chuáº©n",
            "Ä‘iá»ƒm trÃºng tuyá»ƒn",
            "tuyá»ƒn sinh",
            "há»c viÃªn",
            "sinh viÃªn",
            "nÄƒm",
            "khÃ³a",
            "ngÃ nh",
            "chart",
            "graph",
            "statistics",
        ]

        # Check if query asks for statistics/charts
        should_generate_chart = any(
            keyword in query_lower for keyword in chart_keywords
        )

        if should_generate_chart:
            # Example: Admission statistics by year
            if any(word in query_lower for word in ["tuyá»ƒn sinh", "chá»‰ tiÃªu", "nÄƒm"]):
                chart_data.append(
                    {
                        "type": "bar",
                        "title": "Chá»‰ tiÃªu tuyá»ƒn sinh qua cÃ¡c nÄƒm",
                        "data": [
                            {"name": "2021", "Chá»‰ tiÃªu": 450, "TrÃºng tuyá»ƒn": 420},
                            {"name": "2022", "Chá»‰ tiÃªu": 500, "TrÃºng tuyá»ƒn": 480},
                            {"name": "2023", "Chá»‰ tiÃªu": 550, "TrÃºng tuyá»ƒn": 530},
                            {"name": "2024", "Chá»‰ tiÃªu": 600, "TrÃºng tuyá»ƒn": 580},
                            {"name": "2025", "Chá»‰ tiÃªu": 650, "TrÃºng tuyá»ƒn": 0},
                        ],
                        "xKey": "name",
                        "yKeys": ["Chá»‰ tiÃªu", "TrÃºng tuyá»ƒn"],
                        "description": "Biá»ƒu Ä‘á»“ thá»‘ng kÃª chá»‰ tiÃªu tuyá»ƒn sinh (Dá»¯ liá»‡u minh há»a)",
                    }
                )

            # Example: Score distribution by major
            if any(
                word in query_lower
                for word in ["Ä‘iá»ƒm chuáº©n", "Ä‘iá»ƒm trÃºng tuyá»ƒn", "ngÃ nh"]
            ):
                chart_data.append(
                    {
                        "type": "bar",
                        "title": "Äiá»ƒm chuáº©n cÃ¡c ngÃ nh nÄƒm 2024",
                        "data": [
                            {"name": "An ninh chÃ­nh trá»‹", "Äiá»ƒm chuáº©n": 24.5},
                            {"name": "An ninh kinh táº¿", "Äiá»ƒm chuáº©n": 25.0},
                            {"name": "An ninh máº¡ng", "Äiá»ƒm chuáº©n": 26.5},
                            {"name": "Äiá»u tra hÃ¬nh sá»±", "Äiá»ƒm chuáº©n": 25.5},
                            {"name": "Ká»¹ thuáº­t hÃ¬nh sá»±", "Äiá»ƒm chuáº©n": 24.0},
                        ],
                        "xKey": "name",
                        "yKeys": ["Äiá»ƒm chuáº©n"],
                        "description": "Biá»ƒu Ä‘á»“ Ä‘iá»ƒm chuáº©n cÃ¡c ngÃ nh (Dá»¯ liá»‡u minh há»a)",
                    }
                )

            # Example: Student distribution by major (pie chart)
            if any(word in query_lower for word in ["tá»· lá»‡", "phÃ¢n bá»‘", "cÆ¡ cáº¥u"]):
                chart_data.append(
                    {
                        "type": "pie",
                        "title": "Tá»· lá»‡ há»c viÃªn theo ngÃ nh Ä‘Ã o táº¡o",
                        "data": [
                            {"name": "An ninh chÃ­nh trá»‹", "value": 25},
                            {"name": "An ninh kinh táº¿", "value": 20},
                            {"name": "An ninh máº¡ng", "value": 30},
                            {"name": "Äiá»u tra hÃ¬nh sá»±", "value": 15},
                            {"name": "Ká»¹ thuáº­t hÃ¬nh sá»±", "value": 10},
                        ],
                        "xKey": "name",
                        "yKeys": ["value"],
                        "description": "Biá»ƒu Ä‘á»“ tá»· lá»‡ há»c viÃªn theo ngÃ nh (Dá»¯ liá»‡u minh há»a)",
                    }
                )

            # Example: Trend over time (line chart)
            if any(
                word in query_lower
                for word in ["xu hÆ°á»›ng", "trend", "biáº¿n Ä‘á»™ng", "qua cÃ¡c nÄƒm"]
            ):
                chart_data.append(
                    {
                        "type": "line",
                        "title": "Xu hÆ°á»›ng sá»‘ lÆ°á»£ng há»“ sÆ¡ Ä‘Äƒng kÃ½ qua cÃ¡c nÄƒm",
                        "data": [
                            {"name": "2020", "Há»“ sÆ¡": 1200},
                            {"name": "2021", "Há»“ sÆ¡": 1450},
                            {"name": "2022", "Há»“ sÆ¡": 1680},
                            {"name": "2023", "Há»“ sÆ¡": 1920},
                            {"name": "2024", "Há»“ sÆ¡": 2150},
                        ],
                        "xKey": "name",
                        "yKeys": ["Há»“ sÆ¡"],
                        "description": "Biá»ƒu Ä‘á»“ xu hÆ°á»›ng sá»‘ lÆ°á»£ng há»“ sÆ¡ (Dá»¯ liá»‡u minh há»a)",
                    }
                )

        return chart_data

    def retrieve_relevant_chunks(
        self, query: str, top_k: int = TOP_K_RESULTS
    ) -> List[Dict[str, Any]]:
        """Retrieve relevant chunks using hybrid retrieval (dense + sparse search)."""
        try:
            # Generate query embedding
            query_embedding = self.embedding_service.create_embedding(query)

            # Perform hybrid search using PostgreSQL + pgvector
            initial_k = max(top_k * 3, 15)  # Get more candidates for reranking
            hybrid_results = self.retrieval_service.hybrid_search(
                query=query, query_embedding=query_embedding, top_k=initial_k
            )

            if not hybrid_results:
                log.warning(f"No chunks found for query: {query}")
                return []

            log.info(f"Hybrid search found {len(hybrid_results)} chunks.")

            # Rerank results
            reranked_chunks = self._rerank_chunks(query, hybrid_results)

            # Context expansion
            expanded_chunks = self._expand_context(reranked_chunks[:top_k], query)

            # Final ranking
            final_chunks = self._final_ranking(expanded_chunks, query)
            return final_chunks[:top_k]

        except Exception as e:
            log.error(f"Error during retrieve_relevant_chunks: {e}")
            return []

    def _extract_heading_from_content(self, content: str) -> Optional[str]:
        """
        Extract heading from chunk content

        Args:
            content: Chunk content

        Returns:
            Heading if found, None otherwise
        """
        # Try to extract heading from first line
        lines = content.strip().split("\n")
        if not lines:
            return None

        first_line = lines[0].strip()

        # Check if first line matches heading pattern
        heading_patterns = [
            r"^\s*(\d+)\.\s+(.+)$",
            r"^\s*(\d+\.\d+)\.\s+(.+)$",
            r"^\s*(\d+\.\d+\.\d+)\.\s+(.+)$",
        ]

        for pattern in heading_patterns:
            match = re.match(pattern, first_line)
            if match:
                return first_line

        return None

    def _expand_context(
        self, chunks: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Expand context by adding related chunks from the same document/section

        Args:
            chunks: Initial retrieved chunks
            query: User query for relevance checking

        Returns:
            Expanded list of chunks with additional context
        """
        if not chunks:
            return chunks

        expanded_chunks = chunks.copy()

        try:
            # Group chunks by source file
            chunks_by_source = {}
            for chunk in chunks:
                source = chunk.get("source_file", "")
                if source not in chunks_by_source:
                    chunks_by_source[source] = []
                chunks_by_source[source].append(chunk)

            # For each source file, try to find adjacent chunks
            for source_file, source_chunks in chunks_by_source.items():
                for chunk in source_chunks:
                    chunk_index = chunk.get("chunk_index", -1)
                    page_number = chunk.get("page_number", -1)

                    if chunk_index >= 0:
                        # Look for adjacent chunks (before and after)
                        for offset in [-1, 1]:
                            adjacent_chunk = self._get_adjacent_chunk(
                                source_file, chunk_index + offset, page_number
                            )
                            if adjacent_chunk and adjacent_chunk["id"] not in [
                                c["id"] for c in expanded_chunks
                            ]:
                                # Check if adjacent chunk is somewhat relevant
                                if self._is_chunk_relevant(adjacent_chunk, query):
                                    adjacent_chunk["context_expansion"] = True
                                    adjacent_chunk["hybrid_score"] = (
                                        chunk.get("hybrid_score", 0.0) * 0.7
                                    )  # Lower score for context
                                    expanded_chunks.append(adjacent_chunk)

            log.info(
                f"Context expansion added {len(expanded_chunks) - len(chunks)} additional chunks"
            )
            return expanded_chunks

        except Exception as e:
            log.error(f"Error during context expansion: {e}")
            return chunks

    def _get_adjacent_chunk(
        self, source_file: str, chunk_index: int, page_number: int
    ) -> Optional[Dict[str, Any]]:
        """Get adjacent chunk by source file and chunk index using PostgreSQL"""
        try:
            chunk = self.db_service.get_chunk_by_source_and_index(
                source_file, chunk_index
            )
            return chunk
        except Exception as e:
            log.error(f"Error getting adjacent chunk: {e}")
            return None

    def _is_chunk_relevant(self, chunk: Dict[str, Any], query: str) -> bool:
        """Check if a chunk is relevant to the query using simple keyword matching"""
        try:
            content = chunk.get("content", "").lower()
            query_lower = query.lower()

            # Simple keyword overlap check
            query_words = set(query_lower.split())
            content_words = set(content.split())

            # Calculate overlap ratio
            overlap = len(query_words.intersection(content_words))
            overlap_ratio = overlap / len(query_words) if query_words else 0

            # Consider relevant if there's at least 20% keyword overlap
            return overlap_ratio >= 0.2

        except Exception as e:
            log.error(f"Error checking chunk relevance: {e}")
            return False

    def _final_ranking(
        self, chunks: List[Dict[str, Any]], query: str
    ) -> List[Dict[str, Any]]:
        """
        Final ranking of chunks considering multiple factors

        Args:
            chunks: List of chunks to rank
            query: User query

        Returns:
            Ranked list of chunks
        """
        try:
            # Sort by multiple criteria
            def ranking_score(chunk):
                # Primary: rerank_score if available, otherwise hybrid_score
                primary_score = chunk.get(
                    "rerank_score", chunk.get("hybrid_score", 0.0)
                )

                # Bonus for chunks with headings (likely more structured content)
                heading_bonus = 0.1 if chunk.get("heading_text") else 0.0

                # Bonus for chunks that are not context expansions (original results)
                original_bonus = (
                    0.05 if not chunk.get("context_expansion", False) else 0.0
                )

                # Penalty for very short chunks (likely less informative)
                content_length = len(chunk.get("content", ""))
                length_penalty = -0.1 if content_length < 100 else 0.0

                return primary_score + heading_bonus + original_bonus + length_penalty

            ranked_chunks = sorted(chunks, key=ranking_score, reverse=True)

            log.info(f"Final ranking completed for {len(ranked_chunks)} chunks")
            return ranked_chunks

        except Exception as e:
            log.error(f"Error during final ranking: {e}")
            return chunks

    def _generate_vision_answer(
        self,
        query: str,
        images: List[Any],
        conversation_id: Optional[str] = None,
        language: str = "vi",  # Add language parameter
    ) -> Dict[str, Any]:
        """
        Generate answer for image-based queries using Gemini Vision.

        Args:
            query: User's question about the image(s)
            images: List of ImageInput objects with base64 encoded images
            conversation_id: Optional conversation ID
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            Dictionary with answer, confidence, and conversation_id
        """
        try:
            log.info(
                f"Processing vision query with {len(images)} images, language={language}"
            )

            # Create conversation ID if needed
            if not conversation_id:
                conversation_id = str(uuid.uuid4())

            # Build the vision prompt based on language
            if language == "en":
                vision_prompt = f"""You are an AI assistant specializing in supporting information about People's Security University (PSU).
Please analyze the provided image(s) and answer the user's question in ENGLISH.

User's question: {query if query else "Please describe the content of this image."}

Instructions:
- Analyze the image content carefully
- Respond ENTIRELY in ENGLISH
- If the image contains documents or text (which may be in Vietnamese), translate and explain the content in English
- If it's a data table, summarize the important information in English
- Provide a detailed, easy-to-understand answer in English"""
            else:
                vision_prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn há»— trá»£ vá» TrÆ°á»ng Äáº¡i há»c An ninh NhÃ¢n dÃ¢n.
HÃ£y phÃ¢n tÃ­ch hÃ¬nh áº£nh Ä‘Æ°á»£c cung cáº¥p vÃ  tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {query if query else "HÃ£y mÃ´ táº£ ná»™i dung trong hÃ¬nh áº£nh nÃ y."}

HÆ°á»›ng dáº«n:
- PhÃ¢n tÃ­ch ká»¹ ná»™i dung trong hÃ¬nh áº£nh
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
- Náº¿u hÃ¬nh áº£nh liÃªn quan Ä‘áº¿n tÃ i liá»‡u, vÄƒn báº£n, hÃ£y trÃ­ch dáº«n vÃ  giáº£i thÃ­ch ná»™i dung
- Náº¿u lÃ  báº£ng sá»‘ liá»‡u, hÃ£y tÃ³m táº¯t thÃ´ng tin quan trá»ng
- ÄÆ°a ra cÃ¢u tráº£ lá»i chi tiáº¿t, dá»… hiá»ƒu"""

            # Prepare image data for Gemini
            image_parts = []
            for img in images:
                try:
                    # Get base64 data (remove data:image/xxx;base64, prefix if present)
                    base64_data = img.base64
                    if "," in base64_data:
                        base64_data = base64_data.split(",")[1]

                    # Determine mime type
                    mime_type = getattr(img, "mime_type", "image/jpeg")
                    if not mime_type:
                        mime_type = "image/jpeg"

                    image_parts.append({"mime_type": mime_type, "data": base64_data})
                    log.info(
                        f"Processed image: {getattr(img, 'name', 'unknown')} ({mime_type})"
                    )
                except Exception as img_error:
                    log.error(f"Error processing image: {img_error}")
                    continue

            if not image_parts:
                error_msg = (
                    "Sorry, unable to process the image. Please try again with a different format (PNG, JPG, WebP)."
                    if language == "en"
                    else "Xin lá»—i, khÃ´ng thá»ƒ xá»­ lÃ½ hÃ¬nh áº£nh. Vui lÃ²ng thá»­ láº¡i vá»›i Ä‘á»‹nh dáº¡ng áº£nh khÃ¡c (PNG, JPG, WebP)."
                )
                return {
                    "answer": error_msg,
                    "sources": [],
                    "source_references": [],
                    "confidence": 0.0,
                    "conversation_id": conversation_id,
                }

            # Call Gemini Vision
            answer = gemini_service.generate_vision_response(
                prompt=vision_prompt, images=image_parts
            )

            if not answer:
                answer = (
                    "Sorry, I couldn't analyze this image. Please try again or describe more about what you want to ask."
                    if language == "en"
                    else "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ phÃ¢n tÃ­ch hÃ¬nh áº£nh nÃ y. Vui lÃ²ng thá»­ láº¡i hoáº·c mÃ´ táº£ thÃªm vá» ná»™i dung báº¡n muá»‘n há»i."
                )
            else:
                # Add engagement prompt
                answer = self._add_engagement_prompt(answer, query, language)

            log.info("Vision response generated successfully")

            return {
                "answer": answer,
                "sources": [],
                "source_references": [],
                "confidence": 0.85,  # Default confidence for vision queries
                "conversation_id": conversation_id,
                "chart_data": [],
                "images": [],
            }

        except Exception as e:
            log.error(f"Error in vision query processing: {e}")
            return {
                "answer": f"Xin lá»—i, cÃ³ lá»—i xáº£y ra khi phÃ¢n tÃ­ch hÃ¬nh áº£nh: {str(e)}. Vui lÃ²ng thá»­ láº¡i.",
                "sources": [],
                "source_references": [],
                "confidence": 0.0,
                "conversation_id": conversation_id or str(uuid.uuid4()),
            }

    def _create_contextual_followup(
        self, user_query: str, answer: str, language: str = "vi"
    ) -> List[str]:
        """
        Create 2-3 contextual follow-up questions based on the user's query and answer

        Args:
            user_query: The original user question
            answer: The generated answer
            language: Language for the follow-up questions

        Returns:
            List of 2-3 contextual follow-up questions
        """
        try:
            # Extract key topics/entities from the user query
            key_topics = self._extract_key_topics(user_query)
            questions = []

            if key_topics and language == "vi":
                # Create 2-3 contextual follow-up questions in Vietnamese
                if any(
                    word in user_query.lower()
                    for word in [
                        "tuyá»ƒn sinh",
                        "xÃ©t tuyá»ƒn",
                        "Ä‘Äƒng kÃ½",
                        "nháº­p há»c",
                        "chá»‰ tiÃªu",
                    ]
                ):
                    questions = [
                        "Báº¡n cÃ³ muá»‘n biáº¿t chi tiáº¿t vá» há»“ sÆ¡ xÃ©t tuyá»ƒn khÃ´ng?",
                        "Báº¡n cáº§n thÃ´ng tin vá» Ä‘iá»m chuáº©n cÃ¡c ngÃ nh khÃ´ng?",
                        "Báº¡n cÃ³ muá»‘n tÃ¬m hiá»ƒu vá» phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn khÃ´ng?",
                    ]
                elif any(
                    word in user_query.lower()
                    for word in [
                        "há»c phÃ­",
                        "chi phÃ­",
                        "tiá»n há»c",
                        "há»c bá»•ng",
                        "tÃ i chÃ­nh",
                    ]
                ):
                    questions = [
                        "Báº¡n cÃ³ cáº§n thÃ´ng tin vá» cÃ¡c gÃ³i há»— trá»£ tÃ i chÃ­nh khÃ´ng?",
                        "Báº¡n cÃ³ muá»‘n biáº¿t vá» Ä‘iá»u kiá»‡n nháº­n há»c bá»•ng khÃ´ng?",
                        "Báº¡n cáº§n hÆ°á»›ng dáº«n vá» thá»§ tá»¥c tráº£ há»c phÃ­ khÃ´ng?",
                    ]
                elif any(
                    word in user_query.lower()
                    for word in [
                        "ngÃ nh",
                        "chuyÃªn ngÃ nh",
                        "khoa",
                        "chÆ°Æ¡ng trÃ¬nh",
                        "Ä‘Ã o táº¡o",
                    ]
                ):
                    questions = [
                        "Báº¡n cÃ³ muá»‘n tÃ¬m hiá»ƒu káº¿ hoáº¡ch há»c táº­p cá»§a ngÃ nh nÃ y khÃ´ng?",
                        "Báº¡n cáº§n thÃ´ng tin vá» cÆ¡ há»™i thá»±c táº­p vÃ  viá»‡c lÃ m khÃ´ng?",
                        "Báº¡n cÃ³ muá»‘n biáº¿t vá» chá»©ng chá»‰ vÃ  báº±ng cáº¥p khÃ´ng?",
                    ]
                elif any(
                    word in user_query.lower()
                    for word in ["kÃ½ tÃºc xÃ¡", "ktx", "chá»— á»Ÿ", "ná»™i trÃº", "sinh viÃªn"]
                ):
                    questions = [
                        "Báº¡n cÃ³ cáº§n biáº¿t ká»¹ hÆ¡n vá» cÆ¡ sá»Ÿ váº­t cháº¥t kÃ½ tÃºc xÃ¡ khÃ´ng?",
                        "Báº¡n cÃ³ muá»‘n tÃ¬m hiá»ƒu vá» quy Ä‘á»‹nh sinh hoáº¡t táº¡i KTX khÃ´ng?",
                        "Báº¡n cáº§n hÆ°á»›ng dáº«n thá»§ tá»¥c Ä‘Äƒng kÃ½ phÃ²ng á»Ÿ khÃ´ng?",
                    ]
                elif any(
                    word in user_query.lower()
                    for word in [
                        "viá»‡c lÃ m",
                        "nghá» nghiá»‡p",
                        "cÆ¡ há»™i",
                        "tÆ°Æ¡ng lai",
                        "thá»±c táº­p",
                    ]
                ):
                    questions = [
                        "Báº¡n cÃ³ muá»‘n tÃ¬m hiá»ƒu vá» máº¡ng lÆ°á»›i doanh nghiá»‡p Ä‘á»‘i tÃ¡c khÃ´ng?",
                        "Báº¡n cáº§n thÃ´ng tin vá» chÆ°Æ¡ng trÃ¬nh thá»±c táº­p táº¡i cÃ¡c cÃ´ng ty khÃ´ng?",
                        "Báº¡n cÃ³ muá»‘n biáº¿t vá» tá»· lá»‡ cÃ³ viá»‡c cá»§a cá»­ nhÃ¢n khÃ´ng?",
                    ]
                elif any(
                    word in user_query.lower()
                    for word in [
                        "quy Ä‘á»‹nh",
                        "quy cháº¿",
                        "ná»™i quy",
                        "chÃ­nh sÃ¡ch",
                        "thá»§ tá»¥c",
                    ]
                ):
                    questions = [
                        "Báº¡n cÃ³ cáº§n giáº£i thÃ­ch thÃªm vá» quy trÃ¬nh thá»±c hiá»‡n khÃ´ng?",
                        "Báº¡n cÃ³ muá»‘n biáº¿t vá» giáº¥y tá» cáº§n thiáº¿t khÃ´ng?",
                        "Báº¡n cáº§n hÆ°á»›ng dáº«n cá»¥ thá»ƒ vá» thá»i háº¡n khÃ´ng?",
                    ]
                else:
                    # Generic contextual follow-up based on the main topic
                    main_topic = key_topics[0] if key_topics else "chá»§ Ä‘á» nÃ y"
                    questions = [
                        f"Báº¡n cÃ³ muá»‘n biáº¿t thÃªm gÃ¬ vá» {main_topic} khÃ´ng?",
                        "Báº¡n cÃ³ cÃ¢u há»i nÃ o liÃªn quan khÃ¡c khÃ´ng?",
                        "CÃ³ thÃ´ng tin nÃ o khÃ¡c tÃ´i cÃ³ thá»ƒ giÃºp báº¡n khÃ´ng?",
                    ]
            elif language == "en":
                questions = [
                    "Would you like to know more about this topic?",
                    "Do you have any related questions?",
                    "Is there anything else I can help you with?",
                ]
            else:
                questions = [
                    "Báº¡n cÃ³ muá»‘n biáº¿t thÃªm vá» chá»§ Ä‘á» nÃ y khÃ´ng?",
                    "Báº¡n cÃ³ cÃ¢u há»i nÃ o khÃ¡c khÃ´ng?",
                ]

            # Return 2-3 questions (randomly pick 2-3 if more available)
            import random

            num_questions = min(len(questions), random.choice([2, 3]))
            return (
                random.sample(questions, num_questions)
                if len(questions) > num_questions
                else questions
            )

        except Exception as e:
            log.error(f"Error creating contextual follow-up: {e}")
            # Fallback to generic questions
            if language == "vi":
                return [
                    "Báº¡n cÃ³ cÃ¢u há»i gÃ¬ khÃ¡c khÃ´ng?",
                    "TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ thÃªm cho báº¡n khÃ´ng?",
                ]
            else:
                return [
                    "Do you have any other questions?",
                    "Is there anything else I can help you with?",
                ]

    def _extract_key_topics(self, query: str) -> List[str]:
        """
        Extract key topics from user query

        Args:
            query: User query

        Returns:
            List of key topics/entities
        """
        query_lower = query.lower()

        # Define topic keywords
        topic_map = {
            "tuyá»ƒn sinh": ["tuyá»ƒn sinh", "xÃ©t tuyá»ƒn", "Ä‘Äƒng kÃ½", "nháº­p há»c"],
            "há»c phÃ­": ["há»c phÃ­", "chi phÃ­", "tiá»n há»c", "há»c bá»•ng", "tÃ i chÃ­nh"],
            "ngÃ nh há»c": ["ngÃ nh", "chuyÃªn ngÃ nh", "khoa", "chÆ°Æ¡ng trÃ¬nh"],
            "kÃ½ tÃºc xÃ¡": ["kÃ½ tÃºc xÃ¡", "chá»— á»Ÿ", "ná»™i trÃº", "sinh viÃªn"],
            "viá»‡c lÃ m": ["viá»‡c lÃ m", "nghá» nghiá»‡p", "cÆ¡ há»™i", "tÆ°Æ¡ng lai"],
            "quy Ä‘á»‹nh": ["quy Ä‘á»‹nh", "quy cháº¿", "ná»™i quy", "chÃ­nh sÃ¡ch"],
            "Ä‘Ã o táº¡o": ["Ä‘Ã o táº¡o", "há»c táº­p", "giáº£ng dáº¡y", "cháº¥t lÆ°á»£ng"],
        }

        topics = []
        for topic, keywords in topic_map.items():
            if any(keyword in query_lower for keyword in keywords):
                topics.append(topic)

        return topics

    def _add_engagement_prompt(
        self, answer: str, user_query: str = "", language: str = "vi"
    ) -> str:
        """
        Add contextual engagement prompt to the answer

        Args:
            answer: The generated answer
            user_query: The original user query for context
            language: Language for the follow-up question

        Returns:
            Answer with contextual engagement prompt added
        """
        engagement_prompts = [
            "báº¡n cÃ²n cÃ³ tháº¯c máº¯c gÃ¬ khÃ¡c khÃ´ng",
            "tÃ´i sáºµn sÃ ng há»— trá»£ thÃªm",
            "cÃ³ cÃ¢u há»i nÃ o khÃ¡c khÃ´ng",
            "cáº§n há»— trá»£ thÃªm gÃ¬ khÃ´ng",
            "muá»‘n biáº¿t thÃªm",
            "cÃ³ muá»‘n",
        ]

        # Check if any engagement prompt is already present (case insensitive)
        answer_lower = answer.lower()
        has_engagement = any(
            prompt.lower() in answer_lower for prompt in engagement_prompts
        )

        if not has_engagement:
            # Create contextual follow-up questions (2-3 questions)
            followup_questions = self._create_contextual_followup(
                user_query, answer, language
            )

            # Format multiple questions with proper numbering and clear header
            if followup_questions:
                # Create a clear header for follow-up questions
                header = "\n\n**--- CÃC CÃ‚U Há»ŽI LIÃŠN QUAN ---**\n"

                formatted_questions = "\n".join(
                    [f"- **{question}**" for question in followup_questions]
                )

                full_followup = f"{header}{formatted_questions}"

                # Add the contextual follow-ups with proper formatting
                if (
                    answer.strip().endswith(".")
                    or answer.strip().endswith("!")
                    or answer.strip().endswith("?")
                ):
                    return f"{answer}{full_followup}"
                else:
                    return f"{answer}.{full_followup}"

        return answer

    def create_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        Create a simplified and clean context string from retrieved chunks.
        """
        if not chunks:
            return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u."

        context_parts = []
        for chunk in chunks:
            source = chunk.get("source_file", "Unknown")
            page = chunk.get("page_number", "N/A")
            content = chunk.get("content", "").strip()

            # Simplified format for the LLM - removed source citation from content
            context_part = f"###\n{content}\n###"
            context_parts.append(context_part)

        return "\n\n".join(context_parts)

    def create_system_prompt(self, language: str = "vi") -> str:
        """
        Create system prompt for the chatbot

        Args:
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            System prompt string
        """
        # Language-specific instructions
        if language == "en":
            language_instruction = """
**IMPORTANT - RESPONSE LANGUAGE: ENGLISH**
You MUST respond ENTIRELY in ENGLISH. This is a strict requirement from the user who has selected English as their preferred language.
- Translate ALL content to English, including explanations, instructions, and summaries.
- You may keep Vietnamese proper nouns (names of schools, documents, regulations) in their original form when necessary.
- All headings, bullet points, and explanations must be in English.
"""
        else:
            language_instruction = """
**NGÃ”N NGá»® TRáº¢ Lá»œI: TIáº¾NG VIá»†T**
Báº¡n PHáº¢I tráº£ lá»i hoÃ n toÃ n báº±ng TIáº¾NG VIá»†T.
"""

        return f"""{language_instruction}

Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn há»— trá»£ sinh viÃªn, cÃ¡n bá»™, chiáº¿n sÄ© vÃ  ngÆ°á»i quan tÃ¢m vá» **TrÆ°á»ng Äáº¡i há»c An ninh NhÃ¢n dÃ¢n (ANND)** / **People's Security University (PSU)**.

**Pháº¡m vi chuyÃªn mÃ´n chÃ­nh cá»§a báº¡n gá»“m 5 nhÃ³m ná»™i dung:**
1. **TÆ° váº¥n thÃ´ng tin tuyá»ƒn sinh / Admission Information**  
   - Äiá»u kiá»‡n, chá»‰ tiÃªu, phÆ°Æ¡ng thá»©c, há»“ sÆ¡, lá»‹ch trÃ¬nh, phÃ¢n vÃ¹ng tuyá»ƒn sinh...
2. **Quy cháº¿ quáº£n lÃ½ há»c viÃªn / Student Management Regulations**  
   - Quyá»n vÃ  nghÄ©a vá»¥, cháº¿ Ä‘á»™ chÃ­nh sÃ¡ch, khen thÆ°á»Ÿng â€“ ká»· luáº­t, sinh hoáº¡t, rÃ¨n luyá»‡n...
3. **Quy cháº¿ Ä‘Ã o táº¡o cÃ¡c trÃ¬nh Ä‘á»™ / Training Regulations**  
   - NgÃ nh/chuyÃªn ngÃ nh, chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o, há»c cháº¿, há»c láº¡i, thÃ´i há»c, tá»‘t nghiá»‡p...
4. **Quy Ä‘á»‹nh vá» thi, kiá»ƒm tra, Ä‘Ã¡nh giÃ¡ / Examination and Assessment Rules**  
   - HÃ¬nh thá»©c thi/kiá»ƒm tra, thang Ä‘iá»ƒm, Ä‘iá»u kiá»‡n dá»± thi, phÃºc kháº£o, báº£o lÆ°u...
5. **Quy Ä‘á»‹nh vá» kiá»ƒm Ä‘á»‹nh vÃ  báº£o Ä‘áº£m cháº¥t lÆ°á»£ng Ä‘Ã o táº¡o / Quality Assurance**  
   - TiÃªu chuáº©n, quy trÃ¬nh, hoáº¡t Ä‘á»™ng báº£o Ä‘áº£m vÃ  nÃ¢ng cao cháº¥t lÆ°á»£ng Ä‘Ã o táº¡o...

---

### 1. Phong cÃ¡ch & ngÃ´n ngá»¯ tráº£ lá»i / Response Style & Language

{"- **ALWAYS respond in ENGLISH** as the user has selected English language preference." if language == "en" else "- **LUÃ”N tráº£ lá»i báº±ng TIáº¾NG VIá»†T** vÃ¬ ngÆ°á»i dÃ¹ng Ä‘Ã£ chá»n ngÃ´n ngá»¯ Tiáº¿ng Viá»‡t."}
- VÄƒn phong / Style:
  - **ThÃ¢n thiá»‡n, dá»… hiá»ƒu nhÆ°ng váº«n trang trá»ng / Friendly but formal**
  - Háº¡n cháº¿ láº·p láº¡i nguyÃªn vÄƒn; **tÃ³m táº¯t, gáº¡ch Ä‘áº§u dÃ²ng, chia má»¥c rÃµ rÃ ng / Use summaries and bullet points**

---

### 2. CÃ¡ch trÃ¬nh bÃ y má»™t cÃ¢u tráº£ lá»i / Answer Structure

1. **Pháº§n má»Ÿ Ä‘áº§u â€“ TÃ“M Táº®T NHANH / Opening - Quick Summary (3â€“5 lines)**  
   - Váº¥n Ä‘á» Ä‘ang Ä‘Æ°á»£c há»i / The topic being asked
   - Äá»‘i tÆ°á»£ng Ã¡p dá»¥ng / Who this applies to
   - Má»‘c thá»i gian hoáº·c Ã½ chÃ­nh / Key dates or main points

2. **Pháº§n ná»™i dung chi tiáº¿t â€“ TRÃŒNH BÃ€Y CÃ“ Cáº¤U TRÃšC / Detailed Content**  
   - Sá»­ dá»¥ng tiÃªu Ä‘á», gáº¡ch Ä‘áº§u dÃ²ng rÃµ rÃ ng / Use clear headings and bullets

3. **Káº¾T THÃšC báº±ng cÃ¢u nháº¯c vá» tÃ i liá»‡u tham kháº£o / End with reference reminder (REQUIRED)**  
   {"- English: 'ðŸ“„ **Reference Documents:** For full details and original documents, please refer to the attachments displayed below by the system.'" if language == "en" else "- Tiáº¿ng Viá»‡t: 'ðŸ“„ **TÃ i liá»‡u tham kháº£o:** ThÃ´ng tin chi tiáº¿t vÃ  toÃ n vÄƒn vÄƒn báº£n, báº¡n cÃ³ thá»ƒ xem thÃªm á»Ÿ pháº§n tÃ i liá»‡u/thÃ´ng bÃ¡o kÃ¨m theo mÃ  há»‡ thá»‘ng Ä‘Ã£ hiá»ƒn thá»‹ bÃªn dÆ°á»›i.'"}

---

### 3. Æ¯u tiÃªn tÃ i liá»‡u chÃ­nh thá»©c / Prioritize Official Documents

- **LuÃ´n Æ°u tiÃªn thÃ´ng tin trong pháº§n "THÃ”NG TIN TÃ€I LIá»†U"** / Always prioritize information from the provided documents.
- {"Translate and explain Vietnamese documents in English for the user." if language == "en" else "CÃ³ thá»ƒ diá»…n Ä‘áº¡t láº¡i, tÃ³m táº¯t Ä‘á»ƒ ngÆ°á»i dÃ¹ng dá»… hiá»ƒu hÆ¡n."}

---

### 4. Khi thiáº¿u thÃ´ng tin / When Information is Missing

{"1. Start with: '**This information is not explicitly available in the provided university documents, however I can share some general reference information as follows:**'" if language == "en" else "1. Má»Ÿ Ä‘áº§u báº±ng: '**ThÃ´ng tin nÃ y chÆ°a cÃ³ trong tÃ i liá»‡u cá»§a trÆ°á»ng, tuy nhiÃªn tÃ´i cÃ³ thá»ƒ cung cáº¥p cho báº¡n má»™t sá»‘ thÃ´ng tin tham kháº£o chung nhÆ° sau:**'"}
2. {"Provide general knowledge and recommend contacting the relevant department." if language == "en" else "Dá»±a trÃªn kiáº¿n thá»©c chung vÃ  khuyáº¿n khÃ­ch liÃªn há»‡ Ä‘Æ¡n vá»‹ chá»©c nÄƒng."}

---

### 5. YÃªu cáº§u Ä‘á»‹nh dáº¡ng / Formatting (Markdown)

- **TiÃªu Ä‘á» chÃ­nh / Main headings:** dÃ¹ng `**TiÃªu Ä‘á»**`
- **Danh sÃ¡ch / Lists:** dÃ¹ng `- ` hoáº·c `1. `
- **ThÃ´ng tin quan trá»ng / Important info:** dÃ¹ng `**LÆ°u Ã½ quan trá»ng:**` hoáº·c `**Important:**`
- **KhÃ´ng chÃ¨n trÃ­ch dáº«n nguá»“n dáº¡ng [1], [2]...** / No citation numbers needed

---

### 6. YÃªu cáº§u chung quan trá»ng / Important General Requirements

- LuÃ´n cung cáº¥p **cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§, chi tiáº¿t vÃ  há»¯u Ã­ch nháº¥t** / Always provide complete, detailed, and helpful answers.
- **Tá»•ng há»£p, há»‡ thá»‘ng hÃ³a** thÃ´ng tin / Synthesize and organize information.
- **{"REMEMBER: ALL responses must be in ENGLISH" if language == "en" else "NHá»š: Táº¥t cáº£ cÃ¢u tráº£ lá»i pháº£i báº±ng TIáº¾NG VIá»†T"}**"""

    def create_user_prompt(
        self, query: str, context: str, memory_context: str = "", language: str = "vi"
    ) -> str:
        """
        Create user prompt with query, context, and memory

        Args:
            query: User query
            context: Retrieved context from documents
            memory_context: Conversation memory context (optional)
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            Formatted user prompt
        """
        memory_section = ""
        if memory_context:
            memory_section = f"""
NGá»® Cáº¢NH Há»˜I THOáº I TRÆ¯á»šC / PREVIOUS CONVERSATION CONTEXT:
{memory_context}

"""

        # Language-specific instructions
        if language == "en":
            lang_instruction = """**LANGUAGE REQUIREMENT: ENGLISH**
You MUST respond ENTIRELY in ENGLISH. The user has selected English as their preferred language.
- Translate ALL content to English, including explanations, instructions, and summaries.
- You may keep Vietnamese proper nouns (names of schools, documents, regulations) in their original form when necessary.
- All headings, bullet points, and explanations MUST be in English."""
            ending_note = "ðŸ“„ **Reference Documents:** For full details and original documents, please refer to the attachments displayed below by the system."
        else:
            lang_instruction = """**YÃŠU Cáº¦U Vá»€ NGÃ”N NGá»®: TIáº¾NG VIá»†T**
Báº¡n PHáº¢I tráº£ lá»i hoÃ n toÃ n báº±ng TIáº¾NG VIá»†T. NgÆ°á»i dÃ¹ng Ä‘Ã£ chá»n Tiáº¿ng Viá»‡t lÃ m ngÃ´n ngá»¯ Æ°a thÃ­ch."""
            ending_note = "ðŸ“„ **TÃ i liá»‡u tham kháº£o:** ThÃ´ng tin chi tiáº¿t vÃ  toÃ n vÄƒn vÄƒn báº£n, báº¡n cÃ³ thá»ƒ xem thÃªm á»Ÿ pháº§n tÃ i liá»‡u/thÃ´ng bÃ¡o kÃ¨m theo mÃ  há»‡ thá»‘ng Ä‘Ã£ hiá»ƒn thá»‹ bÃªn dÆ°á»›i."

        return f"""Dá»±a trÃªn thÃ´ng tin tÃ i liá»‡u sau Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch **CHI TIáº¾T, TOÃ€N DIá»†N vÃ  CHÃNH XÃC** nháº¥t cÃ³ thá»ƒ.

{lang_instruction}

{memory_section}THÃ”NG TIN TÃ€I LIá»†U / DOCUMENT INFORMATION (cÃ¡c thÃ´ng bÃ¡o/quy cháº¿/tÃ i liá»‡u chÃ­nh thá»©c):
{context}

CÃ‚U Há»ŽI Cá»¦A NGÆ¯á»œI DÃ™NG / USER QUESTION:
{query}

**HÆ¯á»šNG DáºªN TRáº¢ Lá»œI / RESPONSE GUIDELINES:**
- {"Respond ENTIRELY in ENGLISH." if language == "en" else "Tráº£ lá»i hoÃ n toÃ n báº±ng TIáº¾NG VIá»†T."}
- **Báº®T Äáº¦U / START** vá»›i **TÃ“M Táº®T NGáº®N / BRIEF SUMMARY (3â€“5 points)**
- Sau Ä‘Ã³ trÃ¬nh bÃ y **CHI TIáº¾T, CÃ“ Cáº¤U TRÃšC / DETAILED & STRUCTURED**
- **Báº®T BUá»˜C Káº¾T THÃšC / MUST END** vá»›i: "{ending_note}"
- **KHÃ”NG** káº¿t thÃºc báº±ng cÃ¢u há»i khÃ¡c / Do NOT end with another question
- TrÃ¬nh bÃ y báº±ng **Markdown** vá»›i tiÃªu Ä‘á», gáº¡ch Ä‘áº§u dÃ²ng / Use Markdown formatting

{"**IMPORTANT: ALL text must be in ENGLISH (except proper nouns).**" if language == "en" else ""}

Tráº£ lá»i / Response:"""

    def _rewrite_query_with_history(
        self, query: str, history: List[Dict[str, str]]
    ) -> str:
        """
        Rewrite the user's query using conversation history for better context.
        """
        if not history:
            return query

        formatted_history = "\n".join(
            [f"{msg['role']}: {msg['content']}" for msg in history]
        )

        rewrite_prompt = f"""Dá»±a vÃ o lá»‹ch sá»­ trÃ² chuyá»‡n sau Ä‘Ã¢y, hÃ£y viáº¿t láº¡i cÃ¢u há»i cuá»‘i cÃ¹ng cá»§a ngÆ°á»i dÃ¹ng thÃ nh má»™t cÃ¢u há»i Ä‘á»™c láº­p, Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh Ä‘á»ƒ cÃ³ thá»ƒ dÃ¹ng cho viá»‡c tÃ¬m kiáº¿m thÃ´ng tin.

### Lá»‹ch sá»­ trÃ² chuyá»‡n:
{formatted_history}

### CÃ¢u há»i cuá»‘i cÃ¹ng cá»§a ngÆ°á»i dÃ¹ng:
{query}

### CÃ¢u há»i Ä‘á»™c láº­p, Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh:"""

        rewritten_query = query  # Default to original query
        try:
            log.info("Rewriting query with history...")
            if LLM_PROVIDER.lower() == "gemini":
                response = gemini_service.generate_response(
                    prompt=rewrite_prompt, temperature=0.0
                )
                if response:
                    rewritten_query = response.strip()
            elif LLM_PROVIDER.lower() == "ollama":
                response = self.ollama_service.generate_response(
                    prompt=rewrite_prompt,
                    system_prompt="Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn viáº¿t láº¡i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng thÃ nh má»™t cÃ¢u há»i Ä‘áº§y Ä‘á»§ ngá»¯ cáº£nh dá»±a trÃªn lá»‹ch sá»­ trÃ² chuyá»‡n.",
                    temperature=0.0,
                )
                if response:
                    rewritten_query = response.strip()

            if rewritten_query != query:
                log.info(f"Original query: '{query}'")
                log.info(f"Rewritten query: '{rewritten_query}'")
            else:
                log.info("Query does not need rewriting.")

            return rewritten_query

        except Exception as e:
            log.error(f"Error during query rewriting: {e}")
            return query  # Fallback to original query on error

    def generate_answer(
        self,
        query: str,
        conversation_id: Optional[str] = None,
        conversation_history: Optional[List[dict]] = None,
        images: Optional[List[Any]] = None,
        language: str = "vi",  # Add language parameter
    ) -> Dict[str, Any]:
        """
        Generate answer using RAG approach

        Args:
            query: User query
            conversation_id: Optional conversation ID
            conversation_history: Optional conversation history
            images: Optional list of images for vision analysis
            language: Response language - 'vi' for Vietnamese (default) or 'en' for English

        Returns:
            Dictionary with answer, sources, confidence, and conversation_id
        """
        try:
            # Handle image-based queries using Gemini Vision
            if images and len(images) > 0:
                return self._generate_vision_answer(
                    query=query,
                    images=images,
                    conversation_id=conversation_id,
                    language=language,  # Pass language to vision handler
                )

            # Create new conversation if needed
            if not conversation_id:
                conversation_id = str(uuid.uuid4())
                self.conversations[conversation_id] = []
            elif conversation_id not in self.conversations:
                self.conversations[conversation_id] = []

            # Use conversation history if provided
            if conversation_history and not self.conversations[conversation_id]:
                # Convert conversation history to internal format
                for message in conversation_history:
                    # Kiá»ƒm tra xem message cÃ³ chá»©a 'role' vÃ  'content' khÃ´ng
                    if (
                        isinstance(message, dict)
                        and "role" in message
                        and "content" in message
                    ):
                        if message["role"] in ["user", "assistant"]:
                            self.conversations[conversation_id].append(
                                {"role": message["role"], "content": message["content"]}
                            )

            # Step 1: Normalize the user's question using Gemini AI
            log.info(f"Original query: {query}")
            normalized_query = normalize_question(query)
            log.info(f"Normalized query: {normalized_query}")

            # Track if normalization was applied
            normalization_applied = (
                normalized_query != query
            ) and ENABLE_GEMINI_NORMALIZATION

            # Step 1.5: Get persistent memory context (sliding window + summarization)
            memory_context = ""
            try:
                conv_context = self.memory_service.get_conversation_context(
                    conversation_id=conversation_id,
                    query=normalized_query,
                    include_memory_search=True,
                )
                if conv_context.has_long_term_memory or conv_context.recent_messages:
                    memory_context = self.memory_service.format_context_for_prompt(
                        conv_context
                    )
                    log.info(
                        f"ðŸ§  Loaded memory context: {len(conv_context.memory_summaries)} summaries, {len(conv_context.recent_messages)} recent messages"
                    )
            except Exception as mem_error:
                log.warning(f"Could not load memory context: {mem_error}")

            # Step 2: Rewrite query using conversation history for context
            current_history = self.conversations.get(conversation_id, [])
            rewritten_query = self._rewrite_query_with_history(
                normalized_query, current_history
            )

            # Step 3: Retrieve relevant chunks using the normalized and rewritten query
            relevant_chunks = self.retrieve_relevant_chunks(rewritten_query)

            # Create formatted context from chunks
            context = self.create_context(relevant_chunks)

            # Get source documents (backward compatible - just filenames)
            sources = []
            for chunk in relevant_chunks:
                source = chunk.get("source_file", "") or chunk.get("source", "")
                if source and source not in sources:
                    sources.append(source)

            # Build detailed source references
            source_references = []
            for chunk in relevant_chunks:
                chunk_id = chunk.get("chunk_id", "")
                content = chunk.get("content", "")
                # Create a snippet (first 200 chars, ending at a word boundary)
                snippet = content[:200]
                if len(content) > 200:
                    last_space = snippet.rfind(" ")
                    if last_space > 150:
                        snippet = snippet[:last_space]
                    snippet += "..."

                # Use the best available score (rerank > combined > dense)
                relevance_score = (
                    chunk.get("rerank_score")
                    or chunk.get("combined_score")
                    or chunk.get("dense_score")
                    or 0.0
                )
                # Normalize rerank score if it's out of 0-1 range (cross-encoder scores can be -10 to 10)
                if relevance_score > 1.0:
                    relevance_score = min(
                        1.0, (relevance_score + 10) / 20
                    )  # Normalize to 0-1
                elif relevance_score < 0:
                    relevance_score = max(0.0, (relevance_score + 10) / 20)

                source_ref = {
                    "chunk_id": str(chunk_id),
                    "filename": chunk.get("source_file", "") or chunk.get("source", ""),
                    "page_number": chunk.get("page_number"),
                    "heading": chunk.get("heading_text"),
                    "content_snippet": snippet,
                    "full_content": content,
                    "relevance_score": relevance_score,
                    "dense_score": chunk.get("dense_score"),
                    "sparse_score": chunk.get("sparse_score"),
                }
                source_references.append(source_ref)

            # Create system prompt and user prompt with memory context
            system_prompt = self.create_system_prompt(language=language)
            user_prompt = self.create_user_prompt(
                query, context, memory_context, language=language
            )

            # Log context and prompts for debugging
            log.info(f"Context created with {len(relevant_chunks)} chunks")
            if memory_context:
                log.info(
                    f"ðŸ§  Including memory context in prompt ({len(memory_context)} chars)"
                )
            log.debug(f"System prompt: {system_prompt[:200]}...")
            log.debug(f"Full context sent to LLM:\n{context}")
            log.debug(f"User prompt: {user_prompt[:200]}...")

            # Generate answer using the configured LLM provider
            answer = None
            if LLM_PROVIDER.lower() == "gemini":
                log.info("Calling Gemini service to generate response...")
                # Gemini API works best with a single, consolidated prompt
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                answer = gemini_service.generate_response(prompt=full_prompt)

            elif LLM_PROVIDER.lower() == "ollama":
                log.info("Calling Ollama service to generate response...")
                answer = self.ollama_service.generate_response(
                    prompt=user_prompt, system_prompt=system_prompt, temperature=0.7
                )
            else:
                log.error(f"Unsupported LLM_PROVIDER configured: {LLM_PROVIDER}")
                answer = "Lá»—i: NhÃ  cung cáº¥p LLM khÃ´ng Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng."

            log.info(f"LLM response received: {answer is not None}")
            if answer:
                log.debug(f"Answer preview: {answer[:100]}...")

            # Calculate confidence based on relevance scores
            if relevant_chunks:
                # Get the best available score for each chunk
                scores = []
                for chunk in relevant_chunks:
                    score = (
                        chunk.get("rerank_score")
                        or chunk.get("combined_score")
                        or chunk.get("dense_score")
                        or 0.0
                    )
                    # Normalize if needed (cross-encoder scores can be -10 to 10)
                    if score > 1.0:
                        score = min(1.0, (score + 10) / 20)
                    elif score < 0:
                        score = max(0.0, (score + 10) / 20)
                    scores.append(score)

                avg_score = sum(scores) / len(scores)
                confidence = min(max(avg_score, 0.0), 1.0)
                log.info(
                    f"Calculated confidence: {confidence:.3f} from avg score: {avg_score:.3f}"
                )
            else:
                confidence = 0.0
                log.warning("No relevant chunks found, confidence set to 0")

            # Handle case where LLM provider returns None or empty
            if answer is None:
                log.error("LLM provider returned None response")
                answer = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau.\n\nBáº¡n cÃ²n cÃ³ tháº¯c máº¯c gÃ¬ khÃ¡c khÃ´ng? TÃ´i sáºµn sÃ ng há»— trá»£ thÃªm!"
                confidence = 0.0
            elif not answer.strip():
                log.error("LLM provider returned empty response")
                answer = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau.\n\nBáº¡n cÃ²n cÃ³ tháº¯c máº¯c gÃ¬ khÃ¡c khÃ´ng? TÃ´i sáºµn sÃ ng há»— trá»£ thÃªm!"
                confidence = 0.0
            else:
                log.info(f"Raw answer from LLM: {repr(answer)}")
                # Add engagement prompt if not already present
                answer = self._add_engagement_prompt(answer, query, language)
                log.info(f"Using answer with engagement prompt: {repr(answer)}")

            # Update conversation history (in-memory cache)
            self.conversations[conversation_id].append(
                {"role": "user", "content": query}
            )
            self.conversations[conversation_id].append(
                {"role": "assistant", "content": answer}
            )

            # Limit conversation history (in-memory)
            if len(self.conversations[conversation_id]) > 10:
                self.conversations[conversation_id] = self.conversations[
                    conversation_id
                ][-10:]

            # Save to persistent memory with sliding window + summarization
            try:
                self.memory_service.add_exchange(
                    conversation_id=conversation_id,
                    user_message=query,
                    assistant_message=answer,
                    metadata={
                        "confidence": confidence,
                        "sources": sources,
                        "normalized_query": (
                            normalized_query if normalization_applied else None
                        ),
                    },
                )
                log.debug(
                    f"ðŸ’¾ Saved exchange to persistent memory for {conversation_id}"
                )
            except Exception as mem_error:
                log.warning(f"Could not save to persistent memory: {mem_error}")

            # Save conversation to PostgreSQL (legacy)
            processing_time = time.time() - time.time()  # Will be calculated properly
            try:
                self.db_service.save_conversation(
                    conversation_id=conversation_id,
                    user_message=query,
                    assistant_response=answer,
                    sources=sources,
                    confidence=confidence,
                    processing_time=0.0,  # Processing time will be set at API level
                )
            except Exception as save_error:
                log.warning(f"Could not save conversation to DB: {save_error}")

            # Detect if chart visualization is needed
            chart_data = self._detect_chart_request(query, answer)
            if chart_data:
                log.info(f"ðŸ“Š Generated {len(chart_data)} chart(s) for visualization")

            # Get attachments using hybrid approach: chunk linking + keyword matching
            attachments = []
            try:
                log.info("ðŸ” Starting attachment retrieval...")
                attachment_ids_found = set()

                # Strategy 1: Get attachments linked to retrieved chunks
                if relevant_chunks:
                    log.info(
                        f"Strategy 1: Checking {len(relevant_chunks)} chunks for linked attachments"
                    )
                    chunk_ids = [
                        chunk.get("id") for chunk in relevant_chunks if chunk.get("id")
                    ]
                    if chunk_ids:
                        chunk_attachments = (
                            self.attachment_service.get_attachments_by_chunk_ids(
                                chunk_ids
                            )
                        )
                        for att in chunk_attachments:
                            attachment_ids_found.add(att.id)
                            attachments.append(
                                {
                                    "file_name": att.file_name,
                                    "file_type": att.file_type,
                                    "download_url": att.download_url,
                                    "description": att.description,
                                    "file_size": att.file_size,
                                }
                            )

                # Strategy 2: Search attachments by keywords from query
                from src.services.smart_attachment_matcher import SmartAttachmentMatcher

                query_keywords = SmartAttachmentMatcher.extract_keywords_from_query(
                    query
                )
                log.info(f"Strategy 2: Extracted keywords from query: {query_keywords}")
                if query_keywords:
                    keyword_attachments = self.attachment_service.search_attachments(
                        keywords=query_keywords
                    )
                    log.info(
                        f"Strategy 2: Found {len(keyword_attachments)} attachments by keyword search"
                    )

                    # Score and filter keyword-matched attachments
                    for att in keyword_attachments:
                        if att.id not in attachment_ids_found:
                            # Calculate relevance score
                            score = SmartAttachmentMatcher.score_attachment_relevance(
                                att.keywords or [], query_keywords
                            )

                            # Only include if relevance score is high enough
                            if score > 0.3:  # Threshold
                                attachment_ids_found.add(att.id)
                                attachments.append(
                                    {
                                        "file_name": att.file_name,
                                        "file_type": att.file_type,
                                        "download_url": att.download_url,
                                        "description": att.description,
                                        "file_size": att.file_size,
                                    }
                                )
                                log.info(
                                    f"ðŸ“Ž Found keyword-matched attachment: {att.file_name} (score: {score:.2f})"
                                )

                if attachments:
                    log.info(
                        f"ðŸ“Ž Found {len(attachments)} attachment(s) for this response"
                    )
            except Exception as att_error:
                log.warning(f"Could not retrieve attachments: {att_error}")

            return {
                "answer": answer,
                "sources": sources,
                "source_references": source_references,
                "attachments": attachments,
                "confidence": confidence,
                "conversation_id": conversation_id,
                "normalization_applied": normalization_applied,
                "original_query": query if normalization_applied else None,
                "normalized_query": normalized_query if normalization_applied else None,
                "chart_data": chart_data,  # Charts for visualization
                "images": [],  # Will be populated if images are found in sources
            }

        except Exception as e:
            log.error(f"Error generating answer: {e}")
            return {
                "answer": "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y. Vui lÃ²ng thá»­ láº¡i sau.",
                "sources": [],
                "source_references": [],
                "attachments": [],
                "confidence": 0.0,
                "conversation_id": conversation_id or str(uuid.uuid4()),
                "chart_data": [],
                "images": [],
            }

    def get_conversation_history(self, conversation_id: str) -> List[Dict[str, Any]]:
        """
        Get conversation history

        Args:
            conversation_id: Conversation ID

        Returns:
            List of conversation exchanges
        """
        return self.conversations.get(conversation_id, [])

    def check_system_health(self) -> Dict[str, Any]:
        """
        Check health of all RAG components

        Returns:
            Health status dictionary
        """
        health_status = {"overall_status": "healthy", "components": {}}

        # Check Ollama
        ollama_health = self.ollama_service.check_health()
        health_status["components"]["ollama"] = ollama_health

        # Check PostgreSQL + pgvector
        try:
            db_stats = self.db_service.get_database_stats()
            health_status["components"]["database"] = {
                "status": "healthy",
                "stats": db_stats,
            }
        except Exception as e:
            health_status["components"]["database"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check Hybrid Retrieval Service (BM25 + pgvector)
        try:
            health_status["components"]["hybrid_retrieval"] = {
                "status": "healthy",
                "type": "BM25 + pgvector",
            }
        except Exception as e:
            health_status["components"]["hybrid_retrieval"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check embedding service
        try:
            embedding_dim = self.embedding_service.get_embedding_dimension()
            health_status["components"]["embedding"] = {
                "status": "healthy",
                "dimension": embedding_dim,
            }
        except Exception as e:
            health_status["components"]["embedding"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Check ingestion service
        try:
            health_status["components"]["ingestion"] = {
                "status": "healthy",
                "type": "PDF file watcher",
            }
        except Exception as e:
            health_status["components"]["ingestion"] = {
                "status": "unhealthy",
                "error": str(e),
            }

        # Determine overall status
        component_statuses = [
            comp.get("status", "unknown")
            for comp in health_status["components"].values()
        ]
        if any(status == "unhealthy" for status in component_statuses):
            health_status["overall_status"] = "unhealthy"
        elif any(status == "unknown" for status in component_statuses):
            health_status["overall_status"] = "degraded"

        return health_status

    def cleanup(self):
        """Cleanup resources - stop ingestion service"""
        try:
            if hasattr(self, "ingestion_service"):
                log.info("Stopping ingestion service...")
                self.ingestion_service.stop_watching()
                log.info("Ingestion service stopped successfully.")
        except Exception as e:
            log.error(f"Error during cleanup: {e}")
