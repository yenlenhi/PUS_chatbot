# CHÆ¯Æ NG 3: CÆ  Sá» LÃ THUYáº¾T

## Má»¤C Lá»¤C

1. [RAG - Retrieval-Augmented Generation](#31-rag---retrieval-augmented-generation)
2. [Vector Embeddings vÃ  Semantic Search](#32-vector-embeddings-vÃ -semantic-search)
3. [BM25 vÃ  Sparse Retrieval](#33-bm25-vÃ -sparse-retrieval)
4. [Hybrid Search](#34-hybrid-search)
5. [Cross-Encoder Reranking](#35-cross-encoder-reranking)
6. [Large Language Models](#36-large-language-models)
7. [CÃ´ng nghá»‡ Backend](#37-cÃ´ng-nghá»‡-backend)
8. [CÃ´ng nghá»‡ Frontend](#38-cÃ´ng-nghá»‡-frontend)
9. [CÆ¡ sá»Ÿ dá»¯ liá»‡u](#39-cÆ¡-sá»Ÿ-dá»¯-liá»‡u)
10. [Containerization vÃ  Deployment](#310-containerization-vÃ -deployment)

---

## 3.1. RAG - Retrieval-Augmented Generation

### 3.1.1. KhÃ¡i niá»‡m

**RAG (Retrieval-Augmented Generation)** lÃ  má»™t kiáº¿n trÃºc AI tiÃªn tiáº¿n káº¿t há»£p hai thÃ nh pháº§n chÃ­nh:

1. **Retrieval (Truy xuáº¥t)**: TÃ¬m kiáº¿m thÃ´ng tin liÃªn quan tá»« knowledge base
2. **Generation (Sinh vÄƒn báº£n)**: Sá»­ dá»¥ng LLM Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn thÃ´ng tin Ä‘Ã£ truy xuáº¥t

### 3.1.2. Táº¡i sao cáº§n RAG?

**Váº¥n Ä‘á» cá»§a LLM thuáº§n tÃºy:**

```
âŒ LLM standalone:
   - Kiáº¿n thá»©c giá»›i háº¡n (cutoff date)
   - KhÃ´ng biáº¿t thÃ´ng tin cá»¥ thá»ƒ cá»§a tá»• chá»©c
   - CÃ³ thá»ƒ "hallucinate" (bá»‹a Ä‘áº·t thÃ´ng tin)
   - KhÃ´ng cáº­p nháº­t real-time
   
âœ… RAG Solution:
   - Truy cáº­p knowledge base cáº­p nháº­t
   - Dá»¯ liá»‡u domain-specific
   - Giáº£m hallucination
   - CÃ³ nguá»“n trÃ­ch dáº«n rÃµ rÃ ng
```

### 3.1.3. Kiáº¿n trÃºc RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Query: "Äiá»u kiá»‡n xÃ©t há»c bá»•ng lÃ  gÃ¬?"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: QUERY PROCESSING        â”‚
â”‚  - Normalize text                â”‚
â”‚  - Generate query embedding      â”‚
â”‚  - Expand query (optional)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: RETRIEVAL               â”‚
â”‚  - Vector search (semantic)      â”‚
â”‚  - BM25 search (keyword)         â”‚
â”‚  - Hybrid fusion                 â”‚
â”‚  â†’ Top-K documents (e.g., K=20)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: RERANKING               â”‚
â”‚  - Cross-encoder scoring         â”‚
â”‚  - Re-sort by relevance          â”‚
â”‚  â†’ Top-N documents (e.g., N=5)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: CONTEXT GENERATION      â”‚
â”‚  - Format retrieved docs         â”‚
â”‚  - Add metadata (source, page)   â”‚
â”‚  - Build prompt                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: GENERATION              â”‚
â”‚  - Send to LLM (Gemini)          â”‚
â”‚  - Generate answer               â”‚
â”‚  - Include citations             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Response: "Theo quy cháº¿ Ä‘Ã o táº¡o, sinh viÃªn cáº§n Ä‘áº¡t 
Ä‘iá»ƒm trung bÃ¬nh tá»« 3.5 trá»Ÿ lÃªn... [Nguá»“n: QUY_CHE_DAO_TAO.pdf, trang 12]"
```

### 3.1.4. Lá»£i Ã­ch cá»§a RAG

| TiÃªu chÃ­ | LLM thuáº§n | RAG |
|----------|-----------|-----|
| **Äá»™ chÃ­nh xÃ¡c** | Trung bÃ¬nh | Cao |
| **TÃ­nh cáº­p nháº­t** | Cá»‘ Ä‘á»‹nh | Real-time |
| **TrÃ­ch dáº«n nguá»“n** | KhÃ´ng cÃ³ | CÃ³ |
| **Hallucination** | Cao | Tháº¥p |
| **Domain knowledge** | Chung chung | ChuyÃªn biá»‡t |
| **Chi phÃ­** | Tháº¥p | Trung bÃ¬nh |

### 3.1.5. á»¨ng dá»¥ng RAG trong há»‡ thá»‘ng

Há»‡ thá»‘ng chatbot Ã¡p dá»¥ng RAG Ä‘á»ƒ:

- âœ… Tráº£ lá»i cÃ¢u há»i vá» quy cháº¿, quy Ä‘á»‹nh cá»§a trÆ°á»ng
- âœ… Cung cáº¥p thÃ´ng tin tuyá»ƒn sinh, há»c bá»•ng
- âœ… HÆ°á»›ng dáº«n thá»§ tá»¥c hÃ nh chÃ­nh
- âœ… TrÃ­ch dáº«n chÃ­nh xÃ¡c tá»« tÃ i liá»‡u gá»‘c
- âœ… Cáº­p nháº­t thÃ´ng tin khi cÃ³ tÃ i liá»‡u má»›i

### 3.1.6. CÃ´ng thá»©c Ä‘Ã¡nh giÃ¡ RAG

**Precision (Äá»™ chÃ­nh xÃ¡c):**
```
Precision = (Sá»‘ documents liÃªn quan Ä‘Æ°á»£c truy xuáº¥t) / (Tá»•ng sá»‘ documents Ä‘Æ°á»£c truy xuáº¥t)
```

**Recall (Äá»™ phá»§):**
```
Recall = (Sá»‘ documents liÃªn quan Ä‘Æ°á»£c truy xuáº¥t) / (Tá»•ng sá»‘ documents liÃªn quan)
```

**F1-Score:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

**Mean Reciprocal Rank (MRR):**
```
MRR = (1/n) Ã— Î£(1/rank_i)
```
Trong Ä‘Ã³ `rank_i` lÃ  vá»‹ trÃ­ cá»§a document liÃªn quan Ä‘áº§u tiÃªn trong káº¿t quáº£ truy xuáº¥t.

---

## 3.2. Vector Embeddings vÃ  Semantic Search

### 3.2.1. KhÃ¡i niá»‡m Vector Embeddings

**Embedding** lÃ  quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i text thÃ nh vector sá»‘ trong khÃ´ng gian nhiá»u chiá»u, nÆ¡i:
- VÄƒn báº£n cÃ³ nghÄ©a tÆ°Æ¡ng tá»± â†’ Vectors gáº§n nhau
- VÄƒn báº£n khÃ¡c nghÄ©a â†’ Vectors xa nhau

**VÃ­ dá»¥:**
```python
Text: "sinh viÃªn xin nghá»‰ há»c"
â†“ Embedding Model
Vector: [0.23, -0.45, 0.67, ..., 0.12]  # 384 dimensions

Text: "há»c sinh xin phÃ©p váº¯ng máº·t"
â†“ Embedding Model  
Vector: [0.25, -0.43, 0.65, ..., 0.15]  # Gáº§n vá»›i vector trÃªn!

Text: "thá»i tiáº¿t hÃ´m nay"
â†“ Embedding Model
Vector: [-0.78, 0.32, -0.12, ..., 0.89]  # Xa vá»›i 2 vectors trÃªn
```

### 3.2.2. MÃ´ hÃ¬nh Embedding: Vietnamese SBERT

Há»‡ thá»‘ng sá»­ dá»¥ng **keepitreal/vietnamese-sbert**:

```python
Model: keepitreal/vietnamese-sbert
Base: sentence-transformers
Dimension: 384
Language: Vietnamese
Training: Contrastive learning on Vietnamese corpus
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ°á»£c huáº¥n luyá»‡n trÃªn corpus tiáº¿ng Viá»‡t
- âœ… Hiá»ƒu ngá»¯ nghÄ©a tiáº¿ng Viá»‡t tá»‘t
- âœ… KÃ­ch thÆ°á»›c vector nhá» (384 dim) â†’ nhanh
- âœ… Cháº¥t lÆ°á»£ng tá»‘t cho semantic search

**Kiáº¿n trÃºc SBERT:**
```
Input Text
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tokenization    â”‚
â”‚  (WordPiece)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT Encoder    â”‚
â”‚  (12 layers)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pooling Layer   â”‚
â”‚  (Mean pooling)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense Layer     â”‚
â”‚  (384 units)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
384-dimensional Vector
```

### 3.2.3. Semantic Search

**Quy trÃ¬nh:**

```
1. Indexing Phase (Offline):
   Documents â†’ Embeddings â†’ Store in pgvector

2. Query Phase (Online):
   Query â†’ Embedding â†’ Vector Search â†’ Top-K Results
```

**Cosine Similarity:**

Äá»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 2 vectors Ä‘Æ°á»£c tÃ­nh báº±ng cosine cá»§a gÃ³c giá»¯a chÃºng:

```
cosine_similarity(A, B) = (A Â· B) / (||A|| Ã— ||B||)

Trong Ä‘Ã³:
- A Â· B: TÃ­ch vÃ´ hÆ°á»›ng (dot product)
- ||A||: Äá»™ dÃ i (norm) cá»§a vector A
- ||B||: Äá»™ dÃ i (norm) cá»§a vector B

GiÃ¡ trá»‹: [-1, 1]
- 1: HoÃ n toÃ n giá»‘ng nhau
- 0: Trá»±c giao (khÃ´ng liÃªn quan)
- -1: HoÃ n toÃ n ngÆ°á»£c nhau
```

**VÃ­ dá»¥ tÃ­nh toÃ¡n:**
```python
import numpy as np

# Vector A: "sinh viÃªn xin nghá»‰"
A = np.array([0.5, 0.8, 0.2])

# Vector B: "há»c sinh xin phÃ©p"
B = np.array([0.6, 0.7, 0.3])

# TÃ­ch vÃ´ hÆ°á»›ng
dot_product = np.dot(A, B)  # 0.5Ã—0.6 + 0.8Ã—0.7 + 0.2Ã—0.3 = 0.92

# Norm
norm_A = np.linalg.norm(A)  # sqrt(0.5Â² + 0.8Â² + 0.2Â²) = 0.97
norm_B = np.linalg.norm(B)  # sqrt(0.6Â² + 0.7Â² + 0.3Â²) = 0.97

# Cosine similarity
similarity = dot_product / (norm_A * norm_B)  # 0.92 / 0.94 = 0.98
# â†’ Ráº¥t giá»‘ng nhau!
```

### 3.2.4. Vector Search vá»›i pgvector

**pgvector** lÃ  PostgreSQL extension cho vector similarity search:

```sql
-- Create vector column
ALTER TABLE embeddings 
ADD COLUMN embedding vector(384);

-- Create IVFFlat index for fast search
CREATE INDEX embeddings_embedding_idx 
ON embeddings 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Search query
SELECT chunk_id, content, 
       1 - (embedding <=> query_embedding) AS similarity
FROM embeddings
ORDER BY embedding <=> query_embedding
LIMIT 20;
```

**IVFFlat Index:**

```
IVFFlat = Inverted File with Flat compression

Principle:
1. Cluster vectors into N lists (e.g., 100 lists)
2. Each vector belongs to nearest centroid
3. At search time:
   - Find nearest centroids to query
   - Only search vectors in those lists
   - Much faster than brute force

Trade-off:
- Speed: 10-100x faster
- Accuracy: ~95% recall (might miss some results)
```

### 3.2.5. Advantages & Limitations

**Æ¯u Ä‘iá»ƒm Semantic Search:**
- âœ… Hiá»ƒu ngá»¯ nghÄ©a, khÃ´ng chá»‰ tá»« khÃ³a
- âœ… TÃ¬m Ä‘Æ°á»£c vÄƒn báº£n tÆ°Æ¡ng tá»± ngá»¯ nghÄ©a
- âœ… Robust vá»›i typos, synonyms
- âœ… Hoáº¡t Ä‘á»™ng tá»‘t vá»›i queries dÃ i

**Háº¡n cháº¿:**
- âŒ Yáº¿u vá»›i exact keyword matches
- âŒ KÃ©m vá»›i tÃªn riÃªng, mÃ£ sá»‘
- âŒ CÃ³ thá»ƒ bá» sÃ³t káº¿t quáº£ quan trá»ng

â†’ **Giáº£i phÃ¡p: Hybrid Search (káº¿t há»£p vá»›i BM25)**

---

## 3.3. BM25 vÃ  Sparse Retrieval

### 3.3.1. KhÃ¡i niá»‡m BM25

**BM25 (Best Matching 25)** lÃ  thuáº­t toÃ¡n ranking dá»±a trÃªn keyword matching, Ä‘Æ°á»£c cáº£i tiáº¿n tá»« TF-IDF.

**TF-IDF Problems:**
```
TF-IDF cÃ³ váº¥n Ä‘á»:
1. Term frequency khÃ´ng Ä‘Æ°á»£c normalize tá»‘t
2. KhÃ´ng xá»­ lÃ½ document length
3. Saturation: Tá»« xuáº¥t hiá»‡n nhiá»u láº§n bá»‹ Ä‘Ã¡nh giÃ¡ quÃ¡ cao
```

**BM25 Improvements:**
```
BM25 cáº£i thiá»‡n:
1. Term frequency saturation
2. Document length normalization
3. Tunable parameters (k1, b)
```

### 3.3.2. CÃ´ng thá»©c BM25

**BM25 Score:**

```
score(D, Q) = Î£ IDF(qi) Ã— [f(qi, D) Ã— (k1 + 1)] / [f(qi, D) + k1 Ã— (1 - b + b Ã— |D| / avgdl)]

Trong Ä‘Ã³:
- D: Document
- Q: Query = {q1, q2, ..., qn}
- f(qi, D): Táº§n sá»‘ tá»« qi trong document D
- |D|: Äá»™ dÃ i document D (sá»‘ tá»«)
- avgdl: Äá»™ dÃ i trung bÃ¬nh cá»§a documents
- k1: Parameter Ä‘iá»u chá»‰nh term frequency saturation (default: 1.5)
- b: Parameter Ä‘iá»u chá»‰nh document length normalization (default: 0.75)

IDF(qi) = log[(N - n(qi) + 0.5) / (n(qi) + 0.5) + 1]

Trong Ä‘Ã³:
- N: Tá»•ng sá»‘ documents
- n(qi): Sá»‘ documents chá»©a tá»« qi
```

**Ã nghÄ©a cÃ¡c parameters:**

- **k1**: Kiá»ƒm soÃ¡t saturation cá»§a term frequency
  - k1 = 0: Binary (chá»‰ quan tÃ¢m cÃ³/khÃ´ng)
  - k1 cÃ ng lá»›n: Term frequency cÃ ng quan trá»ng
  - ThÆ°á»ng dÃ¹ng: 1.2 - 2.0

- **b**: Kiá»ƒm soÃ¡t áº£nh hÆ°á»Ÿng cá»§a document length
  - b = 0: KhÃ´ng normalize length
  - b = 1: Full normalization
  - ThÆ°á»ng dÃ¹ng: 0.75

### 3.3.3. VÃ­ dá»¥ tÃ­nh BM25

**Corpus:**
```
D1: "sinh viÃªn cáº§n ná»™p Ä‘Æ¡n xin nghá»‰ há»c"
D2: "sinh viÃªn Ä‘áº¡t Ä‘iá»ƒm cao Ä‘Æ°á»£c há»c bá»•ng"
D3: "quy Ä‘á»‹nh vá» há»c bá»•ng cho sinh viÃªn"
```

**Query:** "há»c bá»•ng sinh viÃªn"

**BÆ°á»›c 1: TÃ­nh IDF**
```
N = 3 (tá»•ng sá»‘ documents)

"há»c": xuáº¥t hiá»‡n trong D2, D3 â†’ n = 2
IDF("há»c") = log[(3 - 2 + 0.5) / (2 + 0.5) + 1] = log[1.6] = 0.47

"bá»•ng": xuáº¥t hiá»‡n trong D2, D3 â†’ n = 2
IDF("bá»•ng") = log[1.6] = 0.47

"sinh": xuáº¥t hiá»‡n trong D1, D2, D3 â†’ n = 3
IDF("sinh") = log[(3 - 3 + 0.5) / (3 + 0.5) + 1] = log[1.14] = 0.13

"viÃªn": xuáº¥t hiá»‡n trong D1, D2, D3 â†’ n = 3
IDF("viÃªn") = log[1.14] = 0.13
```

**BÆ°á»›c 2: TÃ­nh score cho D2**
```
D2: "sinh viÃªn Ä‘áº¡t Ä‘iá»ƒm cao Ä‘Æ°á»£c há»c bá»•ng"
|D2| = 8 tá»«
avgdl = (8 + 8 + 7) / 3 = 7.67
k1 = 1.5, b = 0.75

Term "há»c":
- f("há»c", D2) = 1
- Numerator = 1 Ã— (1.5 + 1) = 2.5
- Denominator = 1 + 1.5 Ã— (1 - 0.75 + 0.75 Ã— 8/7.67) = 2.48
- Score = 0.47 Ã— (2.5 / 2.48) = 0.474

Term "bá»•ng":
- f("bá»•ng", D2) = 1
- Score = 0.47 Ã— (2.5 / 2.48) = 0.474

Term "sinh":
- f("sinh", D2) = 1
- Score = 0.13 Ã— (2.5 / 2.48) = 0.131

Term "viÃªn":
- f("viÃªn", D2) = 1
- Score = 0.13 Ã— (2.5 / 2.48) = 0.131

Total score(D2) = 0.474 + 0.474 + 0.131 + 0.131 = 1.21
```

**Káº¿t quáº£:**
```
D3: score = 1.30 (cao nháº¥t - chá»©a cáº£ "há»c bá»•ng sinh viÃªn")
D2: score = 1.21 (trung bÃ¬nh - chá»©a "há»c bá»•ng sinh viÃªn")
D1: score = 0.26 (tháº¥p - chá»‰ chá»©a "sinh viÃªn")
```

### 3.3.4. Implementation vá»›i rank-bm25

```python
from rank_bm25 import BM25Okapi

# Corpus
documents = [
    "sinh viÃªn cáº§n ná»™p Ä‘Æ¡n xin nghá»‰ há»c",
    "sinh viÃªn Ä‘áº¡t Ä‘iá»ƒm cao Ä‘Æ°á»£c há»c bá»•ng",
    "quy Ä‘á»‹nh vá» há»c bá»•ng cho sinh viÃªn"
]

# Tokenize
tokenized_corpus = [doc.split() for doc in documents]

# Build BM25 index
bm25 = BM25Okapi(tokenized_corpus)

# Query
query = "há»c bá»•ng sinh viÃªn"
tokenized_query = query.split()

# Get scores
scores = bm25.get_scores(tokenized_query)
# [0.26, 1.21, 1.30]

# Get top-k
top_docs = bm25.get_top_n(tokenized_query, documents, n=2)
# ["quy Ä‘á»‹nh vá» há»c bá»•ng cho sinh viÃªn", 
#  "sinh viÃªn Ä‘áº¡t Ä‘iá»ƒm cao Ä‘Æ°á»£c há»c bá»•ng"]
```

### 3.3.5. Æ¯u Ä‘iá»ƒm vÃ  Háº¡n cháº¿

**Æ¯u Ä‘iá»ƒm BM25:**
- âœ… Ráº¥t tá»‘t vá»›i exact keyword matches
- âœ… Nhanh, khÃ´ng cáº§n GPU
- âœ… Hoáº¡t Ä‘á»™ng tá»‘t vá»›i tÃªn riÃªng, mÃ£ sá»‘
- âœ… KhÃ´ng cáº§n training
- âœ… Explainable (cÃ³ thá»ƒ giáº£i thÃ­ch káº¿t quáº£)

**Háº¡n cháº¿ BM25:**
- âŒ KhÃ´ng hiá»ƒu ngá»¯ nghÄ©a
- âŒ KhÃ´ng xá»­ lÃ½ synonyms
- âŒ Yáº¿u vá»›i typos
- âŒ YÃªu cáº§u exact word match

---

## 3.4. Hybrid Search

### 3.4.1. Táº¡i sao cáº§n Hybrid Search?

**Váº¥n Ä‘á»:**
```
Vector Search tá»‘t:        "Ä‘iá»u kiá»‡n nháº­n há»c bá»•ng"
                      vs  "yÃªu cáº§u Ä‘á»ƒ Ä‘Æ°á»£c cáº¥p há»c bá»•ng"
                      â†’ Semantic match âœ…

BM25 tá»‘t:                 "mÃ£ sinh viÃªn SV2024001"
                      vs  "SV2024001"
                      â†’ Exact match âœ…
```

**Káº¿t há»£p cáº£ hai â†’ Hybrid Search tá»‘i Æ°u!**

### 3.4.2. Reciprocal Rank Fusion (RRF)

**CÃ´ng thá»©c RRF:**

```
RRF_score(d) = Î£ [1 / (k + rank_i(d))]

Trong Ä‘Ã³:
- d: Document
- rank_i(d): Vá»‹ trÃ­ cá»§a document d trong ranked list i
- k: Constant (thÆ°á»ng = 60)
- Î£: Sum qua táº¥t cáº£ ranked lists (vector + BM25)
```

**VÃ­ dá»¥:**

```
Query: "há»c bá»•ng sinh viÃªn giá»i"

Vector Search Results:
1. Doc A (score: 0.95)
2. Doc B (score: 0.88)
3. Doc C (score: 0.82)
4. Doc D (score: 0.75)

BM25 Results:
1. Doc B (score: 8.5)
2. Doc A (score: 7.2)
3. Doc E (score: 6.8)
4. Doc C (score: 5.9)

RRF Calculation (k=60):

Doc A:
- Vector rank: 1 â†’ 1/(60+1) = 0.0164
- BM25 rank: 2 â†’ 1/(60+2) = 0.0161
- RRF = 0.0164 + 0.0161 = 0.0325

Doc B:
- Vector rank: 2 â†’ 1/(60+2) = 0.0161
- BM25 rank: 1 â†’ 1/(60+1) = 0.0164
- RRF = 0.0161 + 0.0164 = 0.0325

Doc C:
- Vector rank: 3 â†’ 1/(60+3) = 0.0159
- BM25 rank: 4 â†’ 1/(60+4) = 0.0156
- RRF = 0.0159 + 0.0156 = 0.0315

Doc D:
- Vector rank: 4 â†’ 1/(60+4) = 0.0156
- BM25 rank: âˆ (not in BM25 results) â†’ 0
- RRF = 0.0156

Doc E:
- Vector rank: âˆ (not in vector results) â†’ 0
- BM25 rank: 3 â†’ 1/(60+3) = 0.0159
- RRF = 0.0159

Final Ranking:
1. Doc A (RRF: 0.0325)
2. Doc B (RRF: 0.0325)
3. Doc C (RRF: 0.0315)
4. Doc E (RRF: 0.0159)
5. Doc D (RRF: 0.0156)
```

### 3.4.3. Implementation

```python
def hybrid_search(query: str, top_k: int = 20):
    """
    Hybrid search combining vector and BM25
    """
    # 1. Vector search
    query_embedding = embedding_service.get_embedding(query)
    vector_results = vector_search(query_embedding, k=top_k)
    
    # 2. BM25 search
    bm25_results = bm25_search(query, k=top_k)
    
    # 3. RRF fusion
    rrf_scores = {}
    k = 60
    
    # Add vector results
    for rank, (chunk_id, score) in enumerate(vector_results, 1):
        rrf_scores[chunk_id] = 1 / (k + rank)
    
    # Add BM25 results
    for rank, (chunk_id, score) in enumerate(bm25_results, 1):
        if chunk_id in rrf_scores:
            rrf_scores[chunk_id] += 1 / (k + rank)
        else:
            rrf_scores[chunk_id] = 1 / (k + rank)
    
    # 4. Sort by RRF score
    sorted_results = sorted(
        rrf_scores.items(),
        key=lambda x: x[1],
        reverse=True
    )
    
    return sorted_results[:top_k]
```

### 3.4.4. Æ¯u Ä‘iá»ƒm Hybrid Search

| Aspect | Vector Only | BM25 Only | Hybrid |
|--------|------------|-----------|--------|
| Semantic | âœ… | âŒ | âœ… |
| Exact match | âŒ | âœ… | âœ… |
| Synonyms | âœ… | âŒ | âœ… |
| Typos | âœ… | âŒ | âœ… |
| Names/Codes | âŒ | âœ… | âœ… |
| **Overall** | **Good** | **Good** | **Best** |

---

## 3.5. Cross-Encoder Reranking

### 3.5.1. Bi-Encoder vs Cross-Encoder

**Bi-Encoder (SBERT - dÃ¹ng cho retrieval):**
```
Query â†’ Encoder A â†’ Vector A
                              â†“
                         Cosine Similarity
                              â†‘
Document â†’ Encoder B â†’ Vector B

Pros:
âœ… Fast: Encode once, search many times
âœ… Scalable: Can index millions of docs

Cons:
âŒ Less accurate: No interaction between query & doc
```

**Cross-Encoder (dÃ¹ng cho reranking):**
```
[Query + Document] â†’ Encoder â†’ Relevance Score

Pros:
âœ… More accurate: Full attention between query & doc
âœ… Better for final ranking

Cons:
âŒ Slow: Must encode each pair
âŒ Not scalable for large corpus
```

### 3.5.2. Pipeline vá»›i Cross-Encoder

```
Query
  â†“
Retrieval (Bi-Encoder + BM25)
  â†“ Top-K=100
[Doc1, Doc2, ..., Doc100]
  â†“
Reranking (Cross-Encoder)
  â†“ Top-N=5
[Doc7, Doc23, Doc5, Doc89, Doc34]
  â†“
Send to LLM
```

### 3.5.3. Model: MS-MARCO Cross-Encoder

```python
Model: cross-encoder/ms-marco-MiniLM-L-6-v2
Base: MiniLM
Parameters: 22M
Input: Query + Document (max 512 tokens)
Output: Relevance score [0, 1]
```

**Usage:**
```python
from sentence_transformers import CrossEncoder

model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# Score query-document pairs
pairs = [
    ['há»c bá»•ng sinh viÃªn', 'Quy Ä‘á»‹nh vá» há»c bá»•ng...'],
    ['há»c bá»•ng sinh viÃªn', 'Lá»‹ch thi há»c ká»³...'],
]

scores = model.predict(pairs)
# [0.87, 0.23]
```

### 3.5.4. VÃ­ dá»¥ Reranking

**Initial Retrieval (Top-5):**
```
1. Doc A: "Quy Ä‘á»‹nh vá» há»c bá»•ng khuyáº¿n khÃ­ch há»c táº­p" (score: 0.82)
2. Doc B: "Danh sÃ¡ch sinh viÃªn Ä‘áº¡t há»c bá»•ng" (score: 0.79)
3. Doc C: "Äiá»u kiá»‡n xÃ©t há»c bá»•ng: GPA â‰¥ 3.5" (score: 0.77)
4. Doc D: "Há»c phÃ­ vÃ  cÃ¡c khoáº£n thu" (score: 0.75)
5. Doc E: "Thá»§ tá»¥c xin há»c bá»•ng" (score: 0.73)
```

**Query:** "Äiá»u kiá»‡n Ä‘á»ƒ Ä‘Æ°á»£c nháº­n há»c bá»•ng lÃ  gÃ¬?"

**After Cross-Encoder Reranking:**
```
1. Doc C: "Äiá»u kiá»‡n xÃ©t há»c bá»•ng: GPA â‰¥ 3.5" (score: 0.95) â¬†ï¸
2. Doc E: "Thá»§ tá»¥c xin há»c bá»•ng" (score: 0.88) â¬†ï¸
3. Doc A: "Quy Ä‘á»‹nh vá» há»c bá»•ng khuyáº¿n khÃ­ch há»c táº­p" (score: 0.81) â¬‡ï¸
4. Doc B: "Danh sÃ¡ch sinh viÃªn Ä‘áº¡t há»c bá»•ng" (score: 0.45) â¬‡ï¸
5. Doc D: "Há»c phÃ­ vÃ  cÃ¡c khoáº£n thu" (score: 0.21) â¬‡ï¸
```

â†’ Doc C Ä‘Æ°á»£c Ä‘áº©y lÃªn top vÃ¬ chÃ­nh xÃ¡c tráº£ lá»i "Ä‘iá»u kiá»‡n"!

### 3.5.5. Lá»£i Ã­ch Reranking

**Cáº£i thiá»‡n Precision:**
```
Without Reranking:
Top-5 relevant: 3/5 = 60% precision

With Reranking:
Top-5 relevant: 5/5 = 100% precision
```

**Giáº£m chi phÃ­ LLM:**
```
Gá»­i 5 docs relevant â†’ LLM generate tá»‘t
Gá»­i 5 docs irrelevant â†’ LLM confused, waste tokens
```

---

## 3.6. Large Language Models

### 3.6.1. KhÃ¡i niá»‡m LLM

**Large Language Model (LLM)** lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“, cÃ³ kháº£ nÄƒng:
- Hiá»ƒu vÃ  sinh vÄƒn báº£n tá»± nhiÃªn
- Tráº£ lá»i cÃ¢u há»i
- TÃ³m táº¯t, dá»‹ch thuáº­t
- Reasoning vÃ  problem solving

### 3.6.2. Google Gemini 2.0 Flash

Há»‡ thá»‘ng sá»­ dá»¥ng **Gemini 2.0 Flash Experimental**:

```
Model: gemini-2.0-flash-exp
Release: December 2024
Context Window: 1,048,576 tokens (~1M tokens!)
Output: 8,192 tokens max
Modality: Text + Image (multimodal)
Speed: Fast (optimized for low latency)
Cost: Free tier available
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Context window cá»±c lá»›n (1M tokens)
- âœ… Tá»‘c Ä‘á»™ nhanh
- âœ… Há»— trá»£ tiáº¿ng Viá»‡t tá»‘t
- âœ… Multimodal (text + image) â†’ OCR PDFs
- âœ… Free tier hÃ o phÃ³ng

### 3.6.3. Prompt Engineering

**System Prompt:**
```python
SYSTEM_PROMPT = """
Báº¡n lÃ  trá»£ lÃ½ AI cá»§a TrÆ°á»ng Äáº¡i há»c, chuyÃªn tráº£ lá»i cÃ¢u há»i vá»:
- Quy cháº¿ Ä‘Ã o táº¡o
- Tuyá»ƒn sinh
- Há»c bá»•ng
- Thá»§ tá»¥c hÃ nh chÃ­nh

HÆ¯á»šNG DáºªN:
1. Tráº£ lá»i dá»±a trÃªn CONTEXT Ä‘Æ°á»£c cung cáº¥p
2. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong CONTEXT, hÃ£y nÃ³i rÃµ
3. TrÃ­ch dáº«n nguá»“n (tÃªn file, trang sá»‘)
4. Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, rÃµ rÃ ng, lá»‹ch sá»±
5. Format markdown khi cáº§n (danh sÃ¡ch, báº£ng...)

CONTEXT:
{retrieved_documents}

QUESTION:
{user_question}

ANSWER:
"""
```

**Few-shot Examples:**
```python
FEW_SHOT_EXAMPLES = """
Example 1:
Q: Äiá»u kiá»‡n xÃ©t há»c bá»•ng lÃ  gÃ¬?
A: Theo quy cháº¿ Ä‘Ã o táº¡o, sinh viÃªn Ä‘Æ°á»£c xÃ©t há»c bá»•ng khi Ä‘Ã¡p á»©ng:
1. Äiá»ƒm trung bÃ¬nh há»c táº­p â‰¥ 3.5/4.0
2. Äiá»ƒm rÃ¨n luyá»‡n â‰¥ 80/100
3. KhÃ´ng cÃ³ há»c pháº§n nÃ o dÆ°á»›i Ä‘iá»ƒm C

[Nguá»“n: QUY_CHE_DAO_TAO.pdf, trang 15]

Example 2:
Q: Thá»i tiáº¿t hÃ´m nay tháº¿ nÃ o?
A: Xin lá»—i, tÃ´i khÃ´ng cÃ³ thÃ´ng tin vá» thá»i tiáº¿t. TÃ´i chá»‰ cÃ³ thá»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i liÃªn quan Ä‘áº¿n quy Ä‘á»‹nh, tuyá»ƒn sinh, há»c bá»•ng cá»§a trÆ°á»ng.
"""
```

### 3.6.4. Response Generation Flow

```python
def generate_response(query: str, context: List[Document]) -> str:
    """
    Generate response using Gemini
    """
    # 1. Format context
    context_text = "\n\n".join([
        f"[Document {i+1}]\n"
        f"Source: {doc.source_file}, Page: {doc.page_number}\n"
        f"Content: {doc.content}\n"
        for i, doc in enumerate(context)
    ])
    
    # 2. Build prompt
    prompt = SYSTEM_PROMPT.format(
        retrieved_documents=context_text,
        user_question=query
    )
    
    # 3. Call Gemini API
    response = genai.GenerativeModel('gemini-2.0-flash-exp').generate_content(
        prompt,
        generation_config={
            'temperature': 0.2,      # Low for factual answers
            'top_p': 0.8,
            'top_k': 40,
            'max_output_tokens': 2048,
        }
    )
    
    return response.text
```

### 3.6.5. Parameters Tuning

**Temperature:**
```
temperature = 0.0   â†’ Deterministic, factual
temperature = 0.5   â†’ Balanced
temperature = 1.0   â†’ Creative, diverse

Há»‡ thá»‘ng dÃ¹ng: 0.2 (Æ°u tiÃªn Ä‘á»™ chÃ­nh xÃ¡c)
```

**Top-p (Nucleus Sampling):**
```
top_p = 0.1   â†’ Very focused
top_p = 0.9   â†’ More diverse
top_p = 1.0   â†’ All tokens considered

Há»‡ thá»‘ng dÃ¹ng: 0.8
```

**Top-k:**
```
top_k = 1     â†’ Always pick most likely token
top_k = 40    â†’ Consider top 40 tokens
top_k = âˆ     â†’ Consider all tokens

Há»‡ thá»‘ng dÃ¹ng: 40
```

### 3.6.6. Gemini Vision API (OCR)

**Sá»­ dá»¥ng cho PDF OCR:**
```python
def extract_text_with_gemini(pdf_page_image):
    """
    Extract text from PDF page using Gemini Vision
    """
    prompt = """
    Extract all text from this document page.
    Preserve formatting, headings, and structure.
    Output plain text with line breaks preserved.
    """
    
    model = genai.GenerativeModel('gemini-2.0-flash-exp')
    response = model.generate_content([prompt, pdf_page_image])
    
    return response.text
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… OCR quality cao nháº¥t
- âœ… Hiá»ƒu layout phá»©c táº¡p
- âœ… Xá»­ lÃ½ Ä‘Æ°á»£c báº£ng, multi-column
- âœ… Há»— trá»£ tiáº¿ng Viá»‡t tá»‘t

---

## 3.7. CÃ´ng nghá»‡ Backend

### 3.7.1. FastAPI

**FastAPI** lÃ  modern web framework cho Python:

```python
from fastapi import FastAPI

app = FastAPI(
    title="University Chatbot API",
    version="1.0.0",
    docs_url="/api/docs"
)

@app.post("/api/v1/chat")
async def chat(request: ChatRequest):
    """Chat endpoint"""
    response = await chat_service.process_query(request.query)
    return ChatResponse(
        answer=response.answer,
        sources=response.sources,
        confidence=response.confidence
    )
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Ráº¥t nhanh (dá»±a trÃªn Starlette + Pydantic)
- âœ… Auto-generated OpenAPI docs
- âœ… Type hints vÃ  validation
- âœ… Async support
- âœ… Easy to test

### 3.7.2. SQLAlchemy + psycopg2

**SQLAlchemy**: ORM (Object-Relational Mapping)

```python
from sqlalchemy import Column, Integer, String, Text
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Document(Base):
    __tablename__ = "documents"
    
    id = Column(Integer, primary_key=True)
    filename = Column(String(500))
    content = Column(Text)
    status = Column(String(50))
```

**psycopg2**: PostgreSQL adapter

```python
import psycopg2

conn = psycopg2.connect(
    host="localhost",
    database="uni_bot_db",
    user="postgres",
    password="password"
)
```

### 3.7.3. Pydantic

**Data validation vÃ  serialization:**

```python
from pydantic import BaseModel, Field, validator

class ChatRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500)
    conversation_id: Optional[str] = None
    
    @validator('query')
    def validate_query(cls, v):
        if not v.strip():
            raise ValueError('Query cannot be empty')
        return v.strip()

class ChatResponse(BaseModel):
    answer: str
    sources: List[SourceDocument]
    confidence: float
    processing_time: float
```

### 3.7.4. Redis (Caching)

**Caching layer:**

```python
import redis

redis_client = redis.Redis(
    host='localhost',
    port=6379,
    db=0,
    decode_responses=True
)

# Cache embedding
def get_embedding_cached(text: str):
    cache_key = f"emb:{hash(text)}"
    
    # Try cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Generate
    embedding = model.encode(text)
    
    # Save to cache (7 days TTL)
    redis_client.setex(
        cache_key,
        7 * 24 * 3600,
        json.dumps(embedding.tolist())
    )
    
    return embedding
```

---

## 3.8. CÃ´ng nghá»‡ Frontend

### 3.8.1. Next.js 15

**Next.js** lÃ  React framework vá»›i:

```typescript
// App Router (Next.js 15)
// app/chat/page.tsx
export default function ChatPage() {
  return (
    <div>
      <ChatInterface />
    </div>
  )
}

// Server Components by default
// Client Components vá»›i 'use client'
'use client'
export function ChatInterface() {
  const [messages, setMessages] = useState([])
  // ... client logic
}
```

**Features:**
- âœ… App Router (new architecture)
- âœ… Server Components
- âœ… API Routes
- âœ… Image Optimization
- âœ… Built-in CSS support

### 3.8.2. React 19

**Latest React with:**
- âœ… Concurrent rendering
- âœ… Suspense for data fetching
- âœ… Server Components
- âœ… Actions

```tsx
'use client'
import { useState } from 'react'

export function ChatBox() {
  const [query, setQuery] = useState('')
  const [messages, setMessages] = useState([])
  
  const handleSubmit = async (e) => {
    e.preventDefault()
    
    const response = await fetch('/api/v1/chat', {
      method: 'POST',
      body: JSON.stringify({ query })
    })
    
    const data = await response.json()
    setMessages([...messages, { role: 'assistant', content: data.answer }])
  }
  
  return (
    <form onSubmit={handleSubmit}>
      <input value={query} onChange={(e) => setQuery(e.target.value)} />
      <button type="submit">Send</button>
    </form>
  )
}
```

### 3.8.3. TypeScript

**Type safety:**

```typescript
// types/chat.ts
export interface Message {
  id: string
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
  sources?: Source[]
}

export interface Source {
  filename: string
  pageNumber: number
  content: string
}

export interface ChatResponse {
  answer: string
  sources: Source[]
  confidence: number
}
```

### 3.8.4. Tailwind CSS

**Utility-first CSS:**

```tsx
<div className="flex flex-col h-screen bg-gray-50">
  <div className="flex-1 overflow-y-auto p-4 space-y-4">
    {messages.map(msg => (
      <div 
        key={msg.id}
        className={`
          max-w-2xl rounded-lg p-4
          ${msg.role === 'user' 
            ? 'bg-blue-500 text-white ml-auto' 
            : 'bg-white text-gray-800 shadow-md'}
        `}
      >
        {msg.content}
      </div>
    ))}
  </div>
</div>
```

### 3.8.5. shadcn/ui

**Beautiful components:**

```tsx
import { Button } from '@/components/ui/button'
import { Card } from '@/components/ui/card'
import { Input } from '@/components/ui/input'

<Card className="p-6">
  <Input 
    placeholder="Nháº­p cÃ¢u há»i..." 
    value={query}
    onChange={(e) => setQuery(e.target.value)}
  />
  <Button onClick={handleSubmit}>
    Gá»­i cÃ¢u há»i
  </Button>
</Card>
```

---

## 3.9. CÆ¡ sá»Ÿ dá»¯ liá»‡u

### 3.9.1. PostgreSQL 16

**Features:**
- âœ… ACID compliance
- âœ… JSONB support
- âœ… Full-text search
- âœ… Extensions (pgvector)
- âœ… Replication

**Schema Example:**
```sql
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    filename VARCHAR(500) NOT NULL,
    file_path TEXT NOT NULL,
    file_size INTEGER,
    file_hash VARCHAR(64) UNIQUE,
    total_chunks INTEGER DEFAULT 0,
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT true
);

CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_documents_hash ON documents(file_hash);
```

### 3.9.2. pgvector Extension

**Vector similarity search:**

```sql
-- Install extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create table with vector column
CREATE TABLE embeddings (
    id SERIAL PRIMARY KEY,
    chunk_id INTEGER REFERENCES chunks(id),
    embedding vector(384),  -- 384 dimensions
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create index for fast search
CREATE INDEX embeddings_embedding_idx 
ON embeddings 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- Search query
SELECT chunk_id, 
       1 - (embedding <=> '[0.1,0.2,...]'::vector) AS similarity
FROM embeddings
ORDER BY embedding <=> '[0.1,0.2,...]'::vector
LIMIT 20;
```

**Operators:**
```sql
<=>   -- Cosine distance (1 - cosine similarity)
<->   -- L2 distance (Euclidean)
<#>   -- Inner product
```

### 3.9.3. Indexes

**B-tree Index:**
```sql
CREATE INDEX idx_chunks_doc_id ON chunks(document_id);
```

**GIN Index (Full-text search):**
```sql
CREATE INDEX idx_chunks_content_gin 
ON chunks 
USING gin(to_tsvector('english', content));
```

**IVFFlat Index (Vector):**
```sql
CREATE INDEX embeddings_embedding_idx 
ON embeddings 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

---

## 3.10. Containerization vÃ  Deployment

### 3.10.1. Docker

**Containerization benefits:**
- âœ… Consistent environment
- âœ… Easy deployment
- âœ… Isolation
- âœ… Scalability

**Dockerfile Example:**
```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3.10.2. Docker Compose

**Multi-container orchestration:**

```yaml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: uni_bot_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  backend:
    build: .
    depends_on:
      - postgres
      - redis
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/uni_bot_db
      REDIS_URL: redis://redis:6379
    ports:
      - "8000:8000"
  
  frontend:
    build: ./frontend
    depends_on:
      - backend
    ports:
      - "3000:3000"

volumes:
  postgres_data:
```

**Commands:**
```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f backend

# Stop all services
docker-compose down

# Rebuild
docker-compose up --build
```

### 3.10.3. Environment Variables

**.env file:**
```bash
# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/uni_bot_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=password
POSTGRES_DB=uni_bot_db

# Redis
REDIS_URL=redis://localhost:6379
REDIS_PASSWORD=

# API Keys
GEMINI_API_KEY=your_gemini_api_key_here

# Application
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO

# CORS
ALLOWED_ORIGINS=http://localhost:3000,https://your-domain.com
```

---

## Káº¾T LUáº¬N CHÆ¯Æ NG 3

ChÆ°Æ¡ng nÃ y Ä‘Ã£ trÃ¬nh bÃ y Ä‘áº§y Ä‘á»§ cÃ¡c cÆ¡ sá»Ÿ lÃ½ thuyáº¿t vÃ  cÃ´ng nghá»‡ Ä‘Æ°á»£c sá»­ dá»¥ng trong há»‡ thá»‘ng:

### ğŸ¯ CÃ¡c ká»¹ thuáº­t AI/ML:
- **RAG**: Káº¿t há»£p retrieval vÃ  generation cho cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c
- **Vector Embeddings**: Semantic search vá»›i Vietnamese SBERT
- **BM25**: Sparse retrieval cho keyword matching
- **Hybrid Search**: Káº¿t há»£p vector + BM25 vá»›i RRF
- **Cross-Encoder**: Reranking Ä‘á»ƒ cáº£i thiá»‡n precision
- **LLM**: Gemini 2.0 Flash cho generation vÃ  OCR

### ğŸ’» Stack cÃ´ng nghá»‡:
- **Backend**: FastAPI, SQLAlchemy, Redis
- **Frontend**: Next.js 15, React 19, TypeScript, Tailwind CSS
- **Database**: PostgreSQL 16 vá»›i pgvector
- **Deployment**: Docker, Docker Compose

### ğŸ“Š So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p:

| Method | Accuracy | Speed | Use Case |
|--------|----------|-------|----------|
| Vector Search | High (semantic) | Fast | General queries |
| BM25 | High (exact) | Very Fast | Keyword queries |
| Hybrid | Very High | Fast | Best overall |
| + Reranking | Excellent | Medium | Final top-N |

CÃ¡c lÃ½ thuyáº¿t nÃ y táº¡o ná»n táº£ng vá»¯ng cháº¯c cho viá»‡c hiá»ƒu vÃ  phÃ¡t triá»ƒn há»‡ thá»‘ng chatbot trong cÃ¡c chÆ°Æ¡ng tiáº¿p theo.

---

**ChÆ°Æ¡ng tiáº¿p theo**: [ChÆ°Æ¡ng 4 - Kiáº¿n trÃºc Há»‡ thá»‘ng](./TECHNICAL_ARCHITECTURE.md)

**Document Version**: 1.0.0  
**Last Updated**: December 2025  
**Author**: University Chatbot Development Team
