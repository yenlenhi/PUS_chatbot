# üß™ B∆∞·ªõc 8: Testing & Verification Guide

H∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ test v√† verify h·ªá th·ªëng Data Layer m·ªõi.

## üìã Testing Phases

### Phase 1: Infrastructure Testing
### Phase 2: Service Testing
### Phase 3: Integration Testing
### Phase 4: End-to-End Testing

---

## Phase 1: Infrastructure Testing

### 1.1 PostgreSQL Connection Test

**File**: `test_postgres_connection.py` (ƒë√£ t·∫°o)

```bash
# Ch·∫°y test
powershell python test_postgres_connection.py
```

**Ki·ªÉm tra**:
- ‚úÖ PostgreSQL connection successful
- ‚úÖ pgvector extension installed
- ‚úÖ Database tables created
- ‚úÖ Services can be imported

### 1.2 Docker Containers Status

```bash
# Ki·ªÉm tra containers
docker-compose ps

# Ki·ªÉm tra logs
docker-compose logs postgres
docker-compose logs pgadmin
```

**K·ª≥ v·ªçng**:
- `uni_bot_postgres` - HEALTHY
- `uni_bot_pgadmin` - running (optional)

### 1.3 Database Schema Verification

```bash
# K·∫øt n·ªëi t·ªõi database
docker exec -it uni_bot_postgres psql -U uni_bot_user -d uni_bot_db

# Ki·ªÉm tra tables
\dt

# Ki·ªÉm tra extensions
SELECT * FROM pg_extension WHERE extname = 'vector';

# Ki·ªÉm tra indexes
\di

# Tho√°t
\q
```

---

## Phase 2: Service Testing

### 2.1 PostgreSQL Database Service Test

**T·∫°o file**: `test_postgres_service.py`

```python
import asyncio
from src.services.postgres_database_service import PostgresDatabaseService
from src.models.schemas import DocumentChunk

async def test_database_service():
    # Initialize service
    db_service = PostgresDatabaseService()
    
    # Test insert chunks
    chunks = [
        DocumentChunk(
            content="Test content 1",
            source_file="test.pdf",
            page_number=1,
            chunk_index=0,
            word_count=3,
            char_count=15
        ),
        DocumentChunk(
            content="Test content 2",
            source_file="test.pdf",
            page_number=1,
            chunk_index=1,
            word_count=3,
            char_count=15
        )
    ]
    
    chunk_ids = db_service.insert_chunks(chunks)
    print(f"‚úÖ Inserted {len(chunk_ids)} chunks")
    
    # Test get chunks
    all_chunks = db_service.get_all_chunks()
    print(f"‚úÖ Retrieved {len(all_chunks)} chunks")
    
    # Test stats
    stats = db_service.get_database_stats()
    print(f"‚úÖ Database stats: {stats}")
    
    # Cleanup
    db_service.delete_chunks_by_file("test.pdf")
    print("‚úÖ Cleanup completed")

# Run test
asyncio.run(test_database_service())
```

**Ch·∫°y test**:
```bash
powershell python test_postgres_service.py
```

### 2.2 Embedding Service Test

**T·∫°o file**: `test_embedding_service.py`

```python
from src.services.embedding_service import EmbeddingService

def test_embedding_service():
    service = EmbeddingService()
    
    # Test single embedding
    text = "ƒê√¢y l√† m·ªôt b√†i ki·ªÉm tra"
    embedding = service.generate_embedding(text)
    print(f"‚úÖ Generated embedding with shape: {embedding.shape}")
    
    # Test batch embeddings
    texts = ["Text 1", "Text 2", "Text 3"]
    embeddings = service.generate_embeddings(texts)
    print(f"‚úÖ Generated {len(embeddings)} embeddings")
    
    # Verify dimension
    assert embedding.shape[0] == 384, "Embedding dimension should be 384"
    print("‚úÖ Embedding dimension correct (384)")

test_embedding_service()
```

**Ch·∫°y test**:
```bash
powershell python test_embedding_service.py
```

### 2.3 Hybrid Retrieval Service Test

**T·∫°o file**: `test_hybrid_retrieval.py`

```python
import numpy as np
from src.services.postgres_database_service import PostgresDatabaseService
from src.services.embedding_service import EmbeddingService
from src.services.hybrid_retrieval_service import HybridRetrievalService
from src.models.schemas import DocumentChunk

def test_hybrid_retrieval():
    # Initialize services
    db_service = PostgresDatabaseService()
    embedding_service = EmbeddingService()
    
    # Insert test chunks
    chunks = [
        DocumentChunk(
            content="Tr∆∞·ªùng ƒê·∫°i h·ªçc An ninh Nh√¢n d√¢n l√† m·ªôt tr∆∞·ªùng ƒë·∫°i h·ªçc h√†ng ƒë·∫ßu",
            source_file="test.pdf",
            page_number=1,
            chunk_index=0,
            word_count=10,
            char_count=60
        ),
        DocumentChunk(
            content="Tuy·ªÉn sinh nƒÉm 2024 c√≥ nhi·ªÅu ng√†nh ƒë√†o t·∫°o m·ªõi",
            source_file="test.pdf",
            page_number=2,
            chunk_index=1,
            word_count=9,
            char_count=45
        )
    ]
    
    chunk_ids = db_service.insert_chunks(chunks)
    
    # Generate embeddings
    embeddings = embedding_service.generate_embeddings(
        [chunk.content for chunk in chunks]
    )
    db_service.insert_embeddings(chunk_ids, embeddings)
    
    # Initialize hybrid retrieval
    hybrid_retrieval = HybridRetrievalService(db_service, embedding_service)
    
    # Test hybrid search
    query = "Tr∆∞·ªùng ƒë·∫°i h·ªçc tuy·ªÉn sinh"
    query_embedding = embedding_service.generate_embedding(query)
    
    results = hybrid_retrieval.hybrid_search(
        query=query,
        query_embedding=query_embedding,
        top_k=5
    )
    
    print(f"‚úÖ Hybrid search found {len(results)} results")
    for result in results:
        print(f"   - Score: {result['combined_score']:.4f}")
        print(f"     Dense: {result['dense_score']:.4f}, Sparse: {result['sparse_score']:.4f}")
    
    # Cleanup
    db_service.delete_chunks_by_file("test.pdf")
    print("‚úÖ Cleanup completed")

test_hybrid_retrieval()
```

**Ch·∫°y test**:
```bash
powershell python test_hybrid_retrieval.py
```

---

## Phase 3: Integration Testing

### 3.1 PDF Ingestion Test

**T·∫°o file**: `test_ingestion.py`

```python
import asyncio
from pathlib import Path
from src.services.postgres_database_service import PostgresDatabaseService
from src.services.embedding_service import EmbeddingService
from src.services.pdf_processor import PDFProcessor
from src.services.ingestion_service import IngestionService

async def test_ingestion():
    # Initialize services
    db_service = PostgresDatabaseService()
    embedding_service = EmbeddingService()
    pdf_processor = PDFProcessor()
    
    ingestion_service = IngestionService(
        db_service,
        embedding_service,
        pdf_processor
    )
    
    # Test processing directory
    pdf_dir = Path("data/pdfs")
    if pdf_dir.exists():
        processed = await ingestion_service.process_directory(pdf_dir)
        print(f"‚úÖ Processed {processed} PDF files")
        
        # Check database
        stats = db_service.get_database_stats()
        print(f"‚úÖ Database stats: {stats}")
    else:
        print("‚ö†Ô∏è No PDF directory found")

asyncio.run(test_ingestion())
```

**Ch·∫°y test**:
```bash
powershell python test_ingestion.py
```

---

## Phase 4: End-to-End Testing

### 4.1 Full RAG Pipeline Test

**T·∫°o file**: `test_rag_pipeline.py`

```python
import asyncio
from src.services.rag_service import RAGService

async def test_rag_pipeline():
    # Initialize RAG service
    rag_service = RAGService()
    
    # Test queries
    test_queries = [
        "Tr∆∞·ªùng ƒë·∫°i h·ªçc An ninh Nh√¢n d√¢n c√≥ nh·ªØng ng√†nh n√†o?",
        "ƒêi·ªÅu ki·ªán tuy·ªÉn sinh l√† g√¨?",
        "H·ªçc ph√≠ bao nhi√™u ti·ªÅn?"
    ]
    
    for query in test_queries:
        print(f"\nüîç Query: {query}")
        
        try:
            response = await rag_service.generate_answer(query)
            
            print(f"‚úÖ Answer: {response['answer'][:100]}...")
            print(f"   Confidence: {response['confidence']:.2f}")
            print(f"   Sources: {response['sources']}")
            print(f"   Processing time: {response['processing_time']:.2f}s")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")

asyncio.run(test_rag_pipeline())
```

**Ch·∫°y test**:
```bash
powershell python test_rag_pipeline.py
```

### 4.2 API Endpoint Test

**T·∫°o file**: `test_api_endpoints.py`

```python
import requests
import json

BASE_URL = "http://localhost:8000/api/v1"

def test_health():
    """Test health endpoint"""
    response = requests.get(f"{BASE_URL}/health")
    print(f"‚úÖ Health: {response.json()}")

def test_chat():
    """Test chat endpoint"""
    payload = {
        "message": "Tr∆∞·ªùng ƒë·∫°i h·ªçc An ninh Nh√¢n d√¢n l√† g√¨?",
        "conversation_id": "test-123"
    }
    
    response = requests.post(f"{BASE_URL}/chat", json=payload)
    result = response.json()
    
    print(f"‚úÖ Chat response:")
    print(f"   Answer: {result['answer'][:100]}...")
    print(f"   Confidence: {result['confidence']}")
    print(f"   Sources: {result['sources']}")

def test_search():
    """Test search endpoint"""
    payload = {
        "query": "tuy·ªÉn sinh",
        "top_k": 5
    }
    
    response = requests.post(f"{BASE_URL}/search", json=payload)
    result = response.json()
    
    print(f"‚úÖ Search found {len(result['results'])} results")

# Run tests
print("Testing API Endpoints...\n")
test_health()
test_chat()
test_search()
```

**Ch·∫°y test**:
```bash
# ƒê·∫£m b·∫£o backend ƒëang ch·∫°y
powershell python main.py

# Trong terminal kh√°c
powershell python test_api_endpoints.py
```

---

## üìä Test Results Checklist

### Infrastructure
- [ ] PostgreSQL connection successful
- [ ] pgvector extension installed
- [ ] All tables created
- [ ] Indexes created

### Services
- [ ] Database service CRUD operations work
- [ ] Embedding service generates correct dimensions
- [ ] Hybrid retrieval combines dense + sparse
- [ ] Ingestion service processes PDFs

### Integration
- [ ] PDF ingestion works end-to-end
- [ ] Chunks inserted correctly
- [ ] Embeddings generated and stored
- [ ] BM25 index built

### End-to-End
- [ ] RAG pipeline works
- [ ] Chat endpoint returns answers
- [ ] Search endpoint returns results
- [ ] Conversation history saved

---

## üöÄ Performance Benchmarks

### Expected Performance
- Dense search: < 100ms
- Sparse search: < 50ms
- Hybrid search: < 150ms
- Embedding generation: < 500ms
- Full RAG pipeline: < 2s

### Monitoring
```bash
# Monitor PostgreSQL
docker exec -it uni_bot_postgres psql -U uni_bot_user -d uni_bot_db
SELECT * FROM pg_stat_statements;

# Monitor application logs
tail -f logs/chatbot.log
```

---

## ‚úÖ Completion Checklist

- [ ] All infrastructure tests pass
- [ ] All service tests pass
- [ ] Integration tests pass
- [ ] End-to-end tests pass
- [ ] Performance meets expectations
- [ ] Documentation updated
- [ ] Ready for production deployment

---

**Next Step**: Production Deployment

